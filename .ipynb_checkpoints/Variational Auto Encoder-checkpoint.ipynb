{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bcc2496-d318-4b5a-b2ea-521a5ef56540",
   "metadata": {},
   "source": [
    "# **一个VAE算法的简单实现**\n",
    "## **算法概述**\n",
    "- 与传统的AE有相似的过程，图片->编码->图片（生成特指编码->图片的过程）\n",
    "- 与传统AE不同点有下：\n",
    "    - 传统AE中图片->编码产生的编码仅代表一组高维特征，VAE中产生的编码有两组，两组编码分别代表高斯分布的$\\mu$和$\\sigma$\n",
    "    - 传统AE中编码->图片过程是直接从编码到图片的端到端的过程，VAE则是利用一标准高斯分布的采样，通过重参数化技巧得到新编码，该新编码执行编码->图片的过程。\n",
    "- VAE下采样阶段使用传统的Conv层，最后一层的输出的特征拉平后通过两个线性层得到$\\mu$和$\\sigma$\n",
    "- VAE上采样阶段之前，将重参数化得到的编码变为(b,c,h,w)的格式，使用ConvTrans层将编码上采样至原图大小\n",
    "\n",
    "论文链接：*https://docs.popo.netease.com/docs/9b877dd824ba440cb286d6f7923d1dbb*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4fbca12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "# from torch import tensor as Tensor\n",
    "\n",
    "Tensor = TypeVar('torch.tensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd4f6dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from abc import abstractmethod\n",
    "\n",
    "class BaseVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        super(BaseVAE, self).__init__()\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def decode(self, input: Tensor) -> Any:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, batch_size:int, current_device: int, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *inputs: Tensor) -> Tensor:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def loss_function(self, *inputs: Any, **kwargs) -> Tensor:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30a7c80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12e59112-e3ce-4fab-bf6d-5c8644900696",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaVAE(BaseVAE):\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 beta: int = 6,\n",
    "                 gamma:float = 1000.,\n",
    "                 max_capacity: int = 25,\n",
    "                 Capacity_max_iter: int = 1e5,\n",
    "                 loss_type:str = 'B',\n",
    "                 **kwargs) -> None:\n",
    "        super(BetaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.loss_type = loss_type\n",
    "        self.C_max = torch.Tensor([max_capacity])\n",
    "        self.C_stop_iter = Capacity_max_iter\n",
    "        \n",
    "        modules = []\n",
    "        hidden_dims = [32, 64, 128, 256]\n",
    "        # Build Encoder\n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels,out_channels=h_dim,\n",
    "                             kernel_size=3,stride=2,padding=1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "            \n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1]*4, latent_dim)\n",
    "        \n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1]*4)\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims)-1):\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],hidden_dims[i+1],\n",
    "                                      kernel_size=3,stride=2,padding=1,\n",
    "                                      output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i+1]),\n",
    "                    nn.LeakyReLU())\n",
    "                )\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "        \n",
    "        # Build final layer\n",
    "        self.final_layer = nn.Sequential(nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                                      hidden_dims[-1],\n",
    "                                                      kernel_size=3,\n",
    "                                                      stride=2,\n",
    "                                                      padding=1,\n",
    "                                                      output_padding=1),\n",
    "                                   nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                                   nn.LeakyReLU(),\n",
    "                                   nn.Conv2d(hidden_dims[-1],out_channels=1,\n",
    "                                            kernel_size=3,padding=1),\n",
    "                                   nn.Tanh())\n",
    "        \n",
    "    def encode(self, input):\n",
    "        result = self.encoder(input)\n",
    "        result = torch.flatten(result,start_dim=1)\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "        \n",
    "        return [mu, log_var]\n",
    "    \n",
    "    def decode(self, z):\n",
    "        result = self.decoder_input(z)\n",
    "        result = result.view(-1,256,2,2)\n",
    "        result = self.decoder(result)\n",
    "        result = self.final_layer(result)\n",
    "        return result\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(logvar)\n",
    "        return eps*std + mu\n",
    "    \n",
    "    def forward(self, input, **kwargs):\n",
    "        mu, logvar = self.encode(input)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        result = self.decode(z)\n",
    "        return [result, input, mu, logvar]\n",
    "    \n",
    "    def loss_calculate(self,*args,**kwargs):\n",
    "        \n",
    "        recons = args[0]\n",
    "        input = args[1]\n",
    "        mu = args[2]\n",
    "        log_var = args[3]\n",
    "        kld_weight = 0.00025\n",
    "        recons_loss = F.mse_loss(recons, input)\n",
    "        kld_loss = torch.mean(-0.5 * torch.sum(1+log_var - log_var.exp() - mu ** 2, dim=1),dim=0)\n",
    "        loss = recons_loss + self.beta * kld_weight * kld_loss\n",
    "        \n",
    "        return {'loss': loss, 'Reconstruction_Loss':recons_loss, 'KLD':kld_loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71b306d9-00c3-473e-9f5e-05d5e164f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af81f698-32e3-4130-b782-6573a4999e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a814401-e193-40b0-a426-5db45a447838",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    torchvision.datasets.MNIST('./data/', train=True, download=True,\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   torchvision.transforms.Resize((32,32)),\n",
    "                                   torchvision.transforms.ToTensor(),\n",
    "                               ])),\n",
    "    batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a2401d9-5f23-462d-a9a7-3eb35ea6c4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "vae_model = BetaVAE(1,128)\n",
    "vae_model.cuda(0)\n",
    "optimizer = optim.Adam(vae_model.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2404664e-ae7f-4f38-acb6-1cbe7957128e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    loss_sum_in_one_batch = 0\n",
    "    batch_count = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        data = data.cuda(0)\n",
    "        result = vae_model(data)\n",
    "        loss_dict = vae_model.loss_calculate(*result, M_N=None)\n",
    "        loss_dict['loss'].backward()\n",
    "        optimizer.step()\n",
    "        loss_sum_in_one_batch += loss_dict['loss'].item()\n",
    "        batch_count += 1\n",
    "        if batch_count % 100 == 0:\n",
    "            print(f\"epoch {epoch} batch count {batch_count} , avg loss is {loss_sum_in_one_batch / batch_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "696618f4-31c0-4f1a-8fd7-7f3a938e33b5",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch count 100 , avg loss is 0.03260975547134876\n",
      "epoch 1 batch count 200 , avg loss is 0.03240977791137993\n",
      "epoch 1 batch count 300 , avg loss is 0.03231917046631376\n",
      "epoch 1 batch count 400 , avg loss is 0.03224421329330653\n",
      "epoch 1 batch count 500 , avg loss is 0.03221241130307317\n",
      "epoch 1 batch count 600 , avg loss is 0.03221311222140988\n",
      "epoch 1 batch count 700 , avg loss is 0.03219418898224831\n",
      "epoch 1 batch count 800 , avg loss is 0.03217401189496741\n",
      "epoch 1 batch count 900 , avg loss is 0.03215733760553929\n",
      "epoch 1 batch count 1000 , avg loss is 0.0321540221888572\n",
      "epoch 1 batch count 1100 , avg loss is 0.03212797933342782\n",
      "epoch 1 batch count 1200 , avg loss is 0.03211579115595668\n",
      "epoch 1 batch count 1300 , avg loss is 0.03210677533911971\n",
      "epoch 1 batch count 1400 , avg loss is 0.032105460351865206\n",
      "epoch 1 batch count 1500 , avg loss is 0.03209236857543389\n",
      "epoch 1 batch count 1600 , avg loss is 0.03208875045529567\n",
      "epoch 1 batch count 1700 , avg loss is 0.03207043449120486\n",
      "epoch 1 batch count 1800 , avg loss is 0.032074597290613585\n",
      "epoch 2 batch count 100 , avg loss is 0.03190499225631356\n",
      "epoch 2 batch count 200 , avg loss is 0.03195294455625117\n",
      "epoch 2 batch count 300 , avg loss is 0.031907881144434214\n",
      "epoch 2 batch count 400 , avg loss is 0.03191671359352768\n",
      "epoch 2 batch count 500 , avg loss is 0.03197869931161404\n",
      "epoch 2 batch count 600 , avg loss is 0.0319685705875357\n",
      "epoch 2 batch count 700 , avg loss is 0.03198473799441542\n",
      "epoch 2 batch count 800 , avg loss is 0.0319707979215309\n",
      "epoch 2 batch count 900 , avg loss is 0.031964498348534105\n",
      "epoch 2 batch count 1000 , avg loss is 0.03197977182269096\n",
      "epoch 2 batch count 1100 , avg loss is 0.03198691727932204\n",
      "epoch 2 batch count 1200 , avg loss is 0.031975741089942555\n",
      "epoch 2 batch count 1300 , avg loss is 0.031970964264697754\n",
      "epoch 2 batch count 1400 , avg loss is 0.03196578324107187\n",
      "epoch 2 batch count 1500 , avg loss is 0.03195066115881006\n",
      "epoch 2 batch count 1600 , avg loss is 0.03193478669389151\n",
      "epoch 2 batch count 1700 , avg loss is 0.031931816504939516\n",
      "epoch 2 batch count 1800 , avg loss is 0.03193748204244508\n",
      "epoch 3 batch count 100 , avg loss is 0.03163825314491987\n",
      "epoch 3 batch count 200 , avg loss is 0.03178914955817163\n",
      "epoch 3 batch count 300 , avg loss is 0.03179687447845936\n",
      "epoch 3 batch count 400 , avg loss is 0.03177187557797879\n",
      "epoch 3 batch count 500 , avg loss is 0.03171821793541312\n",
      "epoch 3 batch count 600 , avg loss is 0.031671191112448774\n",
      "epoch 3 batch count 700 , avg loss is 0.03169074547077928\n",
      "epoch 3 batch count 800 , avg loss is 0.03168694284046069\n",
      "epoch 3 batch count 900 , avg loss is 0.031724213121665845\n",
      "epoch 3 batch count 1000 , avg loss is 0.031744894675910476\n",
      "epoch 3 batch count 1100 , avg loss is 0.031775228152559565\n",
      "epoch 3 batch count 1200 , avg loss is 0.031770173038045565\n",
      "epoch 3 batch count 1300 , avg loss is 0.03177038688069353\n",
      "epoch 3 batch count 1400 , avg loss is 0.03176979956483202\n",
      "epoch 3 batch count 1500 , avg loss is 0.031753928421686096\n",
      "epoch 3 batch count 1600 , avg loss is 0.031743626042734834\n",
      "epoch 3 batch count 1700 , avg loss is 0.03174689378041555\n",
      "epoch 3 batch count 1800 , avg loss is 0.03175555024606486\n",
      "epoch 4 batch count 100 , avg loss is 0.03149171981960535\n",
      "epoch 4 batch count 200 , avg loss is 0.031545025035738944\n",
      "epoch 4 batch count 300 , avg loss is 0.0316351135323445\n",
      "epoch 4 batch count 400 , avg loss is 0.031686114706099035\n",
      "epoch 4 batch count 500 , avg loss is 0.03172612026706338\n",
      "epoch 4 batch count 600 , avg loss is 0.031703821262344715\n",
      "epoch 4 batch count 700 , avg loss is 0.03170987549903137\n",
      "epoch 4 batch count 800 , avg loss is 0.031681205113418404\n",
      "epoch 4 batch count 900 , avg loss is 0.03167129917691151\n",
      "epoch 4 batch count 1000 , avg loss is 0.031653036002069715\n",
      "epoch 4 batch count 1100 , avg loss is 0.03164502956311811\n",
      "epoch 4 batch count 1200 , avg loss is 0.0316444347333163\n",
      "epoch 4 batch count 1300 , avg loss is 0.03165108712533345\n",
      "epoch 4 batch count 1400 , avg loss is 0.031642111780654104\n",
      "epoch 4 batch count 1500 , avg loss is 0.03165612978984912\n",
      "epoch 4 batch count 1600 , avg loss is 0.031663927630288524\n",
      "epoch 4 batch count 1700 , avg loss is 0.031675323586472695\n",
      "epoch 4 batch count 1800 , avg loss is 0.03167702743990554\n",
      "epoch 5 batch count 100 , avg loss is 0.03176352508366108\n",
      "epoch 5 batch count 200 , avg loss is 0.03177559969015419\n",
      "epoch 5 batch count 300 , avg loss is 0.031646097066501774\n",
      "epoch 5 batch count 400 , avg loss is 0.03157583551015705\n",
      "epoch 5 batch count 500 , avg loss is 0.03158992031216622\n",
      "epoch 5 batch count 600 , avg loss is 0.031608117142071325\n",
      "epoch 5 batch count 700 , avg loss is 0.03163242545777133\n",
      "epoch 5 batch count 800 , avg loss is 0.0316257695062086\n",
      "epoch 5 batch count 900 , avg loss is 0.031614058692422176\n",
      "epoch 5 batch count 1000 , avg loss is 0.03163254751823843\n",
      "epoch 5 batch count 1100 , avg loss is 0.03160954782231287\n",
      "epoch 5 batch count 1200 , avg loss is 0.031594708467212816\n",
      "epoch 5 batch count 1300 , avg loss is 0.03159778884970225\n",
      "epoch 5 batch count 1400 , avg loss is 0.031580950569893636\n",
      "epoch 5 batch count 1500 , avg loss is 0.03158903980255127\n",
      "epoch 5 batch count 1600 , avg loss is 0.03158958891406655\n",
      "epoch 5 batch count 1700 , avg loss is 0.03158744620707105\n",
      "epoch 5 batch count 1800 , avg loss is 0.03160045014590853\n",
      "epoch 6 batch count 100 , avg loss is 0.03160497264936566\n",
      "epoch 6 batch count 200 , avg loss is 0.03177965759299695\n",
      "epoch 6 batch count 300 , avg loss is 0.03166323285549879\n",
      "epoch 6 batch count 400 , avg loss is 0.031594223654828966\n",
      "epoch 6 batch count 500 , avg loss is 0.03158709134161472\n",
      "epoch 6 batch count 600 , avg loss is 0.03155399498840173\n",
      "epoch 6 batch count 700 , avg loss is 0.03156439204035061\n",
      "epoch 6 batch count 800 , avg loss is 0.031554746674373746\n",
      "epoch 6 batch count 900 , avg loss is 0.03152177426757084\n",
      "epoch 6 batch count 1000 , avg loss is 0.03151058809272945\n",
      "epoch 6 batch count 1100 , avg loss is 0.03153801364316182\n",
      "epoch 6 batch count 1200 , avg loss is 0.03154717506219944\n",
      "epoch 6 batch count 1300 , avg loss is 0.03155290976023445\n",
      "epoch 6 batch count 1400 , avg loss is 0.031563555071396486\n",
      "epoch 6 batch count 1500 , avg loss is 0.03152500316252311\n",
      "epoch 6 batch count 1600 , avg loss is 0.03152441367739812\n",
      "epoch 6 batch count 1700 , avg loss is 0.031521049637128325\n",
      "epoch 6 batch count 1800 , avg loss is 0.031523716126879056\n",
      "epoch 7 batch count 100 , avg loss is 0.03142745308578014\n",
      "epoch 7 batch count 200 , avg loss is 0.031628489093855024\n",
      "epoch 7 batch count 300 , avg loss is 0.03166111337641875\n",
      "epoch 7 batch count 400 , avg loss is 0.03165797984693199\n",
      "epoch 7 batch count 500 , avg loss is 0.03169800903648138\n",
      "epoch 7 batch count 600 , avg loss is 0.03163386683600644\n",
      "epoch 7 batch count 700 , avg loss is 0.031581955972526755\n",
      "epoch 7 batch count 800 , avg loss is 0.03157651856308803\n",
      "epoch 7 batch count 900 , avg loss is 0.031599707253691225\n",
      "epoch 7 batch count 1000 , avg loss is 0.03158622060529888\n",
      "epoch 7 batch count 1100 , avg loss is 0.0315637234564532\n",
      "epoch 7 batch count 1200 , avg loss is 0.03154395642224699\n",
      "epoch 7 batch count 1300 , avg loss is 0.031549086433190564\n",
      "epoch 7 batch count 1400 , avg loss is 0.03151639787746327\n",
      "epoch 7 batch count 1500 , avg loss is 0.03152776185423136\n",
      "epoch 7 batch count 1600 , avg loss is 0.031510991322575135\n",
      "epoch 7 batch count 1700 , avg loss is 0.03150899396223181\n",
      "epoch 7 batch count 1800 , avg loss is 0.03150391531280346\n",
      "epoch 8 batch count 100 , avg loss is 0.03148255547508597\n",
      "epoch 8 batch count 200 , avg loss is 0.031274257730692624\n",
      "epoch 8 batch count 300 , avg loss is 0.03140135110045473\n",
      "epoch 8 batch count 400 , avg loss is 0.03146726538427174\n",
      "epoch 8 batch count 500 , avg loss is 0.03141762725263834\n",
      "epoch 8 batch count 600 , avg loss is 0.031413008856276674\n",
      "epoch 8 batch count 700 , avg loss is 0.03141219601035118\n",
      "epoch 8 batch count 800 , avg loss is 0.03141191102797165\n",
      "epoch 8 batch count 900 , avg loss is 0.03141871490412288\n",
      "epoch 8 batch count 1000 , avg loss is 0.03144537186436355\n",
      "epoch 8 batch count 1100 , avg loss is 0.03143501994115385\n",
      "epoch 8 batch count 1200 , avg loss is 0.03142184128208707\n",
      "epoch 8 batch count 1300 , avg loss is 0.031404989887602056\n",
      "epoch 8 batch count 1400 , avg loss is 0.03139809252295111\n",
      "epoch 8 batch count 1500 , avg loss is 0.03140000977491339\n",
      "epoch 8 batch count 1600 , avg loss is 0.031399722426431256\n",
      "epoch 8 batch count 1700 , avg loss is 0.03139033410807743\n",
      "epoch 8 batch count 1800 , avg loss is 0.03138350494412912\n",
      "epoch 9 batch count 100 , avg loss is 0.031233705170452594\n",
      "epoch 9 batch count 200 , avg loss is 0.031360531263053416\n",
      "epoch 9 batch count 300 , avg loss is 0.031289479645589986\n",
      "epoch 9 batch count 400 , avg loss is 0.03128539389930665\n",
      "epoch 9 batch count 500 , avg loss is 0.0313140950910747\n",
      "epoch 9 batch count 600 , avg loss is 0.03133323911887904\n",
      "epoch 9 batch count 700 , avg loss is 0.03130828065531594\n",
      "epoch 9 batch count 800 , avg loss is 0.03132357108639553\n",
      "epoch 9 batch count 900 , avg loss is 0.03132126419287589\n",
      "epoch 9 batch count 1000 , avg loss is 0.03131743982248008\n",
      "epoch 9 batch count 1100 , avg loss is 0.03132140653207898\n",
      "epoch 9 batch count 1200 , avg loss is 0.03132004295475781\n",
      "epoch 9 batch count 1300 , avg loss is 0.031327193638739675\n",
      "epoch 9 batch count 1400 , avg loss is 0.031329927809004274\n",
      "epoch 9 batch count 1500 , avg loss is 0.031347030190130076\n",
      "epoch 9 batch count 1600 , avg loss is 0.031339145959354935\n",
      "epoch 9 batch count 1700 , avg loss is 0.031327332757851656\n",
      "epoch 9 batch count 1800 , avg loss is 0.031337543073006804\n",
      "epoch 10 batch count 100 , avg loss is 0.031450526341795924\n",
      "epoch 10 batch count 200 , avg loss is 0.03138168421573937\n",
      "epoch 10 batch count 300 , avg loss is 0.03134969253713886\n",
      "epoch 10 batch count 400 , avg loss is 0.03136922423262149\n",
      "epoch 10 batch count 500 , avg loss is 0.03133963224291801\n",
      "epoch 10 batch count 600 , avg loss is 0.031291450994710125\n",
      "epoch 10 batch count 700 , avg loss is 0.031276163091616974\n",
      "epoch 10 batch count 800 , avg loss is 0.03127675365889445\n",
      "epoch 10 batch count 900 , avg loss is 0.03127352235010929\n",
      "epoch 10 batch count 1000 , avg loss is 0.03125117452442646\n",
      "epoch 10 batch count 1100 , avg loss is 0.03126071563837203\n",
      "epoch 10 batch count 1200 , avg loss is 0.031284014489501716\n",
      "epoch 10 batch count 1300 , avg loss is 0.03126770833793741\n",
      "epoch 10 batch count 1400 , avg loss is 0.03125691304249423\n",
      "epoch 10 batch count 1500 , avg loss is 0.03124691555152337\n",
      "epoch 10 batch count 1600 , avg loss is 0.03125618041027337\n",
      "epoch 10 batch count 1700 , avg loss is 0.03126935118590208\n",
      "epoch 10 batch count 1800 , avg loss is 0.0312860079916815\n"
     ]
    }
   ],
   "source": [
    "vae_model.train()\n",
    "for i in range(10):\n",
    "    train(i+1)\n",
    "torch.save(vae_model.state_dict(),\"./model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6cc5198-82a2-47ad-bd43-479c327b24b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.89828485\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "z = torch.randn(1,128)\n",
    "z = z.cuda()\n",
    "vae_model.eval()\n",
    "samples = vae_model.decode(z)\n",
    "img = samples.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66d4e5c4-c35d-4064-989a-c5b00aa6b211",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f076c0b5-ff5c-40ad-8fe0-31515a0e04ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89828485"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af552ce9-5a34-4751-9fef-9c5f28712568",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_np = img.reshape(32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aec899c9-ad80-4b9d-95e0-fc9d5c3595ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f099998d-15b2-4eda-9bb2-4addae43eef4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAAAAABWESUoAAAEC0lEQVR4nAXBCTCcZwAG4Pf/dq0gG2HFXVtDXEPQqMSxBEPSpqKM7qhB44gcnSDtBKmUuFotiRhEutWYjQyZOEbiGOyq1rVEMcGMY11xLUlQ16KLv89DKQ0OnNDPjKAFohpFd+Qwdcy1ef/+X9VKrBv3GX2lBftEbsxFjgUbrXCH/4wLOBxA9KUa+EamSORyQTYiGIiveotF3Ua8zm6Foxzw693F5UgZxnalINp7bNSuKaG+iUbo9R0UGx8iPEiOvCB1+LHMQPjMNxAPG6LZRhUcPxX08dk43aMBzYVJ6EeNgQQu22NraAtLeu4QtfCp4BwDFDs7YzbADbHPdUDcW1ZxJVGB18I+pL6qoHOyBlDBfgM7635cau8G+cV6CQrZU+gGPkV9ZhsCfOcxvtuMxFvvgSEzMPNcgPZXWBi+4YZrR70QMucxuzJKrZ7f1UDu8zQGoU86gj4cQWzwKbyMDaGaP9hS/y3ZIUbwETYOWSBXkyWIcthGnOcs1lZK6WxBNa0eIIWSvzJOT54AYca4wb+fwtdzUzheNYb4Ii0sWk1ggL8Fl6x9EGllD654u8JJwwjLRSbQmdFC774+kucV6Ho0CZJ73hIJxUsY3OPhcea3FOuFO/XdhBM1XWsIxdI5kK76d0iZncGOVAAX8wf08s0ZeoDeoiV3DxD1VgayXcPEttU6eozk8DjoQcOpRiR/UokzBcMQWQAkbhL49KI3tDMcsd5wDtErNui7bgrzepND56Cue+RuWSeOlztS0f+OUqEcCSUpLKEcdVTQNuzLiPLKTyWVa0Kq6zYHyz9nwvZxA0aVa1F4YAzL22308OftNDPDXzIq1b9Kd4lT6O/LVulf/UX/yITqyGv6AM8zamB6xAyyhHyDbUVaIhTaOTBnVH5cyd5MC5Mc5rfrNLFxwcqT175YyGuq6OS9sCRubcxxntxhiHezMZjHj+fwoKod57oseOayGWckvvNTpNjO5Ky4SFVLbHVyX5Q0pSKmAi76jCZEmpL8hx169hPjepoxqTSPLjUuCJFZJnsEy5g51/SZ3uHdB75yQCE8C+uWkme3DHxCWnUfpH0V48oiT4zG0/OK9dN7D3zTj6lHZZxbCGPoPtwnm70a1L1HbsDI0fe87DIWj+c+4FLX8Y1YV+ojztfeE1UZLoiilVdEVNh61nxJeTP4Au76QfzvG9Nc7twPF7wUoYF2XMFvU2tEOiaBzZYc0u15NO2MIamhM1VYMptmf2cy3SJBO51ZW52rGa3yx3qNbc581vSf01reZYy/HarItKyDuCo5mTIlPcI4Wk7od8F1l7mMOstylUucJEM7pzhZfc/LhfZJ5hemZj+2TjzpUwvfSi6WCdjlxf3jn61FjLBTU47Mex/B/4f8xuaij9UVAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=32x32>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Image.fromarray(img_np,mode='L'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42233fac-eb19-4352-904f-a6df0e539248",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalle2",
   "language": "python",
   "name": "dalle2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
