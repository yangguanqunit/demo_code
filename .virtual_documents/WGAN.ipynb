import os,sys
import time
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.autograd as autograd
import torch.optim as optim
from torchsummary import summary


device = 0 if torch.cuda.is_available() else "cpu"
device = "cpu"


DIM = 64
BATCH_SIZE = 50
CRITIC_ITERS = 5
LAMBDA = 5
ITERS = 200000
OUTPUT_DIM = 784

# dataset_name = "mnist"
dataset_name = "fashion_mnist" # 一个时装数据集，以灰度图的形式保存了各种时装的照片


import torchvision
from torchvision import transforms
from torch.utils.data import DataLoader
from torchvision.transforms import Compose
from datasets import load_dataset
from torchvision import datasets

transformed_dataset = dict()

def transforms_(examples):
    examples["pixel_values"] = [transform(image.convert("L")) for image in examples["image"]]
    del examples["image"]
    return examples

try:
    dataset = load_dataset(dataset_name)
except Exception as e:
    print(e)
    
transform = Compose([
transforms.RandomHorizontalFlip(),
transforms.ToTensor(),
])
transformed_dataset = dataset.with_transform(transforms_).remove_columns("label")
    
train_loader = DataLoader(transformed_dataset['train'], batch_size=BATCH_SIZE, shuffle=True)


import matplotlib.pyplot as plt
sample = next(iter(train_loader))['pixel_values'].data.numpy()
plt.imshow(sample[0,0,:,:], cmap="gray")
plt.show()


#构建生成器模型
class Generator(nn.Module):
    
    def __init__(self):
        super(Generator, self).__init__()
        
        self.preprocess = nn.Sequential(
            nn.Linear(128, 4*4*4*DIM),
            nn.ReLU(True),
        )
        self.block1 = nn.Sequential(
            nn.ConvTranspose2d(4*DIM, 2*DIM, 5),
            nn.ReLU(True),
        )
        self.block2 = nn.Sequential(
            nn.ConvTranspose2d(2*DIM, DIM, 5),
            nn.ReLU(True),
        )
        self.deconv_out = nn.ConvTranspose2d(DIM, 1, 8, stride=2)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self, input):
        output = self.preprocess(input)
        output = output.view(-1, 4*DIM, 4 , 4)
        output = self.block1(output)
        # output = output[:, :, :7, :7]
        # 改成插值法进行特征裁剪效果差不多
        output = F.interpolate(output,[7,7])
        output = self.block2(output)
        output = self.deconv_out(output)
        output = self.sigmoid(output)
        return output.view(-1, OUTPUT_DIM)
    
generator = Generator().to(device)
summary(generator,(128,), device=device)


#构建判别器模型
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.main = nn.Sequential(
            nn.Conv2d(1, DIM, 5, stride=2, padding=2),
            nn.ReLU(True),
            nn.Conv2d(DIM, 2*DIM, 5, stride=2, padding=2),
            nn.ReLU(True),
            nn.Conv2d(2*DIM, 4*DIM, 5, stride=2, padding=2),
            nn.ReLU(True),
        )
        self.output = nn.Linear(4*4*4*DIM, 1)
    
    def forward(self, input):
        input = input.view(-1, 1, 28, 28)
        out = self.main(input)
        out = out.view(-1,4*4*4*DIM)
        out = self.output(out)
        return out.view(-1)

discriminator = Discriminator().to(device)
summary(discriminator, (1,784),device=device)



import numpy as np
import matplotlib.pyplot as plt
def generate_image(frame, net):
    noise = torch.randn(1, 128).to(device)
    with torch.no_grad():
        sample = net(noise)
    sample = sample.view(1, 28, 28).squeeze(0)
    sample = sample.cpu().data.numpy()

    plt.imshow(sample, cmap='gray')
    plt.show()

    
def calc_gradient_penalty(netD, real_data, fake_data):
    alpha = torch.rand(BATCH_SIZE, 1)
    alpha = alpha.expand(real_data.size()).to(device)
    interpolates = alpha * real_data + (1-alpha) * fake_data
    interpolates.requires_grad = True
    disc_interpolates = netD(interpolates)
    gradients = autograd.grad(disc_interpolates, interpolates, 
                             grad_outputs=torch.ones(disc_interpolates.size()).to(device),
                             create_graph=True, retain_graph=True, only_inputs=True)[0]
    
    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() * LAMBDA
    return gradient_penalty


netG = Generator().to(device)
netD = Discriminator().to(device)

optimizerD = optim.RMSprop(netD.parameters(), lr=1e-4)
optimizerG = optim.RMSprop(netG.parameters(), lr=1e-4)

for iteration in range(ITERS):
    netD.train()
    for iter_d in range(CRITIC_ITERS):
        optimizerD.zero_grad()
        real_data = next(iter(train_loader))['pixel_values'].view((-1, 784)).to(device)
        # print(real_data['pixel_values'].shape)
        D_real = netD(real_data)
        D_real = -D_real.mean()
        D_real.backward()

        noise = torch.randn(BATCH_SIZE, 128).to(device)
        with torch.no_grad():
            fake_data = netG(noise)
        D_fake = netD(fake_data)
        D_fake = D_fake.mean()
        D_fake.backward()

        gradient_penalty = calc_gradient_penalty(netD, real_data.data, fake_data.data)
        gradient_penalty.backward()

        D_cost = D_fake + D_real + gradient_penalty
        Wasserstein_D = D_real - D_fake
        optimizerD.step()
    optimizerG.zero_grad()
    noise = torch.randn(BATCH_SIZE, 128).to(device)
    fake = netG(noise)
    netD.eval()
    G_loss = netD(fake)
    G_loss = -G_loss.mean()
    G_loss.backward()
    optimizerG.step()
    
    
    if iteration % 100 == 99:
        print(f"G_loss is {G_loss.item()}")
        print(f"D_loss is {D_cost.item()}")
        
        generate_image(iteration, netG)



