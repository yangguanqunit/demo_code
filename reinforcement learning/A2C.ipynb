{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89871d9f-c6de-488c-8e59-74660ff43dc9",
   "metadata": {},
   "source": [
    "# **一个Advantage Actor-Critic算法的简单实现**\n",
    "## **算法概述**\n",
    "- AC算法的改进版，将TD_error改成了Advantage;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bfffcf1c-655c-40ca-81cb-d4091d7db7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import platform\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3df9b5ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if platform.system() == \"Darwin\":\n",
    "    PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "value_method = \"Advantage\"\n",
    "all_value_method = [\"Q_value\",\"V_value\",\"TD_error\",\"Advantage\"]\n",
    "assert value_method in all_value_method, \"You choose a wrong value_method!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "93546163-3b36-42b0-863f-a84c7b787f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CTrainer(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(A2CTrainer, self).__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.create_training_network()\n",
    "        self.create_training_method()\n",
    "        self.GAMMA = 0.9\n",
    "        self.to(device)\n",
    "\n",
    "        self.state_batch = []\n",
    "        self.action_batch = []\n",
    "        self.reward_batch = []\n",
    "        self.next_state_batch = []\n",
    "\n",
    "    def create_training_network(self):\n",
    "        self.fc = nn.Linear(self.state_dim, 20)\n",
    "        self.critic = nn.Sequential(self.fc,nn.ReLU(),nn.Linear(20,1))\n",
    "        self.actor = nn.Sequential(self.fc,nn.ReLU(),nn.Linear(20, self.action_dim))\n",
    "\n",
    "    def create_training_method(self):\n",
    "        self.optim = optim.Adam(self.parameters(),lr=0.01)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, device=device)\n",
    "            action_probs = F.softmax(self.actor(state), dim=-1)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "            return action\n",
    "\n",
    "    def calculate_batch_advantage(self, state_batch:torch.Tensor, reward_batch:torch.Tensor, done:bool):\n",
    "        value_batch = self.critic(state_batch)\n",
    "        values = value_batch[:-1,:].squeeze(-1)\n",
    "        next_values = value_batch[1:,:].squeeze(-1)\n",
    "        if done:\n",
    "            next_values[-1] = 0.\n",
    "\n",
    "        count = next_values.shape[0]\n",
    "        q_eval_list = [next_values[-1]]\n",
    "        for idx in range(count):\n",
    "            q_eval = reward_batch[-(idx+1)] + self.GAMMA * q_eval_list[idx]\n",
    "            q_eval_list.append(q_eval)\n",
    "        q_eval_list.reverse()\n",
    "        q_evals = torch.tensor(q_eval_list[1:], device=device)\n",
    "        advantage = q_evals - values\n",
    "        return advantage\n",
    "\n",
    "    def calculate_policy_loss(self, state_batch, action_batch, advantages):\n",
    "        action_logits_batch = self.actor(state_batch)\n",
    "        log_probs = torch.log(F.softmax(action_logits_batch, dim=-1))\n",
    "        action_log_probs = torch.gather(log_probs,1,action_batch.unsqueeze(-1)).squeeze(-1)\n",
    "        policy_loss = action_log_probs * advantages\n",
    "        return policy_loss\n",
    "\n",
    "    def perceive(self, state, action, reward, next_state):\n",
    "        if len(self.state_batch) != 0:\n",
    "            self.state_batch.pop(-1)\n",
    "        self.state_batch.append(state)\n",
    "        self.action_batch.append(action)\n",
    "        self.reward_batch.append(reward)\n",
    "        self.state_batch.append(next_state)\n",
    "        \n",
    "    def train_loop(self, done):\n",
    "        state_batch = torch.tensor(self.state_batch, device=device)\n",
    "        action_batch = torch.tensor(self.action_batch, device=device)\n",
    "        reward_batch = torch.tensor(self.reward_batch, device=device)\n",
    "\n",
    "        advantage = self.calculate_batch_advantage(state_batch, reward_batch, done)\n",
    "        value_loss = torch.square(advantage).mean()\n",
    "        policy_loss = self.calculate_policy_loss(state_batch, action_batch, advantage.detach()).mean()\n",
    "        loss = value_loss - policy_loss\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        \n",
    "    def clear_list(self):\n",
    "        self.state_batch.clear()\n",
    "        self.action_batch.clear()\n",
    "        self.reward_batch.clear()\n",
    "        self.next_state_batch.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24dc855e-bd20-406f-9fd1-41ef90ab5aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "agent = A2CTrainer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1083f4b-b438-469a-a14d-795aaca45cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Start training...\")\n",
    "    for episode in range(3000):\n",
    "        state, _ = env.reset()\n",
    "        for step in range(300):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = -1 if done else 0.01\n",
    "            agent.perceive(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        agent.train_loop(done)\n",
    "        agent.clear_list()\n",
    "        if episode % 100 == 0 and episode != 0:\n",
    "            total_reward = 0\n",
    "            for i in range(10):\n",
    "                state, _ = env.reset()\n",
    "                for step in range(300):\n",
    "                    action = agent.choose_action(state)\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "            print(f\"episode {episode} total reward is {total_reward/10}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"total time is {end_time - start_time}\")\n",
    "    print(\"End training.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5811bae0-61a6-4e5c-991b-b3036b3c2edb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "episode 100 total reward is 78.4\n",
      "episode 200 total reward is 264.4\n",
      "episode 300 total reward is 241.2\n",
      "episode 400 total reward is 245.6\n",
      "episode 500 total reward is 263.5\n",
      "episode 600 total reward is 242.6\n",
      "episode 700 total reward is 244.4\n",
      "episode 800 total reward is 256.9\n",
      "episode 900 total reward is 263.1\n",
      "episode 1000 total reward is 239.0\n",
      "episode 1100 total reward is 226.3\n",
      "episode 1200 total reward is 226.8\n",
      "episode 1300 total reward is 243.4\n",
      "episode 1400 total reward is 300.0\n",
      "episode 1500 total reward is 290.8\n",
      "episode 1600 total reward is 283.4\n",
      "episode 1700 total reward is 291.6\n",
      "episode 1800 total reward is 280.3\n",
      "episode 1900 total reward is 292.7\n",
      "episode 2000 total reward is 300.0\n",
      "episode 2100 total reward is 298.3\n",
      "episode 2200 total reward is 281.4\n",
      "episode 2300 total reward is 295.4\n",
      "episode 2400 total reward is 300.0\n",
      "episode 2500 total reward is 300.0\n",
      "episode 2600 total reward is 256.3\n",
      "episode 2700 total reward is 192.9\n",
      "episode 2800 total reward is 278.8\n",
      "episode 2900 total reward is 238.7\n",
      "total time is 553.5409708023071\n",
      "End training.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f0df5",
   "metadata": {},
   "source": [
    "## 实验记录\n",
    "2023-3-7 \\\n",
    "~~1、将TD_errors改成多步的advantage。 \\\n",
    "结果：效果不太行，训不起来。~~ \\\n",
    "2023-3-8 \\\n",
    "2、上述情况是因为advantage取了绝对值导致的，把绝对值取消。 \\\n",
    "结果：效果有了大幅度提升，但稳定性相对较差。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
