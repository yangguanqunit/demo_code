{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89871d9f-c6de-488c-8e59-74660ff43dc9",
   "metadata": {},
   "source": [
    "# **一个Actor-Critic算法的（批处理方式）简单实现**\n",
    "## **算法概述**\n",
    "- 采用批输入数据的方式加速训练;\n",
    "- 批输入的数据包括TD误差以及log概率;\n",
    "- 采用TD误差进行批处理训练过程及其不稳定，很难训起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bfffcf1c-655c-40ca-81cb-d4091d7db7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import platform\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3df9b5ac",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if platform.system() == \"Darwin\":\n",
    "    PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "value_method = \"TD_error\"\n",
    "all_value_method = [\"Q_value\",\"V_value\",\"TD_error\",\"Advantage\"]\n",
    "assert value_method in all_value_method, \"You choose a wrong value_method!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93546163-3b36-42b0-863f-a84c7b787f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrainer(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(ActorCriticTrainer, self).__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.create_training_network()\n",
    "        self.create_training_method()\n",
    "        self.GAMMA = 0.9\n",
    "        self.to(device)\n",
    "\n",
    "        self.state_batch = []\n",
    "        self.action_batch = []\n",
    "        self.reward_batch = []\n",
    "        self.next_state_batch = []\n",
    "\n",
    "    def create_training_network(self):\n",
    "        self.fc = nn.Linear(self.state_dim, 20)\n",
    "        self.critic = nn.Sequential(self.fc,nn.ReLU(),nn.Linear(20,1))\n",
    "        self.actor = nn.Sequential(self.fc,nn.ReLU(),nn.Linear(20, self.action_dim))\n",
    "\n",
    "    def create_training_method(self):\n",
    "        self.optim = optim.Adam(self.parameters(),lr=0.001)\n",
    "        # self.value_loss = nn.MSELoss()\n",
    "        # self.actor_loss = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, device=device)\n",
    "            action_probs = F.softmax(self.actor(state), dim=-1)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "            return action\n",
    "\n",
    "    def calculate_batch_td_error(self, state_batch:torch.Tensor, reward_batch:torch.Tensor):\n",
    "        value_batch = self.critic(state_batch)\n",
    "        values = copy.copy(value_batch).squeeze(-1)\n",
    "        next_values = torch.cat((copy.copy(value_batch[1:,:]).squeeze(-1), torch.tensor([0.], device=device)))\n",
    "        td_errors = reward_batch + self.GAMMA * next_values - values\n",
    "        return td_errors\n",
    "\n",
    "    def calculate_policy_loss(self, state_batch, action_batch, td_errors):\n",
    "        action_logits_batch = self.actor(state_batch)\n",
    "        log_probs = torch.log(F.softmax(action_logits_batch, dim=-1))\n",
    "        action_log_probs = torch.gather(log_probs,1,action_batch.unsqueeze(-1)).squeeze(-1)\n",
    "        policy_loss = action_log_probs * torch.abs(td_errors)\n",
    "        return policy_loss\n",
    "\n",
    "    def perceive(self, state, action, reward, next_state):\n",
    "        self.state_batch.append(state)\n",
    "        self.action_batch.append(action)\n",
    "        self.reward_batch.append(reward)\n",
    "        self.next_state_batch.append(next_state)\n",
    "\n",
    "    def train_loop(self):\n",
    "        state_batch = torch.tensor(self.state_batch, device=device)\n",
    "        action_batch = torch.tensor(self.action_batch, device=device)\n",
    "        reward_batch = torch.tensor(self.reward_batch, device=device)\n",
    "\n",
    "        td_errors = self.calculate_batch_td_error(state_batch, reward_batch)\n",
    "        value_loss = torch.square(td_errors).mean()\n",
    "        policy_loss = self.calculate_policy_loss(state_batch, action_batch, td_errors.detach()).mean()\n",
    "        loss = value_loss - policy_loss\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        \n",
    "    def clear_list(self):\n",
    "        self.state_batch.clear()\n",
    "        self.action_batch.clear()\n",
    "        self.reward_batch.clear()\n",
    "        self.next_state_batch.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24dc855e-bd20-406f-9fd1-41ef90ab5aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "agent = ActorCriticTrainer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1083f4b-b438-469a-a14d-795aaca45cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    for episode in range(3000):\n",
    "        state, _ = env.reset()\n",
    "        for step in range(300):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = -1 if done else 0.01\n",
    "            agent.perceive(state,action,reward,next_state)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                agent.train_loop()\n",
    "                agent.clear_list()\n",
    "                break\n",
    "        if episode % 100 == 0 and episode != 0:\n",
    "            total_reward = 0\n",
    "            for i in range(10):\n",
    "                state, _ = env.reset()\n",
    "                for step in range(300):\n",
    "                    action = agent.choose_action(state)\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "            print(f\"episode {episode} total reward is {total_reward/10}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"total time is {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5811bae0-61a6-4e5c-991b-b3036b3c2edb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 100 total reward is 20.5\n",
      "episode 200 total reward is 19.3\n",
      "episode 300 total reward is 22.2\n",
      "episode 400 total reward is 28.2\n",
      "episode 500 total reward is 17.3\n",
      "episode 600 total reward is 21.7\n",
      "episode 700 total reward is 29.0\n",
      "episode 800 total reward is 25.2\n",
      "episode 900 total reward is 26.1\n",
      "episode 1000 total reward is 21.1\n",
      "episode 1100 total reward is 37.1\n",
      "episode 1200 total reward is 36.1\n",
      "episode 1300 total reward is 31.0\n",
      "episode 1400 total reward is 27.0\n",
      "episode 1500 total reward is 39.0\n",
      "episode 1600 total reward is 36.1\n",
      "episode 1700 total reward is 34.4\n",
      "episode 1800 total reward is 28.9\n",
      "episode 1900 total reward is 30.1\n",
      "episode 2000 total reward is 40.5\n",
      "episode 2100 total reward is 43.1\n",
      "episode 2200 total reward is 42.4\n",
      "episode 2300 total reward is 34.4\n",
      "episode 2400 total reward is 26.8\n",
      "episode 2500 total reward is 24.6\n",
      "episode 2600 total reward is 26.0\n",
      "episode 2700 total reward is 28.4\n",
      "episode 2800 total reward is 27.7\n",
      "episode 2900 total reward is 20.4\n",
      "total time is 381.3641998767853\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 实验记录\n",
    "1、将td_errors取了绝对值。\n",
    "结果：效果好了一些，因为td_errors有正有负，如果直接乘log_probs再取绝对值的话loss值会抵消，造成训练缓慢的问题。"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63d75185-faba-48b9-b578-4665456179e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
