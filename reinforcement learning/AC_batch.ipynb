{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89871d9f-c6de-488c-8e59-74660ff43dc9",
   "metadata": {},
   "source": [
    "# 一个A3C算法的简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfffcf1c-655c-40ca-81cb-d4091d7db7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import platform\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3df9b5ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if platform.system() == \"Darwin\":\n",
    "    PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93546163-3b36-42b0-863f-a84c7b787f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrainer(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(ActorCriticTrainer, self).__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.create_training_network()\n",
    "        self.create_training_method()\n",
    "        self.GAMMA = 0.9\n",
    "        self.to(device)\n",
    "\n",
    "        self.state_batch = []\n",
    "        self.action_batch = []\n",
    "        self.reward_batch = []\n",
    "        self.next_state_batch = []\n",
    "\n",
    "    def create_training_network(self):\n",
    "        self.fc = nn.Linear(self.state_dim, 20)\n",
    "        self.critic = nn.Sequential(self.fc,nn.ReLU(),nn.Linear(20,1))\n",
    "        self.actor = nn.Sequential(self.fc,nn.ReLU(),nn.Linear(20, self.action_dim))\n",
    "\n",
    "    def create_training_method(self):\n",
    "        self.optim = optim.Adam(self.parameters(),lr=0.001)\n",
    "        self.value_loss = nn.MSELoss()\n",
    "        self.actor_loss = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, device=device)\n",
    "            action_probs = F.softmax(self.actor(state), dim=-1)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "            return action\n",
    "\n",
    "    def calculate_batch_td_error(self, state_batch, reward_batch):\n",
    "        value_batch = self.critic(state_batch)\n",
    "        values = copy.copy(value_batch).squeeze(-1)\n",
    "        next_values = torch.cat((copy.copy(value_batch[1:]).squeeze(-1), torch.tensor([0.], device=device)))\n",
    "        td_errors = reward_batch + self.GAMMA * next_values - values\n",
    "        return td_errors\n",
    "\n",
    "    def calculate_policy_loss(self, state_batch, action_batch, td_errors):\n",
    "        action_logits_batch = self.actor(state_batch)\n",
    "        log_probs = torch.log(F.softmax(action_logits_batch, dim=-1))\n",
    "        action_log_probs = torch.gather(log_probs,1,action_batch.unsqueeze(-1)).squeeze(-1)\n",
    "        return action_log_probs * td_errors\n",
    "\n",
    "    def perceive(self, state, action, reward, next_state):\n",
    "        self.state_batch.append(state)\n",
    "        self.action_batch.append(action)\n",
    "        self.reward_batch.append(reward)\n",
    "        self.next_state_batch.append(next_state)\n",
    "\n",
    "    def train_loop(self):\n",
    "        state_batch = torch.tensor(self.state_batch, device=device)\n",
    "        action_batch = torch.tensor(self.action_batch, device=device)\n",
    "        reward_batch = torch.tensor(self.reward_batch, device=device)\n",
    "\n",
    "        td_errors = self.calculate_batch_td_error(state_batch, reward_batch)\n",
    "        value_loss = torch.square(td_errors).mean()\n",
    "        policy_loss = self.calculate_policy_loss(state_batch, action_batch, td_errors.detach()).mean()\n",
    "        loss = value_loss - policy_loss\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        \n",
    "    def clear_list(self):\n",
    "        self.state_batch.clear()\n",
    "        self.action_batch.clear()\n",
    "        self.reward_batch.clear()\n",
    "        self.next_state_batch.clear()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24dc855e-bd20-406f-9fd1-41ef90ab5aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "agent = ActorCriticTrainer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1083f4b-b438-469a-a14d-795aaca45cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    for episode in range(3000):\n",
    "        state, _ = env.reset()\n",
    "        for step in range(300):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = -1 if done else 0.01\n",
    "            # agent.train_loop(state, action, reward, next_state)\n",
    "            agent.perceive(state,action,reward,next_state)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                agent.train_loop()\n",
    "                agent.clear_list()\n",
    "                break\n",
    "        if episode % 100 == 0:\n",
    "            total_reward = 0\n",
    "            for i in range(10):\n",
    "                state, _ = env.reset()\n",
    "                for step in range(300):\n",
    "                    action = agent.choose_action(state)\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "            print(f\"episode {episode} total reward is {total_reward/10}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"total time is {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5811bae0-61a6-4e5c-991b-b3036b3c2edb",
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/k0d9hm015g76gx4pfq9dvz_40000gn/T/ipykernel_1781/2822431757.py:53: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525474122/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  state_batch = torch.tensor(self.state_batch, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 total reward is 22.7\n",
      "episode 100 total reward is 23.6\n",
      "episode 200 total reward is 17.9\n",
      "episode 300 total reward is 14.1\n",
      "episode 400 total reward is 12.9\n",
      "episode 500 total reward is 11.4\n",
      "episode 600 total reward is 11.2\n",
      "episode 700 total reward is 10.1\n",
      "episode 800 total reward is 10.2\n",
      "episode 900 total reward is 9.7\n",
      "episode 1000 total reward is 9.7\n",
      "episode 1100 total reward is 9.3\n",
      "episode 1200 total reward is 9.7\n",
      "episode 1300 total reward is 9.2\n",
      "episode 1400 total reward is 9.7\n",
      "episode 1500 total reward is 9.2\n",
      "episode 1600 total reward is 9.4\n",
      "episode 1700 total reward is 9.4\n",
      "episode 1800 total reward is 9.4\n",
      "episode 1900 total reward is 9.4\n",
      "episode 2000 total reward is 9.4\n",
      "episode 2100 total reward is 9.1\n",
      "episode 2200 total reward is 9.3\n",
      "episode 2300 total reward is 9.8\n",
      "episode 2400 total reward is 10.1\n",
      "episode 2500 total reward is 8.8\n",
      "episode 2600 total reward is 9.2\n",
      "episode 2700 total reward is 9.2\n",
      "episode 2800 total reward is 9.2\n",
      "episode 2900 total reward is 9.2\n",
      "total time is 160.47441911697388\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5431d3e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d75185-faba-48b9-b578-4665456179e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
