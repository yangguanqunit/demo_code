{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89871d9f-c6de-488c-8e59-74660ff43dc9",
   "metadata": {},
   "source": [
    "# **一个Actor-Critic算法的（批处理方式）简单实现**\n",
    "## **算法概述**\n",
    "- 采用批输入数据的方式加速训练;\n",
    "- 批输入的数据包括TD误差以及log概率;\n",
    "- 采用TD误差进行批处理训练过程及其不稳定，很难训起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bfffcf1c-655c-40ca-81cb-d4091d7db7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import platform\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3df9b5ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if platform.system() == \"Darwin\":\n",
    "    PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "value_method = \"TD_error\"\n",
    "all_value_method = [\"Q_value\",\"V_value\",\"TD_error\",\"Advantage\"]\n",
    "assert value_method in all_value_method, \"You choose a wrong value_method!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "93546163-3b36-42b0-863f-a84c7b787f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrainer(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(ActorCriticTrainer, self).__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.create_training_network()\n",
    "        self.create_training_method()\n",
    "        self.GAMMA = 0.9\n",
    "        self.to(device)\n",
    "\n",
    "        self.state_batch = []\n",
    "        self.action_batch = []\n",
    "        self.reward_batch = []\n",
    "        self.next_state_batch = []\n",
    "\n",
    "    def create_training_network(self):\n",
    "        self.fc = nn.Linear(self.state_dim, 20)\n",
    "        self.critic = nn.Sequential(self.fc,nn.ReLU(),nn.Linear(20,1))\n",
    "        self.actor = nn.Sequential(self.fc,nn.ReLU(),nn.Linear(20, self.action_dim))\n",
    "\n",
    "    def create_training_method(self):\n",
    "        self.optim = optim.Adam(self.parameters(),lr=0.001)\n",
    "        # self.value_loss = nn.MSELoss()\n",
    "        # self.actor_loss = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, device=device)\n",
    "            action_probs = F.softmax(self.actor(state), dim=-1)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "            return action\n",
    "\n",
    "    def calculate_batch_td_error(self, state_batch:torch.Tensor, reward_batch:torch.Tensor, done:bool):\n",
    "        value_batch = self.critic(state_batch)\n",
    "        values = value_batch[:-1,:].squeeze(-1)\n",
    "        next_values = value_batch[1:,:].squeeze(-1)\n",
    "        if done:\n",
    "            next_values[-1] = 0.\n",
    "        td_errors = reward_batch + self.GAMMA * next_values - values\n",
    "        return td_errors\n",
    "\n",
    "    def calculate_policy_loss(self, state_batch, action_batch, td_errors):\n",
    "        action_logits_batch = self.actor(state_batch)\n",
    "        log_probs = torch.log(F.softmax(action_logits_batch, dim=-1))\n",
    "        action_log_probs = torch.gather(log_probs,1,action_batch.unsqueeze(-1)).squeeze(-1)\n",
    "        policy_loss = action_log_probs * torch.abs(td_errors)\n",
    "        return policy_loss\n",
    "    \n",
    "    def calculate_batch_advantage(self, state_batch:torch.Tensor, reward_batch:torch.Tensor):\n",
    "        value_batch = self.critic(state_batch)\n",
    "        pass\n",
    "\n",
    "    def perceive(self, state, action, reward, next_state):\n",
    "        if len(self.state_batch) != 0:\n",
    "            self.state_batch.pop(-1)\n",
    "        self.state_batch.append(state)\n",
    "        self.action_batch.append(action)\n",
    "        self.reward_batch.append(reward)\n",
    "        self.state_batch.append(next_state)\n",
    "        \n",
    "    def train_loop(self, done):\n",
    "        state_batch = torch.tensor(self.state_batch, device=device)\n",
    "        action_batch = torch.tensor(self.action_batch, device=device)\n",
    "        reward_batch = torch.tensor(self.reward_batch, device=device)\n",
    "\n",
    "        td_errors = self.calculate_batch_td_error(state_batch, reward_batch, done)\n",
    "        value_loss = torch.square(td_errors).mean()\n",
    "        policy_loss = self.calculate_policy_loss(state_batch, action_batch, td_errors.detach()).mean()\n",
    "        loss = value_loss - policy_loss\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        \n",
    "    def clear_list(self):\n",
    "        self.state_batch.clear()\n",
    "        self.action_batch.clear()\n",
    "        self.reward_batch.clear()\n",
    "        self.next_state_batch.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "24dc855e-bd20-406f-9fd1-41ef90ab5aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "agent = ActorCriticTrainer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f1083f4b-b438-469a-a14d-795aaca45cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "    for episode in range(3000):\n",
    "        state, _ = env.reset()\n",
    "        for step in range(300):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = -1 if done else 0.01\n",
    "            agent.perceive(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        agent.train_loop(done)\n",
    "        agent.clear_list()\n",
    "        if episode % 100 == 0 and episode != 0:\n",
    "            total_reward = 0\n",
    "            for i in range(10):\n",
    "                state, _ = env.reset()\n",
    "                for step in range(300):\n",
    "                    action = agent.choose_action(state)\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "            print(f\"episode {episode} total reward is {total_reward/10}\")\n",
    "    end_time = time.time()\n",
    "    print(f\"total time is {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5811bae0-61a6-4e5c-991b-b3036b3c2edb",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 100 total reward is 22.0\n",
      "episode 200 total reward is 26.4\n",
      "episode 300 total reward is 40.4\n",
      "episode 400 total reward is 42.7\n",
      "episode 500 total reward is 54.3\n",
      "episode 600 total reward is 71.8\n",
      "episode 700 total reward is 56.1\n",
      "episode 800 total reward is 61.2\n",
      "episode 900 total reward is 56.8\n",
      "episode 1000 total reward is 72.9\n",
      "episode 1100 total reward is 62.8\n",
      "episode 1200 total reward is 73.8\n",
      "episode 1300 total reward is 74.8\n",
      "episode 1400 total reward is 86.3\n",
      "episode 1500 total reward is 130.1\n",
      "episode 1600 total reward is 140.4\n",
      "episode 1700 total reward is 102.8\n",
      "episode 1800 total reward is 227.9\n",
      "episode 1900 total reward is 173.8\n",
      "episode 2000 total reward is 181.9\n",
      "episode 2100 total reward is 155.2\n",
      "episode 2200 total reward is 180.2\n",
      "episode 2300 total reward is 219.4\n",
      "episode 2400 total reward is 235.9\n",
      "episode 2500 total reward is 250.9\n",
      "episode 2600 total reward is 249.8\n",
      "episode 2700 total reward is 272.9\n",
      "episode 2800 total reward is 245.1\n",
      "episode 2900 total reward is 169.6\n",
      "total time is 260.9212920665741\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f0df5",
   "metadata": {},
   "source": [
    "## 实验记录\n",
    "~~2023-3-6 \\\n",
    "1、将td_errors取了绝对值。 \\\n",
    "结果：效果好了一些，因为td_errors有正有负，如果直接乘log_probs再取绝对值的话loss值会抵消，造成训练缓慢的问题。 \\\n",
    "假设：因为变成batch后，损失是取了平均值，但是损失大小没有改变，所以在这里取batch是无效的。 \\\n",
    "2023-3-7 \\\n",
    "2、在以上基础上将epoch调大了一些。 \\\n",
    "结果：确实能取得更好的效果，但是训练时间也随着变成，感觉使用TD_errors进行批处理效果不太行。 (调学习率有一点点作用)\\\n",
    "假设：在其他部分（模型、参数等）不变的情况下，换一种价值计算方式。~~\\\n",
    "3、以上实验都是将train_loop放在了if done中，相当于agent只学到了以done方式结束的trajectories，因此效果不好。\\\n",
    "结果：将train_loop放在step循坏外，训练效果有了大幅度提升。 \\\n",
    "假设：在其他部分（模型、参数等）不变的情况下，换一种价值计算方式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c104234c-7785-448e-ad30-43377a872801",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalle2",
   "language": "python",
   "name": "dalle2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
