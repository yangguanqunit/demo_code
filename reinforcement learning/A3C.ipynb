{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14bf3487",
   "metadata": {},
   "source": [
    "# **一个Asynchronous Advantage Actor-Critic算法的简单实现**\n",
    "## **算法概述**\n",
    "- 基础AC算法的分布式版本\n",
    "- 分为行为策略和目标策略，行为策略用于与环境进行交互进行环境探索，目标策略用于根据行为策略的探索结果进行模型学习迭代;\n",
    "- 一个比较典型的A3C范式是，单个目标策略+多个行为策略;\n",
    "- 行为策略会根据收集到的数据进行loss和梯度的计算，不同行为策略计算得到的梯度会异步的传输给目标策略;\n",
    "- 目标策略利用得到的梯度进行单步梯度下降，训练后的目标策略模型会定期同步给行为策略，进行新的探索。\n",
    "\n",
    "论文链接：*https://docs.popo.netease.com/docs/26e9fa439e2b46dcad6374a043725c4a*# (AC算法的理论基础)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aad9448",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59d7e1d-b0a7-4dd9-a102-90848f454f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.system == \"Darwin\":\n",
    "    PYTORCH_ENABLE_MPS_FALLBACK = 1\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05641c2a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class A3CTrainer(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(A3CTrainer, self).__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        \n",
    "        self.create_training_network()\n",
    "        self.create_training_method()\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "    def create_training_network(self):\n",
    "        self.fc = nn.Linear(self.state_dim, 20)\n",
    "        self.value = nn.Sequential(self.fc, nn.ReLU(), nn.Linear(20,1))\n",
    "        self.actor = nn.Sequential(self.fc, nn.ReLU(), nn.Linear(20, self.action_dim))\n",
    "\n",
    "    def create_training_method(self):\n",
    "        self.optim = optim.Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            action_logits = self.actor(state)\n",
    "            action_probs = F.softmax(action_logits, dim=-1)\n",
    "            action = torch.multinomial(action_probs,1).item()\n",
    "            return action, action_probs[action]\n",
    "    \n",
    "    def sync_grad(self, policy_agent:nn.Module):\n",
    "        for behavior_param, target_param in zip(policy_agent.parameters(), self.parameters()):\n",
    "            if target_param.grad is not None:\n",
    "                break\n",
    "            target_param.grad = behavior_param.grad\n",
    "    \n",
    "    def update_model(self):\n",
    "        self.optim.step()\n",
    "        self.optim.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "202b2581-4747-4d9c-87c3-fc708bc7c2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PolicyAgent(nn.Module):\n",
    "    GAMMA = 0.9\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super(PolicyAgent, self).__init__()\n",
    "\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.create_policy_network()\n",
    "        self.create_policy_method()\n",
    "        \n",
    "        self.state_batch = []\n",
    "        self.action_batch = []\n",
    "        self.reward_batch = []\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def create_policy_network(self):\n",
    "        self.fc = nn.Linear(self.state_dim, 20)\n",
    "        self.critic = nn.Sequential(self.fc, nn.ReLU(), nn.Linear(20,1))\n",
    "        self.actor = nn.Sequential(self.fc, nn.ReLU(), nn.Linear(20, self.action_dim))\n",
    "        \n",
    "    def create_policy_method(self):\n",
    "        self.optim = optim.Adam(self.parameters(), lr = 0.001)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, device=device)\n",
    "            action_logits = self.actor(state)\n",
    "            action_probs = F.softmax(action_logits,dim=-1)\n",
    "            action = torch.multinomial(action_probs, 1)\n",
    "            return action.item()\n",
    "\n",
    "    def calculate_batch_td_error(self, state_batch:torch.Tensor, reward_batch:torch.Tensor, done:bool):\n",
    "        value_batch = self.critic(state_batch)\n",
    "        values = value_batch[:-1,:].squeeze(-1)\n",
    "        next_values = value_batch[1:,:].squeeze(-1)\n",
    "        if done:\n",
    "            next_values[-1] = 0.\n",
    "        td_errors = reward_batch + self.GAMMA * next_values - values\n",
    "        return td_errors\n",
    "    \n",
    "    def calculate_policy_loss(self, state_batch, action_batch, td_errors):\n",
    "        action_logits_batch = self.actor(state_batch)\n",
    "        log_probs = torch.log(F.softmax(action_logits_batch, dim=-1))\n",
    "        action_log_probs = torch.gather(log_probs,1,action_batch.unsqueeze(-1)).squeeze(-1)\n",
    "        policy_loss = action_log_probs * td_errors\n",
    "        return policy_loss\n",
    "        \n",
    "    def perceive(self, state, action, reward, next_state):\n",
    "        if len(self.state_batch) != 0:\n",
    "            self.state_batch.pop(-1)\n",
    "        self.state_batch.append(state)\n",
    "        self.action_batch.append(action)\n",
    "        self.reward_batch.append(reward)\n",
    "        self.state_batch.append(next_state)\n",
    "    \n",
    "    def calculate_grad(self, done):\n",
    "        self.optim.zero_grad()\n",
    "        state_batch = torch.tensor(self.state_batch, device=device)\n",
    "        action_batch = torch.tensor(self.action_batch, device=device)\n",
    "        reward_batch = torch.tensor(self.reward_batch, device=device)\n",
    "        \n",
    "        td_errors = self.calculate_batch_td_error(state_batch, reward_batch, done)\n",
    "        \n",
    "        value_loss = torch.square(td_errors).mean()\n",
    "        policy_loss = self.calculate_policy_loss(state_batch, action_batch, td_errors.detach()).mean()\n",
    "        loss = value_loss - policy_loss\n",
    "        loss.backward()\n",
    "        \n",
    "    \n",
    "    def sync_model(self,model_dict):\n",
    "        self.load_state_dict(model_dict, strict=False)\n",
    "        \n",
    "    def clear_list(self):\n",
    "        self.state_batch.clear()\n",
    "        self.action_batch.clear()\n",
    "        self.reward_batch.clear()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a834bf1f-45a8-4b86-a2d7-3ec5ea5f78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "policy_agent = PolicyAgent(env)\n",
    "target_agent = A3CTrainer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3da5fa99-7ff1-425f-88df-6ea0c44fc971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for episode in range(3000*2):\n",
    "        state, _ = env.reset()\n",
    "        for step in range(300):\n",
    "            action = policy_agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = -1 if done else 0.01\n",
    "            policy_agent.perceive(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        policy_agent.calculate_grad(done)\n",
    "        target_agent.sync_grad(policy_agent)\n",
    "        target_agent.update_model()\n",
    "        policy_agent.sync_model(target_agent.state_dict())\n",
    "        policy_agent.clear_list()\n",
    "        if episode % 100 == 0 and episode !=0:\n",
    "            total_reward = 0\n",
    "            for i in range(10):\n",
    "                state, _ = env.reset()\n",
    "                for step in range(300):\n",
    "                    action = policy_agent.choose_action(state)\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "            print(f\"episode {episode} total reward is {total_reward/10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2079c0-71d9-47e0-8bc5-07b0a3258881",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ygq/miniconda3/envs/my/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "/tmp/ipykernel_30262/1274476647.py:60: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
      "  state_batch = torch.tensor(self.state_batch, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 100 total reward is 23.0\n",
      "episode 200 total reward is 23.4\n",
      "episode 300 total reward is 27.3\n",
      "episode 400 total reward is 33.1\n",
      "episode 500 total reward is 27.0\n",
      "episode 600 total reward is 33.7\n",
      "episode 700 total reward is 31.5\n",
      "episode 800 total reward is 43.5\n",
      "episode 900 total reward is 50.8\n",
      "episode 1000 total reward is 45.6\n",
      "episode 1100 total reward is 52.5\n",
      "episode 1200 total reward is 57.6\n",
      "episode 1300 total reward is 69.5\n",
      "episode 1400 total reward is 67.0\n",
      "episode 1500 total reward is 61.2\n",
      "episode 1600 total reward is 69.7\n",
      "episode 1700 total reward is 66.3\n",
      "episode 1800 total reward is 76.1\n",
      "episode 1900 total reward is 64.8\n",
      "episode 2000 total reward is 83.0\n",
      "episode 2100 total reward is 81.0\n",
      "episode 2200 total reward is 104.3\n",
      "episode 2300 total reward is 79.9\n",
      "episode 2400 total reward is 97.3\n",
      "episode 2500 total reward is 98.2\n",
      "episode 2600 total reward is 91.9\n",
      "episode 2700 total reward is 115.5\n",
      "episode 2800 total reward is 124.6\n",
      "episode 2900 total reward is 97.9\n",
      "episode 3000 total reward is 119.7\n",
      "episode 3100 total reward is 67.7\n",
      "episode 3200 total reward is 78.8\n",
      "episode 3300 total reward is 111.3\n",
      "episode 3400 total reward is 163.2\n",
      "episode 3500 total reward is 162.8\n",
      "episode 3600 total reward is 172.4\n",
      "episode 3700 total reward is 168.4\n",
      "episode 3800 total reward is 149.4\n",
      "episode 3900 total reward is 205.8\n",
      "episode 4000 total reward is 182.1\n",
      "episode 4100 total reward is 186.2\n",
      "episode 4200 total reward is 168.3\n",
      "episode 4300 total reward is 163.7\n",
      "episode 4400 total reward is 174.1\n",
      "episode 4500 total reward is 152.5\n",
      "episode 4600 total reward is 162.3\n",
      "episode 4700 total reward is 176.9\n",
      "episode 4800 total reward is 184.2\n",
      "episode 4900 total reward is 196.5\n",
      "episode 5000 total reward is 196.9\n",
      "episode 5100 total reward is 209.6\n",
      "episode 5200 total reward is 167.2\n",
      "episode 5300 total reward is 170.2\n",
      "episode 5400 total reward is 170.4\n",
      "episode 5500 total reward is 199.2\n",
      "episode 5600 total reward is 241.0\n",
      "episode 5700 total reward is 260.5\n",
      "episode 5800 total reward is 278.0\n",
      "episode 5900 total reward is 288.8\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e515772-d45b-4657-8dd0-46d72f3acd91",
   "metadata": {},
   "source": [
    "## 实验记录 \n",
    "2023-3-8\n",
    "1. 实现了部分A3C算法，还有获取行为策略的梯度部分以及梯度同步、模型同步部分。 \\\n",
    "\n",
    "2023-3-9\n",
    "1. 实现了剩余的部分。 \\\n",
    "结果：一个行为策略效果还不错，但是发现m1芯片在小模型训练上表现奇差。在mac上要换成cpu训练。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b894d3-a5c9-4d4c-9610-dcbe67200d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e261e-8f94-4bb2-827d-fa7a5357fbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
