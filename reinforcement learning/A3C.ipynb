{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14bf3487",
   "metadata": {},
   "source": [
    "# **一个Advantage Actor-Critic算法的简单实现**\n",
    "## **算法概述**\n",
    "- 采用批输入数据的方式加速训练;\n",
    "- 批输入的数据包括TD误差以及log概率;\n",
    "- 采用TD误差进行批处理训练过程及其不稳定，很难训起来。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aad9448",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e59d7e1d-b0a7-4dd9-a102-90848f454f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.system == \"Darwin\":\n",
    "    PYTORCH_ENABLE_MPS_FALLBACK = 1\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05641c2a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class A3CTrainer(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(A3CTrainer, self).__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        \n",
    "        self.create_training_network()\n",
    "        self.create_training_method()\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "    def create_training_network(self):\n",
    "        self.fc = nn.Linear(self.state_dim, 20)\n",
    "        self.value = nn.Sequential(self.fc, nn.ReLU(), nn.Linear(20,1))\n",
    "        self.actor = nn.Sequential(self.fc, nn.ReLU(), nn.Linear(20, self.action_dim))\n",
    "\n",
    "    def create_training_method(self):\n",
    "        self.optim = optim.Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            action_logits = self.actor(state)\n",
    "            action_probs = F.softmax(action_logits, dim=-1)\n",
    "            action = torch.multinomial(action_probs,1).item()\n",
    "            return action, action_probs[action]\n",
    "    \n",
    "    def sync_grad(self, policy_agent:nn.Module):\n",
    "        for behavior_param, target_param in zip(policy_agent.parameters(), self.parameters()):\n",
    "            if target_param.grad is not None:\n",
    "                break\n",
    "            target_param.grad = behavior_param.grad\n",
    "    \n",
    "    def update_model(self):\n",
    "        self.optim.step()\n",
    "        self.optim.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "202b2581-4747-4d9c-87c3-fc708bc7c2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PolicyAgent(nn.Module):\n",
    "    GAMMA = 0.9\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super(PolicyAgent, self).__init__()\n",
    "\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.create_policy_network()\n",
    "        self.create_policy_method()\n",
    "        \n",
    "        self.state_batch = []\n",
    "        self.action_batch = []\n",
    "        self.reward_batch = []\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def create_policy_network(self):\n",
    "        self.fc = nn.Linear(self.state_dim, 20)\n",
    "        self.critic = nn.Sequential(self.fc, nn.ReLU(), nn.Linear(20,1))\n",
    "        self.actor = nn.Sequential(self.fc, nn.ReLU(), nn.Linear(20, self.action_dim))\n",
    "        \n",
    "    def create_policy_method(self):\n",
    "        self.optim = optim.Adam(self.parameters(), lr = 0.001)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, device=device)\n",
    "            action_logits = self.actor(state)\n",
    "            action_probs = F.softmax(action_logits,dim=-1)\n",
    "            action = torch.multinomial(action_probs, 1)\n",
    "            return action.item()\n",
    "\n",
    "    def calculate_batch_td_error(self, state_batch:torch.Tensor, reward_batch:torch.Tensor, done:bool):\n",
    "        value_batch = self.critic(state_batch)\n",
    "        values = value_batch[:-1,:].squeeze(-1)\n",
    "        next_values = value_batch[1:,:].squeeze(-1)\n",
    "        if done:\n",
    "            next_values[-1] = 0.\n",
    "        td_errors = reward_batch + self.GAMMA * next_values - values\n",
    "        return td_errors\n",
    "    \n",
    "    def calculate_policy_loss(self, state_batch, action_batch, td_errors):\n",
    "        action_logits_batch = self.actor(state_batch)\n",
    "        log_probs = torch.log(F.softmax(action_logits_batch, dim=-1))\n",
    "        action_log_probs = torch.gather(log_probs,1,action_batch.unsqueeze(-1)).squeeze(-1)\n",
    "        policy_loss = action_log_probs * td_errors\n",
    "        return policy_loss\n",
    "        \n",
    "    def perceive(self, state, action, reward, next_state):\n",
    "        if len(self.state_batch) != 0:\n",
    "            self.state_batch.pop(-1)\n",
    "        self.state_batch.append(state)\n",
    "        self.action_batch.append(action)\n",
    "        self.reward_batch.append(reward)\n",
    "        self.state_batch.append(next_state)\n",
    "    \n",
    "    def calculate_grad(self, done):\n",
    "        self.optim.zero_grad()\n",
    "        state_batch = torch.tensor(self.state_batch, device=device)\n",
    "        action_batch = torch.tensor(self.action_batch, device=device)\n",
    "        reward_batch = torch.tensor(self.reward_batch, device=device)\n",
    "        \n",
    "        td_errors = self.calculate_batch_td_error(state_batch, reward_batch, done)\n",
    "        \n",
    "        value_loss = torch.square(td_errors).mean()\n",
    "        policy_loss = self.calculate_policy_loss(state_batch, action_batch, td_errors.detach()).mean()\n",
    "        loss = value_loss - policy_loss\n",
    "        loss.backward()\n",
    "        \n",
    "    \n",
    "    def sync_model(self,model_dict):\n",
    "        self.load_state_dict(model_dict, strict=False)\n",
    "        \n",
    "    def clear_list(self):\n",
    "        self.state_batch.clear()\n",
    "        self.action_batch.clear()\n",
    "        self.reward_batch.clear()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a834bf1f-45a8-4b86-a2d7-3ec5ea5f78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "policy_agent = PolicyAgent(env)\n",
    "target_agent = A3CTrainer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3da5fa99-7ff1-425f-88df-6ea0c44fc971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for episode in range(3000*2):\n",
    "        state, _ = env.reset()\n",
    "        for step in range(300):\n",
    "            action = policy_agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = -1 if done else 0.01\n",
    "            policy_agent.perceive(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        policy_agent.calculate_grad(done)\n",
    "        target_agent.sync_grad(policy_agent)\n",
    "        target_agent.update_model()\n",
    "        policy_agent.sync_model(target_agent.state_dict())\n",
    "        policy_agent.clear_list()\n",
    "        if episode % 100 == 0 and episode !=0:\n",
    "            total_reward = 0\n",
    "            for i in range(10):\n",
    "                state, _ = env.reset()\n",
    "                for step in range(300):\n",
    "                    action = policy_agent.choose_action(state)\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "            print(f\"episode {episode} total reward is {total_reward/10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6c2079c0-71d9-47e0-8bc5-07b0a3258881",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 100 total reward is 21.7\n",
      "episode 200 total reward is 22.0\n",
      "episode 300 total reward is 24.8\n",
      "episode 400 total reward is 23.6\n",
      "episode 500 total reward is 46.3\n",
      "episode 600 total reward is 37.7\n",
      "episode 700 total reward is 37.3\n",
      "episode 800 total reward is 34.3\n",
      "episode 900 total reward is 43.4\n",
      "episode 1000 total reward is 47.0\n",
      "episode 1100 total reward is 46.3\n",
      "episode 1200 total reward is 63.0\n",
      "episode 1300 total reward is 56.8\n",
      "episode 1400 total reward is 44.5\n",
      "episode 1500 total reward is 39.6\n",
      "episode 1600 total reward is 50.4\n",
      "episode 1700 total reward is 48.8\n",
      "episode 1800 total reward is 43.6\n",
      "episode 1900 total reward is 58.5\n",
      "episode 2000 total reward is 75.1\n",
      "episode 2100 total reward is 53.1\n",
      "episode 2200 total reward is 55.6\n",
      "episode 2300 total reward is 64.3\n",
      "episode 2400 total reward is 65.7\n",
      "episode 2500 total reward is 53.6\n",
      "episode 2600 total reward is 75.2\n",
      "episode 2700 total reward is 60.0\n",
      "episode 2800 total reward is 77.2\n",
      "episode 2900 total reward is 68.9\n",
      "episode 3000 total reward is 49.0\n",
      "episode 3100 total reward is 77.0\n",
      "episode 3200 total reward is 118.5\n",
      "episode 3300 total reward is 70.6\n",
      "episode 3400 total reward is 97.5\n",
      "episode 3500 total reward is 147.5\n",
      "episode 3600 total reward is 119.6\n",
      "episode 3700 total reward is 122.5\n",
      "episode 3800 total reward is 187.6\n",
      "episode 3900 total reward is 157.3\n",
      "episode 4000 total reward is 168.7\n",
      "episode 4100 total reward is 139.8\n",
      "episode 4200 total reward is 188.3\n",
      "episode 4300 total reward is 169.7\n",
      "episode 4400 total reward is 152.8\n",
      "episode 4500 total reward is 176.9\n",
      "episode 4600 total reward is 146.2\n",
      "episode 4700 total reward is 165.6\n",
      "episode 4800 total reward is 194.4\n",
      "episode 4900 total reward is 180.5\n",
      "episode 5000 total reward is 156.2\n",
      "episode 5100 total reward is 183.0\n",
      "episode 5200 total reward is 170.0\n",
      "episode 5300 total reward is 184.9\n",
      "episode 5400 total reward is 190.1\n",
      "episode 5500 total reward is 234.1\n",
      "episode 5600 total reward is 204.8\n",
      "episode 5700 total reward is 161.4\n",
      "episode 5800 total reward is 160.6\n",
      "episode 5900 total reward is 185.3\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e515772-d45b-4657-8dd0-46d72f3acd91",
   "metadata": {},
   "source": [
    "## 实验记录 \n",
    "2023-3-8 \\\n",
    "1. 实现了部分A3C算法，还有获取行为策略的梯度部分以及梯度同步、模型同步部分。 \\\n",
    "2023-3-9 \\\n",
    "1. 实现了剩余的部分。 \\\n",
    "结果：一个行为策略效果还不错，但是发现m1芯片在小模型训练上表现奇差。在mac上要换成cpu训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b894d3-a5c9-4d4c-9610-dcbe67200d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e261e-8f94-4bb2-827d-fa7a5357fbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
