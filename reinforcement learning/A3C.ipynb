{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89871d9f-c6de-488c-8e59-74660ff43dc9",
   "metadata": {},
   "source": [
    "# 一个A3C算法的简单实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfffcf1c-655c-40ca-81cb-d4091d7db7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736315d4-fe10-435d-87a6-161b65f88061",
   "metadata": {},
   "source": [
    "## Creating ActorCritic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93546163-3b36-42b0-863f-a84c7b787f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_inputs, action_space):\n",
    "        super(ActorCritic, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_inputs, 32, 3 ,stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(32, 32, 3, stride=2, padding=1)\n",
    "        \n",
    "        self.lstm = nn.LSTMCell(32 * 3 * 3, 256)\n",
    "        \n",
    "        num_outputs = action_space.n\n",
    "        self.critical_linear = nn.Linear(256, 1)\n",
    "        self.actor_linear = nn.Linear(256, num_outputs)\n",
    "        \n",
    "        self.train()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        inputs, (hx, cx) = inputs\n",
    "        x = F.elu(self.conv1(inputs))\n",
    "        x = F.elu(self.conv2(x))\n",
    "        x = F.elu(self.conv3(x))\n",
    "        x = F.elu(self.conv4(x))\n",
    "        \n",
    "        x = x.view(-1, 32*3*3)\n",
    "        hx, cx = self.lstm(x, (hx, cx))\n",
    "        x = hx\n",
    "        return self.critical_linear(x), self.actor_linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14549de2-a9de-40cb-8adb-d0b1a551ac82",
   "metadata": {},
   "source": [
    "## Creating atari envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b837e95b-4e55-496b-a7d1-3d929ccefaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import gym\n",
    "import numpy as np\n",
    "from gym.spaces.box import Box\n",
    "\n",
    "def create_atari_env(env_id):\n",
    "    env = gym.make(env_id)\n",
    "\n",
    "    \n",
    "def _process_frame42(frame):\n",
    "    frame = frame[34:34 + 160, :160]\n",
    "    frame = cv2.resize(frame, (80, 80))\n",
    "    frame = cv2.resize(frame, (42, 42))\n",
    "    frame = frame.mean(2, keepdims=True)\n",
    "    frame = frame.astype(np.float32)\n",
    "    frame *= (1.0 / 255.0)\n",
    "    frame = np.moveaxis(frame, -1, 0)\n",
    "    return frame\n",
    "\n",
    "\n",
    "class AtariRescale42x42(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(AtariRescale42x42, self).__init__(env)\n",
    "        self.observation_space = Box(0.0, 1.0, [1, 42, 42])\n",
    "        \n",
    "    def _observation(self, observation):\n",
    "        return _pro\n",
    "    \n",
    "\n",
    "class NormalizedEnv(gym.ObservationWrapper):\n",
    "    def __init__(self, env=None):\n",
    "        super(NormalizedEnv, self).__init__(env)\n",
    "        self.state_mean = 0\n",
    "        self.state_std = 0\n",
    "        self.alpha = 0.9999\n",
    "        self.num_steps = 0\n",
    "        \n",
    "    def _observation(self, observation):\n",
    "        self.num_steps += 1\n",
    "        self.state_mean = self.state_mean * self.alpha + observation.mean() * (1 - self.alpha)\n",
    "        self.state_std = self.state_std * self.alpha + observation.std() * (1 - self.alpha)\n",
    "        \n",
    "        unbiased_mean = self.state_mean / (1 - pow(self.alpha, self.num_steps))\n",
    "        unbiased_std = self.state_std / (1 - pow(self.alpha, self.num_steps))\n",
    "\n",
    "        return (observation - unbiased_mean) / (unbiased_std + 1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996e09c-8602-4b43-9d8e-2075a24c31ce",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc1c090-3f64-42bd-a9e1-f65ff1cce245",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_shared_grads(model, shared_model):\n",
    "    for param, shared_param in zip(model.parameters(), shared_model.parameters()):\n",
    "        if shared_model.grad is not None:\n",
    "            return\n",
    "        shared_param._grad = param.grad\n",
    "        \n",
    "def train(rank, args, shared_model:nn.Module, counter, lock, optimizer=None):\n",
    "    torch.manual_seed(args.seed + rank)\n",
    "    \n",
    "    env = create_atari_env(args.env_name)\n",
    "    env.seed(args.seed + rank)\n",
    "    \n",
    "    model = ActorCritic(env.observation_space.shape[0], env.action_space)\n",
    "    if optimizer is None:\n",
    "        optimizer = optim.Adam(shared_model.parameters(), lr=args.lr)\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = torch.from_numpy(state)\n",
    "    done = True\n",
    "    \n",
    "    episode_length = 0\n",
    "    while True:\n",
    "        model.load_state_dict(shared_model.state_dict())\n",
    "        if done:\n",
    "            cx = torch.zeros(1, 256)\n",
    "            hx = torch.zeros(1, 256)\n",
    "        else:\n",
    "            cx = cx.detach()\n",
    "            hx = hx.detach()\n",
    "        \n",
    "        values = []\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        entropies = []\n",
    "        \n",
    "        for step in range(args.num_steps):\n",
    "            episode_length += 1\n",
    "            value, logit, (hx, cx) = model((state.unsqueeze(0),(hx, cx)))\n",
    "            prob = F.softmax(logit, dim=-1)\n",
    "            log_prob = F.log_softmax(logit, dim=-1)\n",
    "            entropy = -(log_prob * prob).sum(1, keepdim=True)\n",
    "            entropies.append(entropy)\n",
    "            \n",
    "            action = prob.multionmial(num_samples=1).detach()\n",
    "            log_prob = log_prob.gather(1, action)\n",
    "            \n",
    "            state, reward, done, _ = env.step(action.numpy())\n",
    "            done = done or episode_length >= args.max_episode_length\n",
    "            reward = max(min(reward, 1), -1) # 把reward限制在[-1,1]内\n",
    "            \n",
    "            with lock:\n",
    "                counter.value += 1\n",
    "            \n",
    "            if done:\n",
    "                episode_length = 0\n",
    "                state = env.reset()\n",
    "            \n",
    "            state = torch.from_numpy(state)\n",
    "            values.append(value)\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "        R = torch.zeros(1, 1)\n",
    "        if not done:\n",
    "            value, _, _ = model((state.unsqueeze(0), (hx, cx)))\n",
    "            R = value.detach()\n",
    "        \n",
    "        values.append(R)\n",
    "        policy_loss = 0\n",
    "        value_loss = 0\n",
    "        gae = torch.zeros(1, 1)\n",
    "        for i in reversed(range(rewards)):\n",
    "            R = args.gamma * R + rewards[i]\n",
    "            advantage = R - values[i]\n",
    "            value_loss = value_loss + 0.5 * advantage.pow(2)\n",
    "            \n",
    "            delta_t = rewards[i] + args.gamma * values[i+1]\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalle2",
   "language": "python",
   "name": "dalle2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
