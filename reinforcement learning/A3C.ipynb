{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14bf3487",
   "metadata": {},
   "source": [
    "# **一个Advantage Actor-Critic算法的简单实现**\n",
    "## **算法概述**\n",
    "- 采用批输入数据的方式加速训练;\n",
    "- 批输入的数据包括TD误差以及log概率;\n",
    "- 采用TD误差进行批处理训练过程及其不稳定，很难训起来。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6aad9448",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e59d7e1d-b0a7-4dd9-a102-90848f454f05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "if platform.system == \"Darwin\":\n",
    "    PYTORCH_ENABLE_MPS_FALLBACK = 1\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05641c2a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class A3CTrainer(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(A3CTrainer, self).__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        \n",
    "        self.create_training_network()\n",
    "        self.create_training_method()\n",
    "        \n",
    "        self.to(device)\n",
    "\n",
    "    def create_training_network(self):\n",
    "        self.fc = nn.Linear(self.state_dim, 20)\n",
    "        self.value = nn.Sequential(self.fc, nn.ReLU(), nn.Linear(20,1))\n",
    "        self.actor = nn.Sequential(self.fc, nn.ReLU(), nn.Linear(20, self.action_dim))\n",
    "\n",
    "    def create_training_method(self):\n",
    "        self.optim = optim.Adam(self.parameters(), lr=0.001)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            action_logits = self.actor(state)\n",
    "            action_probs = F.softmax(action_logits, dim=-1)\n",
    "            action = torch.multinomial(action_probs,1).item()\n",
    "            return action, action_probs[action]\n",
    "    \n",
    "    def sync_grad(self, policy_agent:nn.Module):\n",
    "        for behavior_param, target_param in zip(policy_agent.parameters(), self.parameters()):\n",
    "            if target_param.grad is not None:\n",
    "                break\n",
    "            target_param.grad = behavior_param.grad\n",
    "    \n",
    "    def update_model(self):\n",
    "        self.optim.step()\n",
    "        self.optim.zero_grad()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "202b2581-4747-4d9c-87c3-fc708bc7c2ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PolicyAgent(nn.Module):\n",
    "    GAMMA = 0.9\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super(PolicyAgent, self).__init__()\n",
    "\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.create_policy_network()\n",
    "        self.create_policy_method()\n",
    "        \n",
    "        self.state_batch = []\n",
    "        self.action_batch = []\n",
    "        self.reward_batch = []\n",
    "        \n",
    "        self.to(device)\n",
    "        \n",
    "    def create_policy_network(self):\n",
    "        self.fc = nn.Linear(self.state_dim, 20)\n",
    "        self.critic = nn.Sequential(self.fc, nn.ReLU(), nn.Linear(20,1))\n",
    "        self.actor = nn.Sequential(self.fc, nn.ReLU(), nn.Linear(20, self.action_dim))\n",
    "        \n",
    "    def create_policy_method(self):\n",
    "        self.optim = optim.Adam(self.parameters(), lr = 0.001)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, device=device)\n",
    "            action_logits = self.actor(state)\n",
    "            action_probs = F.softmax(action_logits,dim=-1)\n",
    "            action = torch.multinomial(action_probs, 1)\n",
    "            return action.item()\n",
    "\n",
    "    def calculate_batch_td_error(self, state_batch:torch.Tensor, reward_batch:torch.Tensor, done:bool):\n",
    "        value_batch = self.critic(state_batch)\n",
    "        values = value_batch[:-1,:].squeeze(-1)\n",
    "        next_values = value_batch[1:,:].squeeze(-1)\n",
    "        if done:\n",
    "            next_values[-1] = 0.\n",
    "        td_errors = reward_batch + self.GAMMA * next_values - values\n",
    "        return td_errors\n",
    "    \n",
    "    def calculate_policy_loss(self, state_batch, action_batch, td_errors):\n",
    "        action_logits_batch = self.actor(state_batch)\n",
    "        log_probs = torch.log(F.softmax(action_logits_batch, dim=-1))\n",
    "        action_log_probs = torch.gather(log_probs,1,action_batch.unsqueeze(-1)).squeeze(-1)\n",
    "        policy_loss = action_log_probs * td_errors\n",
    "        return policy_loss\n",
    "        \n",
    "    def perceive(self, state, action, reward, next_state):\n",
    "        if len(self.state_batch) != 0:\n",
    "            self.state_batch.pop(-1)\n",
    "        self.state_batch.append(state)\n",
    "        self.action_batch.append(action)\n",
    "        self.reward_batch.append(reward)\n",
    "        self.state_batch.append(next_state)\n",
    "    \n",
    "    def calculate_grad(self, done):\n",
    "        self.optim.zero_grad()\n",
    "        state_batch = torch.tensor(self.state_batch, device=device)\n",
    "        action_batch = torch.tensor(self.action_batch, device=device)\n",
    "        reward_batch = torch.tensor(self.reward_batch, device=device)\n",
    "        \n",
    "        td_errors = self.calculate_batch_td_error(state_batch, reward_batch, done)\n",
    "        \n",
    "        value_loss = torch.square(td_errors).mean()\n",
    "        policy_loss = self.calculate_policy_loss(state_batch, action_batch, td_errors.detach()).mean()\n",
    "        loss = value_loss - policy_loss\n",
    "        loss.backward()\n",
    "        \n",
    "    \n",
    "    def sync_model(self,model_dict):\n",
    "        self.load_state_dict(model_dict, strict=False)\n",
    "        \n",
    "    def clear_list(self):\n",
    "        self.state_batch.clear()\n",
    "        self.action_batch.clear()\n",
    "        self.reward_batch.clear()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a834bf1f-45a8-4b86-a2d7-3ec5ea5f78a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "policy_agent = PolicyAgent(env)\n",
    "target_agent = A3CTrainer(env)\n",
    "policy_agent.sync_model(target_agent.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3da5fa99-7ff1-425f-88df-6ea0c44fc971",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for episode in range(3000*2):\n",
    "        state, _ = env.reset()\n",
    "        for step in range(300):\n",
    "            action = policy_agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = -1 if done else 0.01\n",
    "            policy_agent.perceive(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        policy_agent.calculate_grad(done)\n",
    "        target_agent.sync_grad(policy_agent)\n",
    "        target_agent.update_model()\n",
    "        policy_agent.sync_model(target_agent.state_dict())\n",
    "        policy_agent.clear_list()\n",
    "        if episode % 100 == 0 and episode !=0:\n",
    "            total_reward = 0\n",
    "            for i in range(10):\n",
    "                state, _ = env.reset()\n",
    "                for step in range(300):\n",
    "                    action = policy_agent.choose_action(state)\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "            print(f\"episode {episode} total reward is {total_reward/10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6c2079c0-71d9-47e0-8bc5-07b0a3258881",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zr/k0d9hm015g76gx4pfq9dvz_40000gn/T/ipykernel_55010/1274476647.py:60: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1670525474122/work/torch/csrc/utils/tensor_new.cpp:233.)\n",
      "  state_batch = torch.tensor(self.state_batch, device=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 100 total reward is 22.2\n",
      "episode 200 total reward is 25.3\n",
      "episode 300 total reward is 26.2\n",
      "episode 400 total reward is 44.1\n",
      "episode 500 total reward is 37.0\n",
      "episode 600 total reward is 47.9\n",
      "episode 700 total reward is 55.8\n",
      "episode 800 total reward is 45.5\n",
      "episode 900 total reward is 46.4\n",
      "episode 1000 total reward is 61.8\n",
      "episode 1100 total reward is 59.7\n",
      "episode 1200 total reward is 58.0\n",
      "episode 1300 total reward is 71.9\n",
      "episode 1400 total reward is 72.6\n",
      "episode 1500 total reward is 94.1\n",
      "episode 1600 total reward is 72.9\n",
      "episode 1700 total reward is 79.9\n",
      "episode 1800 total reward is 68.6\n",
      "episode 1900 total reward is 73.4\n",
      "episode 2000 total reward is 102.9\n",
      "episode 2100 total reward is 118.3\n",
      "episode 2200 total reward is 99.1\n",
      "episode 2300 total reward is 100.4\n",
      "episode 2400 total reward is 141.7\n",
      "episode 2500 total reward is 190.1\n",
      "episode 2600 total reward is 152.3\n",
      "episode 2700 total reward is 199.2\n",
      "episode 2800 total reward is 231.3\n",
      "episode 2900 total reward is 213.7\n",
      "episode 3000 total reward is 204.8\n",
      "episode 3100 total reward is 203.6\n",
      "episode 3200 total reward is 205.5\n",
      "episode 3300 total reward is 125.3\n",
      "episode 3400 total reward is 159.2\n",
      "episode 3500 total reward is 155.1\n",
      "episode 3600 total reward is 155.5\n",
      "episode 3700 total reward is 130.7\n",
      "episode 3800 total reward is 121.8\n",
      "episode 3900 total reward is 125.9\n",
      "episode 4000 total reward is 140.7\n",
      "episode 4100 total reward is 138.4\n",
      "episode 4200 total reward is 124.8\n",
      "episode 4300 total reward is 140.9\n",
      "episode 4400 total reward is 121.7\n",
      "episode 4500 total reward is 123.3\n",
      "episode 4600 total reward is 136.6\n",
      "episode 4700 total reward is 134.5\n",
      "episode 4800 total reward is 147.1\n",
      "episode 4900 total reward is 150.8\n",
      "episode 5000 total reward is 132.0\n",
      "episode 5100 total reward is 132.4\n",
      "episode 5200 total reward is 132.0\n",
      "episode 5300 total reward is 142.0\n",
      "episode 5400 total reward is 160.5\n",
      "episode 5500 total reward is 169.9\n",
      "episode 5600 total reward is 153.5\n",
      "episode 5700 total reward is 145.8\n",
      "episode 5800 total reward is 136.2\n",
      "episode 5900 total reward is 143.8\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e515772-d45b-4657-8dd0-46d72f3acd91",
   "metadata": {},
   "source": [
    "## 实验记录 \n",
    "2023-3-8\n",
    "1. 实现了部分A3C算法，还有获取行为策略的梯度部分以及"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b894d3-a5c9-4d4c-9610-dcbe67200d02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0e261e-8f94-4bb2-827d-fa7a5357fbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
