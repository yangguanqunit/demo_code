{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d57e25a9-3ab5-46c0-b345-4219a861d909",
   "metadata": {},
   "source": [
    "# **一个Duleing DQN算法的简单实现**\n",
    "## **算法概述**\n",
    "- Nature DQN的改进版。\n",
    "- 改进了网络结构，原本的DQN只有一个输出头，输出各动作的Q值；Duleing DQN有两个输出头，一个输出与动作无关的价值（函数），另外一个输出与动作有关的优势（函数）。\n",
    "- 将原来的一个输出分解为两个，价值函数专注于预测当前状态的好坏，而优势函数专注于预测当前状态下每个待执行动作的好坏。\n",
    "- off-policy算法，value-based算法。\n",
    "\n",
    "论文链接：*https://docs.popo.netease.com/docs/dcc8c9f994bc441bab9fc84be9572bb7*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a61ce33-7e02-438d-b481-bb6b511d47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "161bd262-af65-45be-ab4e-077a7d1f223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "epsilon = 0.5\n",
    "start_epsilon = 0.5\n",
    "end_epsilon = 0.01\n",
    "batch_size = 32\n",
    "replay_size = 10000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd83a56e-96fb-437d-a9ca-d8ca6c351a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuleDQN(nn.Module):\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        super(DuleDQN, self).__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.create_training_network()\n",
    "        self.create_training_method()\n",
    "        self.replay_buffer = deque()\n",
    "        self.to(device)\n",
    "        \n",
    "        \n",
    "    def create_training_network(self):\n",
    "        self.fc1 = nn.Linear(self.state_dim, 20)\n",
    "        self.value_func = nn.Linear(20, 1)\n",
    "        self.advantage_func = nn.Linear(20, self.action_dim)\n",
    "    \n",
    "    def create_training_method(self):\n",
    "        self.optim = optim.Adam(self.parameters(), lr=0.001)\n",
    "        self.loss_cal = F.mse_loss\n",
    "        \n",
    "    def get_target_network(self, target_network):\n",
    "        self.target_network = target_network\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x = F.relu(self.fc1(input))\n",
    "        value = self.value_func(x)\n",
    "        advantage = self.advantage_func(x)\n",
    "        return value, advantage\n",
    "    \n",
    "    def calculate_dule_Q(self, value, advantage):\n",
    "        # return [B,A_dim]\n",
    "        if advantage.dim() == 2:\n",
    "            return value.view(-1, 1) + (advantage - torch.mean(advantage, dim=-1, keepdim=True))\n",
    "        elif advantage.dim() == 1:\n",
    "            return value + (advantage - torch.mean(advantage, dim=-1, keepdim=True))\n",
    "    def episode_greedy(self, state):\n",
    "        global epsilon\n",
    "        with torch.no_grad():\n",
    "            # 如果源类型是float，则tensor()和from_tensor()结果相同；否则tensor()返回的类型是float32，from_tensor返回类型与源类型相同。\n",
    "            state = torch.tensor(state, device=device)\n",
    "            # state = torch.from_numpy(state) \n",
    "            if random.random() > epsilon:\n",
    "                value, advantage = self(state)\n",
    "                state_action_value = self.calculate_dule_Q(value, advantage)\n",
    "                action = torch.argmax(state_action_value).item()\n",
    "            else:\n",
    "                action = np.random.randint(0, 2)\n",
    "            epsilon = epsilon - (start_epsilon - end_epsilon) / 10000\n",
    "            return action\n",
    "    \n",
    "    def action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, device=device)\n",
    "            value, advantage = self(state)\n",
    "            state_action_value = self.calculate_dule_Q(value, advantage)\n",
    "            action = torch.argmax(state_action_value).item()\n",
    "            return action\n",
    "    \n",
    "    def perceive(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(self.replay_buffer) > replay_size:\n",
    "            self.replay_buffer.popleft()\n",
    "        if len(self.replay_buffer) >= batch_size:\n",
    "            self.train_loop()\n",
    "    \n",
    "    def train_loop(self):\n",
    "        \n",
    "        minibatch = random.sample(self.replay_buffer, batch_size)\n",
    "        state_batch = [data[0] for data in minibatch]\n",
    "        action_batch = [data[1] for data in minibatch]\n",
    "        reward_batch = [data[2] for data in minibatch]\n",
    "        next_state_batch = [data[3] for data in minibatch]\n",
    "        done_batch = [data[4] for data in minibatch]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            value, advantage = self.target_network(torch.tensor(np.stack(next_state_batch), device=device))\n",
    "            target_Q_value = torch.max(self.calculate_dule_Q(value,advantage), dim=1)[0]\n",
    "        reward = torch.tensor(np.stack(reward_batch), device=device, dtype=torch.float32)\n",
    "        done = torch.tensor(np.stack(done_batch), device=device)\n",
    "        zero_value = torch.zeros_like(target_Q_value)\n",
    "        y_batch = torch.where(done, zero_value, GAMMA * target_Q_value) + reward\n",
    "        \n",
    "        value, advantage = self(torch.tensor(np.stack(state_batch), device=device))\n",
    "        Q_value = self.calculate_dule_Q(value, advantage)\n",
    "        action = torch.tensor(np.stack(action_batch),device=device).view(-1, 1)\n",
    "        Q_batch = torch.gather(Q_value, 1, action).squeeze(-1)\n",
    "        \n",
    "        loss = self.loss_cal(Q_batch, y_batch)\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n",
    "        \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d15f01b-0bb7-4fe3-80ec-a57f9095bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "agent = DuleDQN(env)\n",
    "agent.train()\n",
    "target_network = DuleDQN(env)\n",
    "agent.get_target_network(target_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "747f8475-f139-4df2-b82c-27fc1cb4f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for episode in range(3000):\n",
    "        state, _ = env.reset()\n",
    "        for step in range(300):\n",
    "            action = agent.episode_greedy(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = -1 if done else 0.01\n",
    "            agent.perceive(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        # for evaluation\n",
    "        if episode % 100 == 0 and episode != 0:\n",
    "            totoal_reward = 0.0\n",
    "            for eval_count in range(10):\n",
    "                state, _ = env.reset()\n",
    "                for step in range(300):\n",
    "                    action = agent.action(state)\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    totoal_reward += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "            totoal_reward /= 10\n",
    "            print(f\"episode {episode} total score is {totoal_reward}\")\n",
    "            agent.update_target_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b8e61-fcce-402b-a189-cc4fd4f06b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ygq/miniconda3/envs/dalle2/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 100 total score is 9.2\n",
      "episode 200 total score is 11.1\n",
      "episode 300 total score is 14.4\n",
      "episode 400 total score is 31.2\n",
      "episode 500 total score is 64.4\n",
      "episode 600 total score is 86.4\n",
      "episode 700 total score is 130.3\n",
      "episode 800 total score is 255.3\n",
      "episode 900 total score is 95.2\n",
      "episode 1000 total score is 144.7\n",
      "episode 1100 total score is 86.5\n",
      "episode 1200 total score is 296.4\n",
      "episode 1300 total score is 275.3\n",
      "episode 1400 total score is 298.5\n",
      "episode 1500 total score is 232.0\n",
      "episode 1600 total score is 300.0\n",
      "episode 1700 total score is 110.2\n",
      "episode 1800 total score is 103.8\n",
      "episode 1900 total score is 107.4\n",
      "episode 2000 total score is 158.3\n",
      "episode 2100 total score is 117.5\n",
      "episode 2200 total score is 105.6\n",
      "episode 2300 total score is 153.2\n",
      "episode 2400 total score is 214.2\n",
      "episode 2500 total score is 90.8\n",
      "episode 2600 total score is 91.9\n",
      "episode 2700 total score is 104.2\n",
      "episode 2800 total score is 110.7\n",
      "episode 2900 total score is 143.2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalle2",
   "language": "python",
   "name": "dalle2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
