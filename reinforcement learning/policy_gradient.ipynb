{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7b135e4-310f-4edd-a00d-ae1f2927463a",
   "metadata": {},
   "source": [
    "# **一个Policy Gradient算法的简单实现**\n",
    "## **算法概述**\n",
    "- 一个较为基础的policybased算法\n",
    "- 为了解决众多以DQN为主的value-based算法对连续动作处理能力不足的问题；为了解决受限状态下的问题处理能力不足的问题，该问题是因为观测的限制或建模的局限，导致真实的环境下不同的两个状态有相同的特征表示，进而可能导致value based的方法无法得到最优解（这一点还不是特别理解，policy based算法也有这样的问题？）；解决随机策略的问题，value based的最优策略通常是确定性的，而有些问题的最优策略是随机的，因此导致value based方法无法得到最优解。\n",
    "- 算法网络直接输出动作（包括离散动作的softmax或者直接输出连续动作）\n",
    "- on-policy算法，policy-based算法\n",
    "\n",
    "论文链接：*https://docs.popo.netease.com/docs/e54e7b5d00a44f52b7edc926efe4e829*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ad2b72-7f41-4873-a4d1-1ccd2e464919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d75fa59-2421-4344-a1e8-1ddc2fae5ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.95\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14dda221-ce94-4215-82e5-dbd185dee67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy_gradient(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(policy_gradient, self).__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.ep_obs, self.ep_as, self.ep_rs = [], [], []\n",
    "        self.create_training_network()\n",
    "        self.create_training_method()\n",
    "        self.to(device)\n",
    "        if not self.training:\n",
    "            self.train()\n",
    "    \n",
    "    def forward(self, states):\n",
    "        x = F.relu(self.fc1(states))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def create_training_network(self):\n",
    "        self.fc1 = nn.Linear(self.state_dim, 20)\n",
    "        self.fc2 = nn.Linear(20, self.action_dim)\n",
    "    \n",
    "    def create_training_method(self):\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.001)\n",
    "        # self.loss_cal = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def choose_action(self, observation):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(observation, device=device)\n",
    "            prob_weight = F.softmax(self(state), dim=0)\n",
    "            action = torch.multinomial(prob_weight,1)\n",
    "            return action.item()\n",
    "    \n",
    "    def choose_max_action(self, observation):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(observation, device=device)\n",
    "            prob_weight = F.softmax(self(state), dim=0)\n",
    "            action = torch.argmax(prob_weight)\n",
    "            return action.item()\n",
    "        \n",
    "    def store_transition(self, s, a, r):\n",
    "        self.ep_obs.append(s)\n",
    "        self.ep_as.append(a)\n",
    "        self.ep_rs.append(r)\n",
    "        \n",
    "    def clear_transition(self):\n",
    "        self.ep_obs.clear()\n",
    "        self.ep_as.clear()\n",
    "        self.ep_rs.clear()\n",
    "        \n",
    "    def train_loop(self):\n",
    "        discounted_ep_rs = np.zeros(len(self.ep_obs))\n",
    "        accumulate_discount_reward = 0.0\n",
    "        for t in reversed(range(0,len(self.ep_rs))):\n",
    "            accumulate_discount_reward += GAMMA * accumulate_discount_reward + self.ep_rs[t]\n",
    "            discounted_ep_rs[t] = accumulate_discount_reward\n",
    "        discounted_ep_rs -= np.mean(discounted_ep_rs)\n",
    "        discounted_ep_rs /= np.std(discounted_ep_rs)\n",
    "        \n",
    "        states = torch.tensor(np.stack(self.ep_obs), device=device)\n",
    "        action_labels = torch.tensor(np.stack(self.ep_as), device=device)\n",
    "        # action_probs = torch.log(F.softmax(self(states), dim=1))\n",
    "        action_logits = self(states)\n",
    "        action_log_probs = torch.log(F.softmax(action_logits, dim=1))\n",
    "        ep_value = torch.tensor(discounted_ep_rs, device=device)\n",
    "        loss = (torch.gather(action_log_probs,1,action_labels.unsqueeze(-1)).squeeze(-1) * ep_value).mean()\n",
    "        # loss = (self.loss_cal(action_logits, action_labels) * ep_value).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        self.clear_transition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9b63505-7cba-4a38-8b5b-3fbe18a21434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "\n",
    "agent = policy_gradient(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e416751-31ef-474f-96b1-b998124b13eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for episode in range(30000):\n",
    "        state, _ = env.reset()\n",
    "        for step in range(3000):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = -1 if done else 0.01\n",
    "            agent.store_transition(state, action, reward)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                agent.train_loop()\n",
    "                break\n",
    "        if episode % 100 == 0:\n",
    "            total_reward = 0\n",
    "            for i in range(10):\n",
    "                state, _ = env.reset()\n",
    "                for step in range(300):\n",
    "                    action = agent.choose_action(state)\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "            print(f\"episode {episode} total reward is {total_reward/10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef721b3d-a7cb-4ce6-ab4d-801615957fd6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ygq/miniconda3/envs/dalle2/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765c5101-397a-4ff4-92f2-0110820b350c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac946fe-c5b8-4eef-85c5-361467684504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
