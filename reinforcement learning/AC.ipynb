{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89871d9f-c6de-488c-8e59-74660ff43dc9",
   "metadata": {},
   "source": [
    "# **一个Actor-Critic算法的简单实现**\n",
    "## **算法概述**\n",
    "- 一个比较典型的Actor-Critic算法的简单实现；\n",
    "- 这类算法融合了Value Based 和 Policy Based方法，其基本范式为AC算法的模型有两类输出头，一类输出价值（状态价值、动作价值、TD误差、优势函数及其他价值等）（以下代码使用TD无误差作为价值），另一类直接输出动作（Softmax或者log_softmax）。此举主要是为了扩展Policy Based的应用场景（因为Policy Based往往会采用MC方法来近似计算价值，但由于有些场景状态长度是无限的，所以没法用MC计算，但可以用价值网络来估计）。\n",
    "- 算法网络直接输出动作（包括离散动作的softmax或者直接输出连续动作），以及输出价值。\n",
    "- on-policy和off-policy都有，AC算法\n",
    "\n",
    "论文链接：*https://docs.popo.netease.com/docs/502c4f53c35444afbce8ef384a2474ed*# (AC算法的理论基础)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfffcf1c-655c-40ca-81cb-d4091d7db7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import platform\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df9b5ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if platform.system() == \"Darwin\":\n",
    "    PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93546163-3b36-42b0-863f-a84c7b787f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrainer(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(ActorCriticTrainer, self).__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.create_training_network()\n",
    "        self.create_training_method()\n",
    "        self.GAMMA = 0.9\n",
    "        self.to(device)\n",
    "\n",
    "    def create_training_network(self):\n",
    "        self.fc = nn.Linear(self.state_dim, 20)\n",
    "        self.critic = nn.Sequential(self.fc,nn.ReLU(),nn.Linear(20,1))\n",
    "        self.actor = nn.Sequential(self.fc,nn.ReLU(),nn.Linear(20, self.action_dim))\n",
    "\n",
    "    def create_training_method(self):\n",
    "        self.optim = optim.Adam(self.parameters(),lr=0.001)\n",
    "        self.value_loss = nn.MSELoss()\n",
    "        self.actor_loss = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, device=device)\n",
    "            action_probs = F.softmax(self.actor(state), dim=-1)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "            return action\n",
    "\n",
    "    def calculate_td_error(self, state, reward, next_state):\n",
    "        next_value = self.critic(next_state)\n",
    "        value = self.critic(state)\n",
    "        td_error = reward + self.GAMMA * next_value - value\n",
    "        return td_error\n",
    "\n",
    "    def calculate_policy_loss(self, state, action, td_error):\n",
    "        action_logits = self.actor(state)\n",
    "        log_probs = torch.log(F.softmax(action_logits, dim=-1))\n",
    "        action_log_probs = torch.gather(log_probs,0,action)\n",
    "        return action_log_probs * td_error\n",
    "\n",
    "    def calculate_entropy_loss(self, state, action):\n",
    "        action_logits = self.actor(state)\n",
    "        probs = F.softmax(action_logits, dim=-1)\n",
    "        log_probs = torch.log(probs)\n",
    "        entropy = -torch.sum(probs * log_probs)\n",
    "        return entropy\n",
    "    \n",
    "    def train_loop(self, state, action, reward, state_next):\n",
    "        state = torch.tensor(state, device=device)\n",
    "        action = torch.tensor(action, device=device)\n",
    "        reward = torch.tensor(reward, device=device)\n",
    "        next_state = torch.tensor(state_next, device=device)\n",
    "        td_error = self.calculate_td_error(state, reward, next_state)\n",
    "        value_loss = torch.square(td_error)\n",
    "        policy_loss = self.calculate_policy_loss(state, action, td_error.item())\n",
    "        entropy_loss = self.calculate_entropy_loss(state, action,)\n",
    "        loss = value_loss - policy_loss - entropy_loss\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24dc855e-bd20-406f-9fd1-41ef90ab5aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "agent = ActorCriticTrainer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1083f4b-b438-469a-a14d-795aaca45cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    for episode in range(3000):\n",
    "        state, _ = env.reset()\n",
    "        for step in range(300):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = -1 if done else 0.01\n",
    "            agent.train_loop(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if episode % 100 == 0 and episode !=0:\n",
    "            total_reward = 0\n",
    "            for i in range(10):\n",
    "                state, _ = env.reset()\n",
    "                for step in range(300):\n",
    "                    action = agent.choose_action(state)\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "            print(f\"episode {episode} total reward is {total_reward/10}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"total time is {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5811bae0-61a6-4e5c-991b-b3036b3c2edb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 100 total reward is 27.1\n",
      "episode 200 total reward is 19.4\n",
      "episode 300 total reward is 21.3\n",
      "episode 400 total reward is 22.4\n",
      "episode 500 total reward is 26.2\n",
      "episode 600 total reward is 26.7\n",
      "episode 700 total reward is 21.0\n",
      "episode 800 total reward is 26.3\n",
      "episode 900 total reward is 22.3\n",
      "episode 1000 total reward is 26.1\n",
      "episode 1100 total reward is 21.0\n",
      "episode 1200 total reward is 23.8\n",
      "episode 1300 total reward is 22.9\n",
      "episode 1400 total reward is 25.2\n",
      "episode 1500 total reward is 19.2\n",
      "episode 1600 total reward is 16.6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;18m__name__\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__main__\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m----> 2\u001B[0m     \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[13], line 9\u001B[0m, in \u001B[0;36mmain\u001B[0;34m()\u001B[0m\n\u001B[1;32m      7\u001B[0m next_state, reward, done, _, _ \u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mstep(action)\n\u001B[1;32m      8\u001B[0m reward \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m done \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m0.01\u001B[39m\n\u001B[0;32m----> 9\u001B[0m \u001B[43magent\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_loop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maction\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreward\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnext_state\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     10\u001B[0m state \u001B[38;5;241m=\u001B[39m next_state\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m done:\n",
      "Cell \u001B[0;32mIn[11], line 56\u001B[0m, in \u001B[0;36mActorCriticTrainer.train_loop\u001B[0;34m(self, state, action, reward, state_next)\u001B[0m\n\u001B[1;32m     54\u001B[0m policy_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcalculate_policy_loss(state, action, td_error\u001B[38;5;241m.\u001B[39mitem())\n\u001B[1;32m     55\u001B[0m entropy_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcalculate_entropy_loss(state, action,)\n\u001B[0;32m---> 56\u001B[0m loss \u001B[38;5;241m=\u001B[39m \u001B[43mvalue_loss\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mpolicy_loss\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mentropy_loss\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     58\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f66f44",
   "metadata": {},
   "source": [
    "## 实验记录\n",
    "2023-3-6\n",
    "1、添加了entropy loss\n",
    "结果：单纯添加上entropy_loss会减慢模型收敛速度。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9cef0e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%s\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b91b35",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
