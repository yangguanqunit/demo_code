{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89871d9f-c6de-488c-8e59-74660ff43dc9",
   "metadata": {},
   "source": [
    "# **一个Actor-Critic算法的简单实现**\n",
    "## **算法概述**\n",
    "- 一个比较典型的Actor-Critic算法的简单实现；\n",
    "- 这类算法融合了Value Based 和 Policy Based方法，其基本范式为AC算法的模型有两类输出头，一类输出价值（状态价值、动作价值、TD误差、优势函数及其他价值等）（以下代码使用TD无误差作为价值），另一类直接输出动作（Softmax或者log_softmax）。此举主要是为了扩展Policy Based的应用场景（因为Policy Based往往会采用MC方法来近似计算价值，但由于有些场景状态长度是无限的，所以没法用MC计算，但可以用价值网络来估计）。\n",
    "- 算法网络直接输出动作（包括离散动作的softmax或者直接输出连续动作），以及输出价值。\n",
    "- on-policy和off-policy都有，AC算法\n",
    "\n",
    "论文链接：*https://docs.popo.netease.com/docs/502c4f53c35444afbce8ef384a2474ed*# (AC算法的理论基础)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bfffcf1c-655c-40ca-81cb-d4091d7db7ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import platform\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3df9b5ac",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if platform.system() == \"Darwin\":\n",
    "    PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93546163-3b36-42b0-863f-a84c7b787f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticTrainer(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(ActorCriticTrainer, self).__init__()\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.create_training_network()\n",
    "        self.create_training_method()\n",
    "        self.GAMMA = 0.9\n",
    "        self.to(device)\n",
    "\n",
    "    def create_training_network(self):\n",
    "        self.fc = nn.Linear(self.state_dim, 20)\n",
    "        self.critic = nn.Sequential(self.fc,nn.ReLU(),nn.Linear(20,1))\n",
    "        self.actor = nn.Sequential(self.fc,nn.ReLU(),nn.Linear(20, self.action_dim))\n",
    "\n",
    "    def create_training_method(self):\n",
    "        self.optim = optim.Adam(self.parameters(),lr=0.001)\n",
    "        self.value_loss = nn.MSELoss()\n",
    "        self.actor_loss = nn.LogSoftmax(dim=-1)\n",
    "    \n",
    "    def choose_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, device=device)\n",
    "            action_probs = F.softmax(self.actor(state), dim=-1)\n",
    "            action = torch.multinomial(action_probs, 1).item()\n",
    "            return action\n",
    "\n",
    "    def calculate_td_error(self, state, reward, next_state):\n",
    "        next_value = self.critic(next_state)\n",
    "        value = self.critic(state)\n",
    "        td_error = reward + self.GAMMA * next_value - value\n",
    "        return td_error\n",
    "\n",
    "    def calculate_policy_loss(self, state, action, td_error):\n",
    "        action_logits = self.actor(state)\n",
    "        log_probs = torch.log(F.softmax(action_logits, dim=-1))\n",
    "        action_log_probs = torch.gather(log_probs,0,action)\n",
    "        return action_log_probs * td_error\n",
    "    \n",
    "    def train_loop(self, state, action, reward, state_next):\n",
    "        state = torch.tensor(state, device=device)\n",
    "        action = torch.tensor(action, device=device)\n",
    "        reward = torch.tensor(reward, device=device)\n",
    "        next_state = torch.tensor(state_next, device=device)\n",
    "        td_error = self.calculate_td_error(state, reward, next_state)\n",
    "        value_loss = torch.square(td_error)\n",
    "        policy_loss = self.calculate_policy_loss(state, action, td_error.item())\n",
    "        loss = value_loss - policy_loss\n",
    "        self.optim.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24dc855e-bd20-406f-9fd1-41ef90ab5aae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "agent = ActorCriticTrainer(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1083f4b-b438-469a-a14d-795aaca45cb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    for episode in range(3000):\n",
    "        state, _ = env.reset()\n",
    "        for step in range(300):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = -1 if done else 0.01\n",
    "            agent.train_loop(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        if episode % 100 == 0:\n",
    "            total_reward = 0\n",
    "            for i in range(10):\n",
    "                state, _ = env.reset()\n",
    "                for step in range(300):\n",
    "                    action = agent.choose_action(state)\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "            print(f\"episode {episode} total reward is {total_reward/10}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"total time is {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5811bae0-61a6-4e5c-991b-b3036b3c2edb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ygq/miniconda3/envs/dalle2/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 0 total reward is 19.6\n",
      "episode 100 total reward is 33.7\n",
      "episode 200 total reward is 42.8\n",
      "episode 300 total reward is 70.4\n",
      "episode 400 total reward is 224.8\n",
      "episode 500 total reward is 83.3\n",
      "episode 600 total reward is 108.2\n",
      "episode 700 total reward is 122.4\n",
      "episode 800 total reward is 129.3\n",
      "episode 900 total reward is 264.9\n",
      "episode 1000 total reward is 288.6\n",
      "episode 1100 total reward is 300.0\n",
      "episode 1200 total reward is 91.2\n",
      "episode 1300 total reward is 124.8\n",
      "episode 1400 total reward is 102.4\n",
      "episode 1500 total reward is 71.7\n",
      "episode 1600 total reward is 90.0\n",
      "episode 1700 total reward is 117.2\n",
      "episode 1800 total reward is 105.4\n",
      "episode 1900 total reward is 114.2\n",
      "episode 2000 total reward is 108.8\n",
      "episode 2100 total reward is 105.8\n",
      "episode 2200 total reward is 119.4\n",
      "episode 2300 total reward is 120.2\n",
      "episode 2400 total reward is 150.5\n",
      "episode 2500 total reward is 126.9\n",
      "episode 2600 total reward is 131.3\n",
      "episode 2700 total reward is 100.9\n",
      "episode 2800 total reward is 123.3\n",
      "episode 2900 total reward is 139.9\n",
      "total time is 1040.2221496105194\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabc62c0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalle2",
   "language": "python",
   "name": "dalle2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
