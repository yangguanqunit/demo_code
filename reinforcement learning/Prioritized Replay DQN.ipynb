{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1dc442-a144-4771-866a-90d58f49fefb",
   "metadata": {},
   "source": [
    "# **一个Prioritized Replay DQN算法的简单实现**\n",
    "## **算法概述**\n",
    "- Double DQN算法的改进版。\n",
    "- 除了使用目标网络和策略网络两个网络外，还使用了Prioritized Replay Buffer。\n",
    "- Prioritized Replay Buffer的核心功能由SumTree提供，SumTree可以存储带有权重数据，并且可以依照权重的大小重新采样这些数据。在实际存储时，会先以相同的权重将一部分样本存储在SumTree种，然后采样一组样本进行训练，计算这组样本中各项的TD error然后以TD error为新权重更新SumTree。\n",
    "- off-policy算法，value-based算法。\n",
    "\n",
    "论文链接：*https://docs.popo.netease.com/ofedit/0000cf9bc83b4ce087d1ecc32b8c7090*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b54b33a-dbed-4705-8874-a3368d2873af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "83d56a80-be83-4cb2-a3fb-eedb9ffb09b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree:\n",
    "    writer = 0\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(capacity * 2 - 1) # 树中总的节点数\n",
    "        self.data = np.zeros(capacity, dtype=object) # 存储权重数据的节点\n",
    "    \n",
    "    def _propagate(self, idx, change):\n",
    "        parent = (idx - 1) // 2\n",
    "        self.tree[parent] += change\n",
    "        \n",
    "        if parent != 0:\n",
    "            self._propagate(parent, change)\n",
    "    \n",
    "    def _retrieve(self, idx, s):\n",
    "        left = 2 * idx + 1\n",
    "        right = left + 1\n",
    "        \n",
    "        if left >= len(self.tree):\n",
    "            return idx\n",
    "        \n",
    "        if self.tree[left] >= s:\n",
    "            return self._retrieve(left, s)\n",
    "        else:\n",
    "            return self._retrieve(right, s-self.tree[left])\n",
    "    @property\n",
    "    def total(self):\n",
    "        return self.tree[0]\n",
    "    \n",
    "    def add(self, p, data):\n",
    "        idx = self.capacity - 1 + self.writer\n",
    "        \n",
    "        self.data[self.writer] = data\n",
    "        self.update(idx, p)\n",
    "        \n",
    "        self.writer += 1\n",
    "        if self.writer >= self.capacity:\n",
    "            self.writer = 0\n",
    "    \n",
    "    def update(self, idx, p):\n",
    "        change = p - self.tree[idx]\n",
    "        \n",
    "        self.tree[idx] = p\n",
    "        self._propagate(idx, change)\n",
    "    \n",
    "    def get(self, s):\n",
    "        idx = self._retrieve(0, s)\n",
    "        dataIdx = idx - self.capacity + 1 # idx - number_of_none_leafnode\n",
    "        \n",
    "        return (idx, self.tree[idx], self.data[dataIdx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f6d7ad20-9427-4490-8844-edf37356c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory:\n",
    "    e = 0.01\n",
    "    a = 0.6\n",
    "    beta = 0.4\n",
    "    beta_increment_per_sampling = 0.001\n",
    "    abs_err_upper = 1.\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.tree = SumTree(capacity)\n",
    "        \n",
    "    def _getPriority(self, error):\n",
    "        return (error + self.e) ** self.a\n",
    "    \n",
    "    def add(self, sample):\n",
    "        max_p = np.max(self.tree.tree[-self.tree.capacity:])\n",
    "        if max_p == 0:\n",
    "            max_p = self.abs_err_upper\n",
    "        # p = self._getPriority(error)\n",
    "        self.tree.add(max_p, sample)\n",
    "        \n",
    "    \n",
    "    def sample(self, n):\n",
    "        b_idx, batch, ISWeights = np.empty((n,), dtype=np.int32), [], np.empty((n, 1))\n",
    "        segment = self.tree.total / n\n",
    "        self.beta = np.min([1., self.beta + self.beta_increment_per_sampling])\n",
    "        \n",
    "        min_prob = np.max([np.min(self.tree.tree[-self.tree.capacity:]) / self.tree.total, 0.00001])\n",
    "        for i in range(n):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            \n",
    "            s = random.uniform(a, b)\n",
    "            (idx, p, data) = self.tree.get(s)\n",
    "            prob = p / self.tree.total\n",
    "            ISWeights[i, 0] = np.power(prob/min_prob, -self.beta)\n",
    "            b_idx[i] = idx\n",
    "            batch.append(data)\n",
    "    \n",
    "        return  batch, b_idx, ISWeights\n",
    "        \n",
    "    \n",
    "    def update(self, idx, abs_err):\n",
    "        abs_err += 0.1\n",
    "        clipped_err = np.minimum(abs_err,self.abs_err_upper)\n",
    "        ps = np.power(clipped_err,self.a)\n",
    "        for ti, p in zip(idx, ps):\n",
    "            self.tree.update(ti, p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7806955e-5e1c-4dc7-9db5-8bbdcd3b42cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6aa5f51e-355b-4f03-81b5-33003d1a3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "epsilon = 0.5\n",
    "start_epsilon = 0.5\n",
    "end_epsilon = 0.01\n",
    "replay_size = 10000\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05b772b1-59a2-44b6-b32f-80616d3b1e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PRDQN(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(PRDQN, self).__init__()\n",
    "        self.prioritized_replay_buffer = Memory(replay_size)\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.create_training_network()\n",
    "        self.create_training_method()\n",
    "        self.replay_total = 0\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        self.state_action_value = self.fc2(x)\n",
    "        return self.state_action_value\n",
    "    \n",
    "    def create_training_network(self):\n",
    "        self.fc1 = nn.Linear(self.state_dim, 20)\n",
    "        self.fc2 = nn.Linear(20, self.action_dim)\n",
    "        \n",
    "    def create_training_method(self):\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=0.0001)\n",
    "        self.loss_cal = F.mse_loss\n",
    "        \n",
    "    def get_target_network(self, target_network):\n",
    "        self.target_network = target_network\n",
    "        \n",
    "    def train_loop(self):\n",
    "        # self.eval()\n",
    "        minibatch, tree_idx, ISWeights = self.prioritized_replay_buffer.sample(batch_size)\n",
    "        state = [data[0] for data in minibatch] \n",
    "        action = [data[1] for data in minibatch]\n",
    "        reward = [data[2] for data in minibatch]\n",
    "        next_state = [data[3] for data in minibatch]\n",
    "        done = [data[4] for data in minibatch]\n",
    "        \n",
    "        # Q(S,A) = Q(S,A) + alpha*(R+gamma*Qmax(S',a) - Q(S,A)) \n",
    "        with torch.no_grad():\n",
    "            next_state = torch.tensor(np.stack(next_state), device=device)\n",
    "            action_max_Q = torch.argmax(self(next_state), dim=1).unsqueeze(-1) # 得到策略网络输出Q值最大的动作\n",
    "            Q_max_value_batch = self.target_network(next_state).gather(1, action_max_Q).squeeze(-1) # 从目标网络中获取该动作对应的Q值        action = torch.tensor(action_batch, device=device).unsqueeze(-1) \n",
    "        action = torch.tensor(action, device=device).unsqueeze(-1)\n",
    "        reward = torch.tensor(reward, device=device) # \n",
    "        done = torch.tensor(done, device=device)\n",
    "        \n",
    "        state = torch.tensor(np.stack(state), device=device)\n",
    "        y_batch = torch.where(done,reward,reward + GAMMA * Q_max_value_batch) # 计算目标Q值\n",
    "        Q_batch = self(state).gather(1, action).squeeze(-1)\n",
    "        y_batch_with_W = y_batch * torch.tensor(ISWeights, device=device).squeeze(-1)\n",
    "        Q_batch_with_W = Q_batch * torch.tensor(ISWeights, device=device).squeeze(-1)\n",
    "        \n",
    "        loss = self.loss_cal(Q_batch_with_W, y_batch_with_W)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        abs_err = torch.abs(y_batch-Q_batch).detach().cpu().numpy()\n",
    "        self.prioritized_replay_buffer.update(tree_idx,abs_err)\n",
    "        \n",
    "    def epsilon_greedy(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.from_numpy(state).to(device)\n",
    "            if random.random() > epsilon:\n",
    "                state_action_value = self(state)\n",
    "                action = torch.argmax(state_action_value).item()\n",
    "            else:\n",
    "                action = np.random.randint(0, 2)\n",
    "            # epsilon = epsilon - (start_epsilon - end_epsilon) / 10000\n",
    "            return action\n",
    "    \n",
    "    def action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.from_numpy(state).to(device)\n",
    "            state_action_value = self(state)\n",
    "            return torch.argmax(state_action_value).item()\n",
    "    \n",
    "    def perceive(self, state, action, reward, next_state, done):\n",
    "        self.prioritized_replay_buffer.add((state, action, reward, next_state, done))\n",
    "        self.replay_total += 1\n",
    "        if self.replay_total > batch_size:\n",
    "            self.train_loop()\n",
    "            \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.state_dict(),strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3120af14-8465-4252-a3b1-1716ac4c41ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PRDQN(\n",
       "  (fc1): Linear(in_features=4, out_features=20, bias=True)\n",
       "  (fc2): Linear(in_features=20, out_features=2, bias=True)\n",
       "  (target_network): PRDQN(\n",
       "    (fc1): Linear(in_features=4, out_features=20, bias=True)\n",
       "    (fc2): Linear(in_features=20, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "agent = PRDQN(env)\n",
    "target_network = PRDQN(env)\n",
    "agent.get_target_network(target_network)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0c7c899e-7eb9-4807-bc18-1aef13cb9431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for episode in range(3000):\n",
    "        state, _ = env.reset()\n",
    "        for step in range(300):\n",
    "            action = agent.epsilon_greedy(state)\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            reward = -1 if done else 0.01\n",
    "            agent.perceive(state,action,reward,next_state,done)\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "        if episode % 100 == 0:\n",
    "            total_reward = 0\n",
    "            for i in range(10):\n",
    "                state, _ = env.reset()\n",
    "                for step in range(300):\n",
    "                    action = agent.action(state)\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    if done:\n",
    "                        break\n",
    "                    state = next_state\n",
    "            total_reward /= 10\n",
    "            print(f\"average reward is {total_reward}\")\n",
    "            agent.update_target_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8386617f-35d7-4886-b60a-e65c707b5dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward is 12.4\n",
      "average reward is 9.3\n",
      "average reward is 13.1\n",
      "average reward is 14.4\n",
      "average reward is 147.7\n",
      "average reward is 209.0\n",
      "average reward is 210.9\n",
      "average reward is 187.1\n",
      "average reward is 186.3\n",
      "average reward is 203.1\n",
      "average reward is 215.7\n",
      "average reward is 196.5\n",
      "average reward is 203.3\n",
      "average reward is 189.8\n",
      "average reward is 209.7\n",
      "average reward is 204.4\n",
      "average reward is 232.4\n",
      "average reward is 213.5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[29], line 8\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m      7\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mperceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[27], line 81\u001b[0m, in \u001b[0;36mPRDQN.perceive\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplay_total \u001b[38;5;241m>\u001b[39m batch_size:\n\u001b[0;32m---> 81\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 30\u001b[0m, in \u001b[0;36mPRDQN.train_loop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_loop\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;66;03m# self.eval()\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m     minibatch, tree_idx, ISWeights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprioritized_replay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     state \u001b[38;5;241m=\u001b[39m [data[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m minibatch] \n\u001b[1;32m     32\u001b[0m     action \u001b[38;5;241m=\u001b[39m [data[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m minibatch]\n",
      "Cell \u001b[0;32mIn[24], line 25\u001b[0m, in \u001b[0;36mMemory.sample\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m     23\u001b[0m b_idx, batch, ISWeights \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mempty((n,), dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mint32), [], np\u001b[38;5;241m.\u001b[39mempty((n, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     24\u001b[0m segment \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mtotal \u001b[38;5;241m/\u001b[39m n\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbeta_increment_per_sampling\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m min_prob \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax([np\u001b[38;5;241m.\u001b[39mmin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mtree[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mcapacity:]) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mtotal, \u001b[38;5;241m0.00001\u001b[39m])\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n):\n",
      "File \u001b[0;32m<__array_function__ internals>:179\u001b[0m, in \u001b[0;36mamin\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c8fc06-4468-4e3c-bef6-ac3b890670c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalle2",
   "language": "python",
   "name": "dalle2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
