{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d8ea7d8-7ce6-4771-bfd3-095331fb76b8",
   "metadata": {},
   "source": [
    "# **一个Double DQN算法的简单实现**\n",
    "## **算法概述**\n",
    "- Narture DQN算法的改进版\n",
    "- 使用了两个网络，目标网络和策略网络\n",
    "    - 目标网络用于提供策略网络需要学习的Q值目标(Qmax(S',a))，策略网络则用于学习更新\n",
    "    - 两个网络的结构完全相同，目标网络参数固定，每隔X步将策略网络的参数更新到目标网络\n",
    "- 与Nature DQN不同的是，Nature DQN中目标网络输出N个Q值（如果有N个动作的话，每个动作一个Q值），选取最大的Q值作为目标Q值的计算项；而在Double DQN中，需要先在策略网络中选择Q值最大的动作，而后在目标网络中选取该动作对应的Q值，并将该Q值作为目标Q值的计算项。\n",
    "- 此举是因为Nature DQN始终选择Q值最大的动作容易导致Q值过估计的问题，最终的结果会有较大的偏差，而将Q值动作的选择和Q值计算解耦可以消除过估计的问题\n",
    "- off-policy算法，value-based算法\n",
    "\n",
    "论文链接：*https://docs.popo.netease.com/docs/0f6d78d83bfa4d63baa73f99cf622543*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad29917-6c65-4d2d-9c85-86496bd2fd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad8cee91-0efb-4387-ae29-a3b22845f482",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA = 0.9\n",
    "epsilon = 0.5\n",
    "start_epsilon = 0.5\n",
    "end_epsilon = 0.01\n",
    "replay_size = 10000\n",
    "batch_size = 32\n",
    "N_ACTIONS = 2\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6537396b-c6dd-4f56-97b5-b63b7b63e5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDQN(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.replay_buffer = deque()\n",
    "        self.epsilon = start_epsilon\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "        self.create_training_network()\n",
    "        self.create_training_method()\n",
    "        self.to(device)\n",
    "        \n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        self.state_action_value = self.fc2(x)\n",
    "        return self.state_action_value\n",
    "    \n",
    "    def create_training_network(self):\n",
    "        self.fc1 = nn.Linear(self.state_dim, 20)\n",
    "        self.fc2 = nn.Linear(20, self.action_dim)\n",
    "    \n",
    "    def create_training_method(self):\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=1e-4)\n",
    "        self.loss_cal = F.mse_loss\n",
    "    \n",
    "    def get_target_network(self, target_network):\n",
    "        self.target_network = target_network\n",
    "        self.target_network.load_state_dict(self.state_dict(), strict=False)\n",
    "    \n",
    "    def train_loop(self):\n",
    "        minibatch = random.sample(self.replay_buffer, batch_size)\n",
    "        state = [data[0] for data in minibatch] \n",
    "        action = [data[1] for data in minibatch]\n",
    "        reward = [data[2] for data in minibatch]\n",
    "        next_state = [data[3] for data in minibatch]\n",
    "        done = [data[4] for data in minibatch]\n",
    "        with torch.no_grad():\n",
    "            next_state = torch.tensor(np.stack(next_state), device=device)\n",
    "            action_max_Q = torch.argmax(self(next_state), dim=1).unsqueeze(-1) # 得到策略网络输出Q值最大的动作\n",
    "            Q_max_value_batch = self.target_network(next_state).gather(1, action_max_Q).squeeze(-1) # 从目标网络中获取该动作对应的Q值\n",
    "        action = torch.tensor(action, device=device).unsqueeze(-1)\n",
    "        reward = torch.tensor(reward, device=device)\n",
    "        done = torch.tensor(done, device=device)\n",
    "        \n",
    "        state = torch.tensor(np.stack(state), device=device)\n",
    "        y_batch = torch.where(done,reward,reward + GAMMA * Q_max_value_batch) # 计算目标Q值\n",
    "        Q_batch = self(state).gather(1, action).squeeze(-1)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_cal(Q_batch, y_batch)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "            \n",
    "    def epsilon_greedy(self, state):\n",
    "        with torch.no_grad():\n",
    "            if random.random() > self.epsilon:\n",
    "                state = torch.tensor(state, device=device)\n",
    "                state_action_value = self(state)\n",
    "                action = torch.argmax(state_action_value, dim=-1).item()\n",
    "            else:\n",
    "                action = np.random.randint(0, N_ACTIONS)\n",
    "            self.epsilon -= (start_epsilon - end_epsilon)/10000\n",
    "            return action\n",
    "                \n",
    "    def action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, device=device)\n",
    "            action = torch.argmax(self(state), dim=-1).item()\n",
    "            return action\n",
    "        \n",
    "    \n",
    "    def perceive(self, state, action, reward, next_state, done):\n",
    "        self.replay_buffer.append((state, action, reward, next_state, done))\n",
    "        if len(self.replay_buffer) > replay_size:\n",
    "            self.replay_buffer.popleft()\n",
    "        \n",
    "        if len(self.replay_buffer) > batch_size:\n",
    "            self.train_loop()\n",
    "            \n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0fcc39-2e24-44bd-9134-b8f35842e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env_name = \"CartPole-v1\"\n",
    "env = gym.make(env_name)\n",
    "agent = DDQN(env)\n",
    "target_network = DDQN(env)\n",
    "agent.get_target_network(target_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4733704b-426c-4e6f-9f23-29229e0f3522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    for episode in range(3000):\n",
    "        state, _ = env.reset()\n",
    "        for step in range(300):\n",
    "            action = agent.epsilon_greedy(state)\n",
    "            next_state, reward, done, _, _= env.step(action)\n",
    "            reward = -1 if done else 0.01\n",
    "            agent.perceive(state, action, reward, next_state, done)\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        if episode % 100 == 0:\n",
    "            print(f\"testing episode {episode / 100}\")\n",
    "            total_reward = 0\n",
    "            for i in range(10):\n",
    "                state, _ = env.reset()\n",
    "                for j in range(300):\n",
    "                    action = agent.action(state)\n",
    "                    next_state, reward, done, _, _ = env.step(action)\n",
    "                    total_reward += reward\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "            total_reward /= 10\n",
    "            print(f\"average reward is {total_reward}\")\n",
    "        if episode % 100 == 0 and episode != 0:\n",
    "            agent.update_target_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd38318c-4844-42fe-b094-d8d140d101dd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ygq/miniconda3/envs/dalle2/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing episode 0.0\n",
      "average reward is 15.2\n",
      "testing episode 1.0\n",
      "average reward is 12.7\n",
      "testing episode 2.0\n",
      "average reward is 27.0\n",
      "testing episode 3.0\n",
      "average reward is 65.5\n",
      "testing episode 4.0\n",
      "average reward is 47.8\n",
      "testing episode 5.0\n",
      "average reward is 169.4\n",
      "testing episode 6.0\n",
      "average reward is 208.5\n",
      "testing episode 7.0\n",
      "average reward is 267.5\n",
      "testing episode 8.0\n",
      "average reward is 29.9\n",
      "testing episode 9.0\n",
      "average reward is 300.0\n",
      "testing episode 10.0\n",
      "average reward is 60.6\n",
      "testing episode 11.0\n",
      "average reward is 300.0\n",
      "testing episode 12.0\n",
      "average reward is 114.1\n",
      "testing episode 13.0\n",
      "average reward is 298.5\n",
      "testing episode 14.0\n",
      "average reward is 173.5\n",
      "testing episode 15.0\n",
      "average reward is 141.0\n",
      "testing episode 16.0\n",
      "average reward is 166.0\n",
      "testing episode 17.0\n",
      "average reward is 99.6\n",
      "testing episode 18.0\n",
      "average reward is 113.9\n",
      "testing episode 19.0\n",
      "average reward is 132.3\n",
      "testing episode 20.0\n",
      "average reward is 117.2\n",
      "testing episode 21.0\n",
      "average reward is 127.5\n",
      "testing episode 22.0\n",
      "average reward is 157.4\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalle2",
   "language": "python",
   "name": "dalle2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
