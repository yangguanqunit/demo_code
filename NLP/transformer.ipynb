{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a4333d-5a9e-4ba7-a49c-1422e39130b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import platform\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bef4857-9c38-42b9-abbc-03ce5436e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == \"Darwin\":\n",
    "    PYTORCH_ENABLE_MPS_FALLBACK=1\n",
    "    device = \"mps\"\n",
    "else:\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79bba1a-19a6-4947-b603-41061575062d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, hidden, drop_prob=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938c2292-e112-47ac-9dce-637cbfb53315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "class ScaleDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaleDotProductAttention, self).__init__()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        \n",
    "    def forward(self, q, k, v, mask=None, e=1e-12):\n",
    "        # the input is [batch_size, head, length, dim]\n",
    "        # dim表示每个词向量的维度\n",
    "        batch_size, head, length, dim = k.size()\n",
    "        \n",
    "        k_t = k.transpose(2, 3)\n",
    "        # 为什么这里要进行scale？\n",
    "        # 从https://zhuanlan.zhihu.com/p/436614439可知，假设q，k是独立且均服从N(0,1)分布，则dot(q,k)得到的结果服从N(0,d_k)，d_k是q,k的维度，\n",
    "        # 当d_k过大时会导致方差过大，softmax的结果稀疏，从而使梯度稀疏。\n",
    "        score = (q @ k_t) / math.sqrt(dim)\n",
    "        if mask is not None:\n",
    "            score = score.masked_fill(mask == 0, -10000)\n",
    "        \n",
    "        score = self.softmax(score)\n",
    "        \n",
    "        v = score @ v # 15 * 15 @ 15 * 64 相当于对64个v特征进行加权平均\n",
    "        \n",
    "        return v, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02104303-d00f-4a8e-a158-149af6da49b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class transformer_blocks(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(transformer_blocks, self).__init__()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b3e84-4cf2-45c2-a11c-74a49719e39e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_head):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.attention = ScaleDotProductAttention()\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_concat = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        \n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        q, k, v = self.w_q(q), self.w_k(k), self.w_v(v)\n",
    "        \n",
    "        q, k, v = self.split(q), self.split(k), self.split(v)\n",
    "        \n",
    "        out, attention = self.attention(q, k, v, mask=mask)\n",
    "        out = self.concat(out)\n",
    "        out = self.w_concat(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def split(self, tensor):\n",
    "        \n",
    "        batch_size, length, d_model = tensor.size()\n",
    "        d_tensor = d_model // self.n_head\n",
    "        tensor = tensor.view(batch_size, length, self.n_head, d_tensor).transpose(1,2)\n",
    "        \n",
    "        return tensor\n",
    "    \n",
    "    def concat(self, tensor):\n",
    "        batch_size, head, length, dim = tensor.size()\n",
    "        d_model = head * dim\n",
    "        \n",
    "        tensor = tensor.transpose(1, 2).contiguous().view(batch_size, length, d_model)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f711d801-6831-441d-83d9-275e4ecf2469",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, d_model, eps=1e-12):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(d_model))\n",
    "        self.beta = nn.Parameter(torch.zeros(d_model))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, unbiased=False, keepdim=True) # unbiased是否使用无偏估计\n",
    "        \n",
    "        out = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        out = self.gamma * out + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f940cb5-db9b-4bd8-9a81-cf50e9f9c3a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len, device):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
    "        self.encoding.requires_grad = False\n",
    "        \n",
    "        pos = torch.arange(0, max_len, device=device)\n",
    "        pos = pos.float().unsqueeze(dim=-1)\n",
    "        \n",
    "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
    "        \n",
    "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
    "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len = x.size()\n",
    "        \n",
    "        return self.encoding[:seq_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24c75ca-59e5-4f9e-b7b0-e2b11329e1ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TokenEmbedding(nn.Embedding):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, device):\n",
    "        super(TokenEmbedding, self).__init__(vocab_size, d_model, padding_idx=1)\n",
    "        self.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30440aa-e8af-4309-a9af-813289b5247d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TransformerEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, max_len, drop_prob, device):\n",
    "        super(TransformerEmbedding, self).__init__()\n",
    "        self.tok_emb = TokenEmbedding(vocab_size, d_model, device)\n",
    "        self.pos_emb = PositionalEncoding(d_model, max_len, device)\n",
    "        self.drop_out = nn.Dropout(p=drop_prob)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        tok_emb = self.tok_emb(x)\n",
    "        pos_emb = self.pos_emb(x)\n",
    "        \n",
    "        return self.drop_out(tok_emb + pos_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f44ee0f-d2b4-49f5-994c-75c4a33f355b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob, device):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x, s_mask):\n",
    "        x_ = x\n",
    "        x = self.attention(q=x, k=x, v=x, mask=s_mask)\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + x_)\n",
    "        \n",
    "        x_ = x\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        x = self.dropout2(x)\n",
    "        x = self.norm2(x + x_)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class DecoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model, ffn_hidden, n_head, drop_prob):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm1 = LayerNorm(d_model=d_model)\n",
    "        self.dropout1 = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.enc_dec_attention = MultiHeadAttention(d_model=d_model, n_head=n_head)\n",
    "        self.norm2 = LayerNorm(d_model=d_model)\n",
    "        self.dropout2 = nn.Dropout(drop_prob)\n",
    "        \n",
    "        self.ffn = PositionwiseFeedForward(d_model=d_model, hidden=ffn_hidden, drop_prob=drop_prob)\n",
    "        self.norm3 = LayerNorm(d_model=d_model)\n",
    "        self.dropout3 = nn.Dropout(drop_prob)\n",
    "    \n",
    "    def forward(self, dec, enc, t_mask, s_mask):\n",
    "        x_ = dec\n",
    "        x = self.self_attention(q=dec, k=dec, v=dec, mask=t_mask)\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "        x = self.norm1(x + x_)\n",
    "        \n",
    "        if enc is not None:\n",
    "            x_ = x\n",
    "            x = self.enc_dec_attention(q=x, k=enc, v=enc, mask=s_mask)\n",
    "            \n",
    "            x = self.dropout2(x)\n",
    "            x = self.norm2(x + x_)\n",
    "        \n",
    "        x_ = x\n",
    "        x = self.ffn(x)\n",
    "        \n",
    "        x = self.dropout3(x)\n",
    "        x = self.norm3(x + x_)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6bc05-dd1e-4e51-b7b3-3e7c02da7ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                       max_len=max_len,\n",
    "                                       vocab_size=enc_voc_size,\n",
    "                                       drop_prob=drop_prob,\n",
    "                                       device=device)\n",
    "        self.layers = nn.ModuleList([EncoderLayer(d_model=d_model,\n",
    "                                                 ffn_hidden=ffn_hidden,\n",
    "                                                 n_head=n_head,\n",
    "                                                  drop_prob=drop_prob,\n",
    "                                                 device=device)\n",
    "                                                  for _ in range(n_layers)])\n",
    "        \n",
    "    def forward(self, x, s_mask=None):\n",
    "        x = self.emb(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, s_mask)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e7903a-a684-40fe-a455-8c62366d06b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, dec_voc_size, max_len, d_model, ffn_hidden, n_head, n_layers, drop_prob, device):\n",
    "        super().__init__()\n",
    "        self.emb = TransformerEmbedding(d_model=d_model,\n",
    "                                        drop_prob=drop_prob,\n",
    "                                        max_len=max_len,\n",
    "                                        vocab_size=dec_voc_size,\n",
    "                                        device=device)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(d_model=d_model,\n",
    "                                                 ffn_hidden=ffn_hidden,\n",
    "                                                 n_head=n_head,\n",
    "                                                 drop_prob=drop_prob)\n",
    "                                    for _ in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, dec_voc_size)\n",
    "        \n",
    "    def forward(self, trg, enc_src, trg_mask, src_mask):\n",
    "        trg = self.emb(trg)\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            trg = layer(trg, enc_src, trg_mask, src_mask)\n",
    "        \n",
    "        output = self.fc(trg)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f1aa26-fb96-42eb-a649-cae5dde01508",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, src_pad_idx, trg_pad_idx, trg_sos_idx, enc_voc_size, dec_voc_size, d_model, n_head, max_len,\n",
    "                ffn_hidden, n_layers, drop_prob, device):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.trg_sos_idx = trg_sos_idx\n",
    "        self.device = device\n",
    "        self.encoder = Encoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               enc_voc_size=enc_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers,\n",
    "                               device=device)\n",
    "        \n",
    "        self.decoder = Decoder(d_model=d_model,\n",
    "                               n_head=n_head,\n",
    "                               max_len=max_len,\n",
    "                               ffn_hidden=ffn_hidden,\n",
    "                               dec_voc_size=dec_voc_size,\n",
    "                               drop_prob=drop_prob,\n",
    "                               n_layers=n_layers,\n",
    "                               device=device)\n",
    "    \n",
    "    def forward(self, src, trg=None):\n",
    "        src_mask = self.make_pad_mask(src, src, self.src_pad_idx, self.src_pad_idx)\n",
    "        # src_trg_mask = self.make_pad_mask(trg, src, self.trg_pad_idx, self.src_pad_idx)\n",
    "        # trg_mask = self.make_pad_mask(trg, trg, self.trg_pad_idx, self.trg_pad_idx) * self.make_no_peak_mask(trg, trg)\n",
    "        \n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        # output = self.decoder(trg, enc_src, trg_mask, src_trg_mask)\n",
    "        return enc_src\n",
    "    \n",
    "    def make_pad_mask(self, q, k, q_pad_idx, k_pad_idx):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        \n",
    "        k = k.ne(k_pad_idx).unsqueeze(1).unsqueeze(2)\n",
    "        k = k.repeat(1, 1, len_q, 1)\n",
    "        \n",
    "        q = q.ne(q_pad_idx).unsqueeze(1).unsqueeze(3)\n",
    "        q = q.repeat(1, 1, 1, len_k)\n",
    "        \n",
    "        mask = k & q\n",
    "        return mask\n",
    "    \n",
    "    def make_no_peak_mask(self, q, k):\n",
    "        len_q, len_k = q.size(1), k.size(1)\n",
    "        mask = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
    "        return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c88ed-0704-45e5-9467-3e38e4c728dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "max_len = 512\n",
    "d_model = 768\n",
    "ffn_hidden = 256\n",
    "drop_prob = 0.9\n",
    "n_head = 12\n",
    "n_layers = 12\n",
    "drop_porb = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998e1afb-568f-4be1-b600-81fa8910ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "\n",
    "# class Tokenizer:\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         self.spacy_de = spacy.load('de_core_news_sm')\n",
    "#         self.spacy_en = spacy.load('en_core_web_sm')\n",
    "        \n",
    "#     def tokenizer_de(self, text):\n",
    "#         return [tok.text for tok in self.spacy_de.tokenizer(text)]\n",
    "    \n",
    "#     def tokenizer_en(self, text):\n",
    "#         return [tok.text for tok in self.spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137a27fe-c0f8-42b1-9956-5086a6f0ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1708e41b-6edd-4c00-b2a0-36cccc17d032",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-zh\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6598fd5f-e5b9-467a-b406-ede2712184ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "# checkpoint = \"Helsinki-NLP/opus-mt-en-zh\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2009d7-2133-4881-9265-5834aebf997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed06b94a-0287-4ccc-814d-dd66e9900ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a this course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9325615e-0544-4d5b-b9bf-339acc8db2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73b1f02-7b71-4bf3-aef6-3a0e3ae590e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_pad_idx = tokenizer.pad_token_id\n",
    "trg_pad_idx = tokenizer.pad_token_id\n",
    "trg_sos_idx = tokenizer.eos_token_id\n",
    "\n",
    "enc_voc_size = tokenizer.vocab_size\n",
    "dec_voc_size = tokenizer.vocab_size\n",
    "\n",
    "d_model = 512\n",
    "decoder_attention_heads = 8\n",
    "decoder_ffn_dim = 2048\n",
    "decoder_layerdrop = 0\n",
    "decoder_layers = 6\n",
    "de_start_token_idx = 65000\n",
    "\n",
    "encoder_attention_heads = 8\n",
    "encoder_ffn_dim = 2048\n",
    "encoder_layerdrop = 0\n",
    "encoder_layers = 6\n",
    "eos_token_id = 0\n",
    "\n",
    "max_len = 512\n",
    "pad_token_id = 65000\n",
    "vocab_size = 65001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b2fcbe-4d0b-4d46-8ea3-4a230f3be8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(src_pad_idx=pad_token_id, \n",
    "                    trg_pad_idx=pad_token_id, \n",
    "                    trg_sos_idx=de_start_token_idx, \n",
    "                    enc_voc_size=vocab_size,\n",
    "                    dec_voc_size=vocab_size,\n",
    "                    d_model=d_model, \n",
    "                    n_head=encoder_attention_heads, \n",
    "                    max_len=max_len,\n",
    "                    ffn_hidden=encoder_ffn_dim,\n",
    "                    n_layers=encoder_layers,\n",
    "                    drop_prob=0,\n",
    "                    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ba85b-5b6d-4385-9013-797335cce75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_data = inputs['input_ids'].to(device)\n",
    "inputs_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cfc638-6190-4354-baaf-bf7baa94ddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(inputs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f87218-c534-4c93-8749-0c6bf2f182b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
