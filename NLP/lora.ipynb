{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18def39f-e2c2-4be9-b010-b4daf5109a0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5bc94d6-c9de-4579-884a-877898aa281e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LoraLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=False, r=4, dropout_p=0.1, scale=1.0):\n",
    "        super(LoraLinear, self).__init__()\n",
    "        assert r <= min(in_features, out_features), f\"LoRA rank {r} must be less or equal than {min(in_features, out_features)}\"\n",
    "        \n",
    "        self.r = r\n",
    "        self.linear = nn.Linear(in_features, out_features, bias)\n",
    "        self.lora_down = nn.Linear(in_features, r, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lora_up = nn.Linear(r, out_features, bias=False)\n",
    "        self.scale = scale\n",
    "        self.selector = nn.Identity()\n",
    "        \n",
    "        nn.init.normal_(self.lora_down.weight, std=1/r)\n",
    "        nn.init.zeros_(self.lora_up.weight)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return (\n",
    "            self.linear(input)\n",
    "            + self.dropout(self.lora_up(self.selector(self.lora_down(input))))\n",
    "            * self.scale\n",
    "        )\n",
    "    \n",
    "    def realize_as_lora(self):\n",
    "        return self.lora_up.weight.data * self.scale, self.lora_down.weight.data\n",
    "    \n",
    "    def set_selector_from_diag(self, diag: torch.Tensor):\n",
    "        assert diag.shape == (self.r, ) # diag是个一维向量，长度为r\n",
    "        self.selector = nn.Linear(self.r, self.r, bias=False)\n",
    "        self.selector.weight.data = torch.diag(diag) # 把selector的权重初始化为了一个对角矩阵？\n",
    "        self.selector.weight.data = self.selector.weight.data.to(\n",
    "            self.lora_up.weight.device\n",
    "        ).to(self.lora_up.weight.dtype)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32a4581e-1b64-4c75-b7de-3fa02017f5af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LoraConv2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size, stride=1, \n",
    "                 padding=0, dilation=1, groups: int=1, bias: bool=True, r: int=4, \n",
    "                 dropout_p: float=0.1, scale: float=1.0,):\n",
    "        super(LoraConv2d, self).__init__() #单继承情况下不用指明当前类和当前对象\n",
    "        assert r <= min(in_channels, out_channels), f\"LoRA rank {r} must be less or equal than {min(in_channels, out_channels)}\"\n",
    "        \n",
    "        self.r = r\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        \n",
    "        self.lora_down = nn.Conv2d(in_channels=in_channels, out_channels=r, kernel_size=kernel_size, stride=stride,\n",
    "                                  padding=padding, dilation=dilation, groups=groups, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lora_up = nn.Conv2d(in_channels=r, out_channels=out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        self.selector = nn.Identity()\n",
    "        self.scale = scale\n",
    "        \n",
    "        nn.init.normal_(self.lora_down.weight, std=1/r)\n",
    "        nn.init.zeros_(self.lora_up.weight)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return (\n",
    "            self.conv(input)\n",
    "            + self.dropout(self.lora_up(self.selector(self.lora_down(input))))\n",
    "            * self.scale\n",
    "        )\n",
    "    \n",
    "    def realize_as_lora(self):\n",
    "        return self.lora_up.weight.data * self.scale, self.lora_down.weight.data\n",
    "    \n",
    "    def set_selector_from_diag(self, diag: torch.Tensor):\n",
    "        assert diag.shape == (self.r,)\n",
    "        self.selector = nn.Conv2d(in_channels=self.r, out_channels=self.r, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.selector.weight.data = torch.diag(diag)\n",
    "        \n",
    "        self.selector.weight.data = self.selector.weight.data.to(\n",
    "            self.lora_up.weight.device\n",
    "        ).to(self.lora_up.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c4c1f5-12d5-432a-8b95-19d8d9a0665a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(in_channels=8, out_channels=32, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "conv.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b3a2c55-284d-404e-b6c5-bd30a038cf75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List, Type, Optional, Set\n",
    "DEFAULT_TARGET_REPLACE = {\"CrossAttention\", \"Attention\", \"GEGLU\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63c1fde1-2ca3-4a32-8d40-e984a2320d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _find_modules_v2(model, ancestor_class: Optional[Set[str]] = None, search_class: List[Type[nn.Module]] = [nn.Linear],\n",
    "                    exclude_children_of: Optional[List[Type[nn.Module]]] = [LoraLinear, LoraConv2d],): # Type动态创建类，也就是返回一个类，在3.10版本中用type()代替\n",
    "    if ancestor_class is not None:\n",
    "        ancestors = (\n",
    "            module for module in model.modules() if module.__class__ in ancestor_class\n",
    "        )\n",
    "    else:\n",
    "        ancestors = [module for module in model.modules()]\n",
    "        \n",
    "    for ancestor in ancestors:\n",
    "        for fullname, module in ancestor.named_modules():\n",
    "            if any([isinstance(module, _class) for _class in search_class]):\n",
    "                *path, name = fullname.split(\".\")\n",
    "                parent = ancestor\n",
    "                while path:\n",
    "                    parent = parent.get_submodule(path.pop(0))\n",
    "                if exclude_children_of and any([isinstance(parent, _class) for _class in exclude_children_of]):\n",
    "                    continue\n",
    "                yield parent, name, module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5af0a8d4-829c-4d9a-8f82-112e14997397",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inject_trainable_lora(model:nn.Module, \n",
    "                         target_replace_module: Set[str] = DEFAULT_TARGET_REPLACE,\n",
    "                         r: int=4,\n",
    "                         loras=None, # path to lora .pt\n",
    "                         verbose:bool=False,\n",
    "                         dropout_p:float=0.0,\n",
    "                         scale:float=1.0,):\n",
    "    require_grad_params = []\n",
    "    names = []\n",
    "    \n",
    "    if loras is not None:\n",
    "        loras = torch.load(loras)\n",
    "        \n",
    "    for _module, name, _child_module in _find_modules_v2(model, target_replace_module, search_class=[nn.Linear]):\n",
    "        weight = _child_module.weight\n",
    "        bias = _child_module.bias\n",
    "        if verbose:\n",
    "            print(\"LoRA Injection : injecting lora into \", name)\n",
    "            print(\"LoRA Injection : weight shape\", weight.shape)\n",
    "        _tmp = LoraLinear(_child_module.in_features, _child_module.out_features,_child_module.bias is not None, r=r,dropout_p=dropout_p, scale=scale)\n",
    "        # 这里是用原来linear中的参数替换掉新lora_linear中的参数\n",
    "        _tmp.linear.weight = weight\n",
    "        if bias is not None:\n",
    "            _tmp.linear.bias = bias\n",
    "            \n",
    "        # switch the module\n",
    "        _tmp.to(_child_module.weight.device).to(_child_module.weight.dtype)\n",
    "        _module._modules[name] = _tmp\n",
    "        \n",
    "        require_grad_params.append(_module._modules[name].lora_up.parameters())\n",
    "        require_grad_params.append(_module._modules[name].lora_down.parameters())\n",
    "        \n",
    "        if loras is not None:\n",
    "            _module._modules[name].lora_up.weight = loras.pop(0)\n",
    "            _module._modules[name].lora_down.weight = loras.pop(0)\n",
    "            \n",
    "        _module._modules[name].lora_up.weight.requires_grad = True\n",
    "        _module._modules[name].lora_down.weight.requires_grad = True\n",
    "        \n",
    "    return require_grad_params, names\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e22dd6c-6490-4705-9382-36819a546ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d890fb-f3a3-491a-bce2-f639e704723a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cannot initialize model with low cpu memory usage because `accelerate` was not found in the environment. Defaulting to `low_cpu_mem_usage=False`. It is strongly recommended to install `accelerate` for faster and less memory-intense model loading. You can do so with: \n",
      "```\n",
      "pip install accelerate\n",
      "```\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "# 下载的模型的名字是在缓存中随机值 ～/.cache/huggingface/hub\n",
    "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"unet\", revision=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08566046-50a3-4b1d-a558-91d7c16c06b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for module in unet.modules():\n",
    "    print(module)\n",
    "    print(\"-\"*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "36fb23b0-9b13-46d2-8913-fb80ee755f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7522a795-40ff-4395-aab3-ddfcdb2ad32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer():\n",
    "    def __init__(self, r: int, lora_alpha:int, lora_dropout: float, merge_weights: bool):\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        \n",
    "        if lora_dropout > 0.:\n",
    "            self.lora_dropout = nn.Dropout(p=lora_dropout)\n",
    "        else:\n",
    "            self.lora_dropout = lambda x: x\n",
    "        \n",
    "        self.merged = False\n",
    "        self.merge_weights = merge_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a4f5460-1c3b-4c56-9f60-94d3a859b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Embedding, LoRALayer):\n",
    "    def __init__(self, num_embeddings: int, embedding_dim: int, r: int=0, lora_alpha: int=1, merge_weights: bool=True, **kwargs):\n",
    "        pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2e070df4-8716-4632-adaa-42d615520540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 采用这种方法构建的LoraLinear up down组件不包含在modules里面\n",
    "class Linear(nn.Linear, LoRALayer):\n",
    "    \n",
    "    def __init__(self, in_features: int, out_features: int, r: int=0, lora_alpha: int=1, lora_dropout: float=0., \n",
    "                 fan_in_fan_out: bool=False, merge_weights: bool=True, **kwargs):\n",
    "        '''\n",
    "            fan_in_fan_out：为True的话就是XW，False是WX，两种计算方式，结果是相同的\n",
    "            merge_weights: 这个参数为True的话，计算起来是(W+BA)X这种方式，否则就是WX + BAX这种方法\n",
    "        '''\n",
    "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
    "        LoRALayer.__init__(self, r, lora_alpha, lora_dropout, merge_weights)\n",
    "        \n",
    "        self.fan_in_fan_out = fan_in_fan_out\n",
    "        \n",
    "        if r > 0:\n",
    "            self.lora_down = nn.Parameter(self.weight.new_zeros((r, in_features)))\n",
    "            self.lora_up = nn.Parameter(self.weight.new_zeros((out_features, r)))\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            \n",
    "            self.weight.requires_grad = False\n",
    "        \n",
    "        self.reset_parameters()\n",
    "        if fan_in_fan_out:\n",
    "            # 原本的weight.shape 是 (out,input)， 转置后为(input,out)\n",
    "            self.weight.data = self.weight.data.T\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.Linear.reset_parameters(self) # 这里是直接调用了nn.Linear类里面定义的函数，把self当作参数传进去\n",
    "        if hasattr(self,'lora_down'):\n",
    "            nn.init.kaiming_normal_(self.lora_down, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_up)\n",
    "    \n",
    "    def train(self, mode: bool=True):\n",
    "        def T(w):\n",
    "            return w.T if self.fan_in_fan_out else w\n",
    "        nn.Linear.train(self, mode)\n",
    "        if self.merge_weights and self.merged:\n",
    "            # 这里是啥意思？\n",
    "            # 把eval中merge的weight减出来\n",
    "            if self.r > 0:\n",
    "                self.weight.data -= T(self.lora_up @ self.lora_down) * self.scaling\n",
    "            self.merged = False\n",
    "            \n",
    "    def eval(self):\n",
    "        def T(w):\n",
    "            return w.T if self.fan_in_fan_out else w\n",
    "        nn.Linear.eval(self)\n",
    "        if self.merge_weights and not self.merged:\n",
    "            # merge_weights为True时，通过W+BA把参数融合在一起\n",
    "            if self.r > 0:\n",
    "                self.weight.data += T(self.lora_up @ self.lora_down) * self.scaling\n",
    "            self.merged = True\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        def T(w):\n",
    "            return w.T if self.fan_in_fan_out else w\n",
    "        if self.r > 0 and not self.merged:\n",
    "            result = F.linear(x, T(self.weight), bias=self.bias)\n",
    "            if self.r > 0:\n",
    "                result += (self.lora_dropout(x) @ self.lora_down.T @ self.lora_up.T) * self.scaling\n",
    "            return result\n",
    "        else:\n",
    "            return F.linear(x, T(self.weight), bias=self.bias)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89eed1c4-e5a4-4d43-8ed0-b0759681794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MergedLinear(nn.Linear, LoRALayer):\n",
    "    def __init__(self, in_features: int, out_features: int, r: int=0, lora_alpha: int=1, \n",
    "                 lora_dropout: float=0., enable_lora: List[bool]=[False], fan_in_fan_out: bool=False, \n",
    "                 merge_weights: bool=True, **kwargs):\n",
    "        nn.Linear.__init__(self, in_features, out_features, **kwargs)\n",
    "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, merge_weights=merge_weights)\n",
    "        assert out_features % len(enable_lora) == 0, \"The length of enable_lora must divide out_features\"\n",
    "        self.enable_lora = enable_lora\n",
    "        self.fan_in_fan_out = fan_in_fan_out\n",
    "        \n",
    "        if r > 0 and any(enable_lora):\n",
    "            # 这里的down和up没看懂是啥意思？\n",
    "            self.lora_down = nn.Parameter(self.weight.new_zeros((r * sum(enable_lor), in_features)))\n",
    "            self.lora_up = nn.Parameter(self.weight.new_zeros((out_features // len(enable_lora) * sum(enable_lora), r)))\n",
    "            self.scaling = self.lora_alpha / r\n",
    "            self.weight.requires_grad = False\n",
    "            \n",
    "            self.lora_ind = self.weight.new_zeros(\n",
    "                (out_features, ), dtype=torch.bool\n",
    "            ).view(len(enable_lora), -1)\n",
    "            self.lora_ind[enable_lora, :] = True\n",
    "            self.lora_ind = self.lora_ind.view(-1)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a5c93940-a6e5-450f-8267-9e1b9112b3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d(nn.Conv2d, LoRALayer):\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, r: int = 0, \n",
    "                 lora_alpha: int=1, lora_dropout: float=0., merge_weights: bool = True, **kwargs):\n",
    "        assert type(kernel_size) is int\n",
    "        nn.Conv2d.__init__(self, in_channels, out_channels, kernel_size, **kwargs)\n",
    "        LoRALayer.__init__(self, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, merge_weights=merge_weights)\n",
    "        if r > 0:\n",
    "            self.lora_down = nn.Parameter(\n",
    "                self.weight.new_zeros((r*kernel_size, in_channels*kernel_size))\n",
    "            )\n",
    "            self.lora_up = nn.Parameter(\n",
    "                self.weight.new_zeros((out_channels*kernel_size, r*kernel_size))\n",
    "            )\n",
    "            self.scaling = self.lora_alpha / self.r\n",
    "            self.weight.requires_grad = False\n",
    "        \n",
    "        self.reset_parameters()\n",
    "    \n",
    "    def reset_parameters(self):\n",
    "        nn.Conv2d.reset_parameters(self)\n",
    "        if hasattr(self, 'lora_down'):\n",
    "            nn.init.kaiming_normal_(self.lora_down, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_up)\n",
    "            \n",
    "    def train(self, mode: bool=True):\n",
    "        nn.Conv2d.train(self, mode)\n",
    "        if self.merge_weights and self.merged:\n",
    "            self.weight.data -= (self.lora_up @ self.lora_down).view(self.weight.shape) * self.scaling\n",
    "            self.merged = False\n",
    "    \n",
    "    def eval(self):\n",
    "        nn.Conv2d.eval(self)\n",
    "        if self.merge_weights and not self.merged:\n",
    "            self.weight.data += (self.lora_up @ self.lora_down).view(self.weight.shape) * self.scaling\n",
    "            self.merged = True\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        if self.r > 0 and self.merged:\n",
    "            return F.Conv2d(\n",
    "                x,\n",
    "                self.weight + (self.lora_up @ self.lora_down).view(self.weight.shape) * self.scaling,\n",
    "                self.bias, self.stride, self.padding, self.dilation, self.groups\n",
    "            )\n",
    "        \n",
    "        return nn.Conv2d.forward(self, x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "86b3ad05-0e03-4011-aa14-cbc98b7ab3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 32])\n"
     ]
    }
   ],
   "source": [
    "lora_fc = Linear(64,32, r=4, fan_in_fan_out=True)\n",
    "data = torch.randn((8, 64))\n",
    "output = lora_fc(data)\n",
    "print(output.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
