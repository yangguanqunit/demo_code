{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18def39f-e2c2-4be9-b010-b4daf5109a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5bc94d6-c9de-4579-884a-877898aa281e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=False, r=4, dropout_p=0.1, scale=1.0):\n",
    "        super(LoraLinear, self).__init__()\n",
    "        assert r <= min(in_features, out_features), f\"LoRA rank {r} must be less or equal than {min(in_features, out_features)}\"\n",
    "        \n",
    "        self.r = r\n",
    "        self.linear = nn.Linear(in_features, out_features, bias)\n",
    "        self.lora_down = nn.Linear(in_features, r, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lora_up = nn.Linear(r, out_features, bias=False)\n",
    "        self.scale = scale\n",
    "        self.selector = nn.Identity()\n",
    "        \n",
    "        nn.init.normal_(self.lora_down.weight, std=1/r)\n",
    "        nn.init.zeros_(self.lora_up.weight)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return (\n",
    "            self.linear(input)\n",
    "            + self.dropout(self.lora_up(self.selector(self.lora_down(input))))\n",
    "            * self.scale\n",
    "        )\n",
    "    \n",
    "    def realize_as_lora(self):\n",
    "        return self.lora_up.weight.data * self.scale, self.lora_down.weight.data\n",
    "    \n",
    "    def set_selector_from_diag(self, diag: torch.Tensor):\n",
    "        assert diag.shape == (self.r, ) # diag是个一维向量，长度为r\n",
    "        self.selector = nn.Linear(self.r, self.r, bias=False)\n",
    "        self.selector.weight.data = torch.diag(diag) # 把selector的权重初始化为了一个对角矩阵？\n",
    "        self.selector.weight.data = self.selector.weight.data.to(\n",
    "            self.lora_up.weight.device\n",
    "        ).to(self.lora_up.weight.dtype)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "32a4581e-1b64-4c75-b7de-3fa02017f5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraConv2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size, stride=1, \n",
    "                 padding=0, dilation=1, groups: int=1, bias: bool=True, r: int=4, \n",
    "                 dropout_p: float=0.1, scale: float=1.0,):\n",
    "        super(LoraConv2d, self).__init__() #单继承情况下不用指明当前类和当前对象\n",
    "        assert r <= min(in_channels, out_channels), f\"LoRA rank {r} must be less or equal than {min(in_channels, out_channels)}\"\n",
    "        \n",
    "        self.r = r\n",
    "        self.conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride,\n",
    "                              padding=padding, dilation=dilation, groups=groups, bias=bias)\n",
    "        \n",
    "        self.lora_down = nn.Conv2d(in_channels=in_channels, out_channels=r, kernel_size=kernel_size, stride=stride,\n",
    "                                  padding=padding, dilation=dilation, groups=groups, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        self.lora_up = nn.Conv2d(in_channels=r, out_channels=out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        self.selector = nn.Identity()\n",
    "        self.scale = scale\n",
    "        \n",
    "        nn.init.normal_(self.lora_down.weight, std=1/r)\n",
    "        nn.init.zeros_(self.lora_up.weight)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        return (\n",
    "            self.conv(input)\n",
    "            + self.dropout(self.lora_up(self.selector(self.lora_down(input))))\n",
    "            * self.scale\n",
    "        )\n",
    "    \n",
    "    def realize_as_lora(self):\n",
    "        return self.lora_up.weight.data * self.scale, self.lora_down.weight.data\n",
    "    \n",
    "    def set_selector_from_diag(self, diag: torch.Tensor):\n",
    "        assert diag.shape == (self.r,)\n",
    "        self.selector = nn.Conv2d(in_channels=self.r, out_channels=self.r, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.selector.weight.data = torch.diag(diag)\n",
    "        \n",
    "        self.selector.weight.data = self.selector.weight.data.to(\n",
    "            self.lora_up.weight.device\n",
    "        ).to(self.lora_up.weight.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41c4c1f5-12d5-432a-8b95-19d8d9a0665a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 8, 3, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = nn.Conv2d(in_channels=8, out_channels=32, kernel_size=3, stride=1, padding=0, bias=False)\n",
    "conv.weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9b3a2c55-284d-404e-b6c5-bd30a038cf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Type, Optional, Set\n",
    "DEFAULT_TARGET_REPLACE = {\"CrossAttention\", \"Attention\", \"GEGLU\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63c1fde1-2ca3-4a32-8d40-e984a2320d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_modules_v2(model, ancestor_class: Optional[Set[str]] = None, search_class: List[Type[nn.Module]] = [nn.Linear],\n",
    "                    exclude_children_of: Optional[List[Type[nn.Module]]] = [LoraLinear, LoraConv2d],): # Type动态创建类，也就是返回一个类，在3.10版本中用type()代替\n",
    "    if ancestor_class is not None:\n",
    "        ancestors = (\n",
    "            module for module in model.modules() if module.__class__ in ancestor_class\n",
    "        )\n",
    "    else:\n",
    "        ancestors = [module for module in model.modules()]\n",
    "        \n",
    "    for ancestor in ancestors:\n",
    "        for fullname, module in ancestor.named_modules():\n",
    "            if any([isinstance(module, _class) for _class in search_class]):\n",
    "                *path, name = fullname.split(\".\")\n",
    "                parent = ancestor\n",
    "                while path:\n",
    "                    parent = parent.get_submodule(path.pop(0))\n",
    "                if exclude_children_of and any([isinstance(parent, _class) for _class in exclude_children_of]):\n",
    "                    continue\n",
    "                yield parent, name, module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5af0a8d4-829c-4d9a-8f82-112e14997397",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_trainable_lora(model:nn.Module, \n",
    "                         target_replace_module: Set[str] = DEFAULT_TARGET_REPLACE,\n",
    "                         r: int=4,\n",
    "                         loras=None, # path to lora .pt\n",
    "                         verbose:bool=False,\n",
    "                         dropout_p:float=0.0,\n",
    "                         scale:float=1.0,):\n",
    "    require_grad_params = []\n",
    "    names = []\n",
    "    \n",
    "    if loras is not None:\n",
    "        loras = torch.load(loras)\n",
    "        \n",
    "    for _module, name, _child_module in _find_modules_v2(model, target_replace_module, search_class=[nn.Linear]):\n",
    "        weight = _child_module.weight\n",
    "        bias = _child_module.bias\n",
    "        if verbose:\n",
    "            print(\"LoRA Injection : injecting lora into \", name)\n",
    "            print(\"LoRA Injection : weight shape\", weight.shape)\n",
    "        _tmp = LoraLinear(_child_module.in_features, _child_module.out_features,_child_module.bias is not None, r=r,dropout_p=dropout_p, scale=scale)\n",
    "        # 这里是用原来linear中的参数替换掉新lora_linear中的参数\n",
    "        _tmp.linear.weight = weight\n",
    "        if bias is not None:\n",
    "            _tmp.linear.bias = bias\n",
    "            \n",
    "        # switch the module\n",
    "        _tmp.to(_child_module.weight.device).to(_child_module.weight.dtype)\n",
    "        _module._modules[name] = _tmp\n",
    "        \n",
    "        require_grad_params.append(_module._modules[name].lora_up.parameters())\n",
    "        require_grad_params.append(_module._modules[name].lora_down.parameters())\n",
    "        \n",
    "        if loras is not None:\n",
    "            _module._modules[name].lora_up.weight = loras.pop(0)\n",
    "            _module._modules[name].lora_down.weight = loras.pop(0)\n",
    "            \n",
    "        _module._modules[name].lora_up.weight.requires_grad = True\n",
    "        _module._modules[name].lora_down.weight.requires_grad = True\n",
    "        \n",
    "    return require_grad_params, names\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4e22dd6c-6490-4705-9382-36819a546ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.modules.conv.Conv2d'>\n"
     ]
    }
   ],
   "source": [
    "conv = nn.Conv2d(1,3,1,1,1)\n",
    "for module in conv.modules():\n",
    "    print(module.__class__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d890fb-f3a3-491a-bce2-f639e704723a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
