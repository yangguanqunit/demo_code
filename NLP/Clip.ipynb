{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa629c-aee2-4e3e-9ead-2fa12e1c0d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Union, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75880ea5-3d2d-47b4-a263-ca88d1269e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 768\n",
    "n_head = 12\n",
    "model = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c38a714c-4110-480c-b090-af6802a656d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9aeba9-ba40-4e77-a248-98882a47cf6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 这里是利用了sigmoid对GELU的计算进行了简化？\n",
    "        return x * torch.sigmoid(1.702 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15578ed0-9e1d-401b-9557-6fd7de9d65e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, attn_mask=None):\n",
    "        super(ResidualAttentionBlock, self).__init__()\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "        \n",
    "    def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0]\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # openai的clip中使用的transformer模型是将正则化放在输入数据之前\n",
    "        # 与常规的atten->add->norm->feedforward->add->norm写法有所不同\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    # 在NLP里叫d_model，在CV里叫width\n",
    "    def __init__(self, d_model: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(d_model, heads, attn_mask) for _ in range(layers)])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.resblocks(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033b8f7e-3792-4fd3-b031-b87e65e3b175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: \n",
    "                 int, heads: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        # 卷积核大小为一个patch的大小，卷积核个数与输入attention层中的d_model维度相同\n",
    "        # 例如512*512大小的图像，使用width个大小为128的卷积核，得到的数据就是b*width*4*4\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "        \n",
    "        scale = width ** -0.5\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "        \n",
    "        self.transformer = Transformer(width, layers, heads)\n",
    "        \n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.conv1(x) # shape=[*,width,grid,grid]\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1) # shape=[*,width,grid**2]\n",
    "        x = x.permute(0, 2, 1) # shape=[*,grid**2,width] width表示这个patch的特征维度\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)\n",
    "        # shape = [*, grid**2+1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype) #这里维度不一样相加可能有问题？\n",
    "        x = self.ln_pre(x)\n",
    "        \n",
    "        x = x.permute(1, 0, 2) #这里为啥要转换一下维度？\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        x = self.ln_post(x)\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8309e0-b227-48ac-ae8b-ddcee4b755f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embed_dim: int,\n",
    "                 # vision\n",
    "                 image_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int):\n",
    "        super().__init__()\n",
    "        self.context_length = context_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e10c5011-2b19-427a-8e37-ef13d7ad61c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_resolution = 512\n",
    "patch_size = 128\n",
    "width = 768\n",
    "scale = 128 ** -0.5\n",
    "\n",
    "positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
    "positional_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2787109-ebea-4cef-b230-366677f91353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (168) must match the size of tensor b (768) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m17\u001b[39m,\u001b[38;5;241m168\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpositional_embedding\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (168) must match the size of tensor b (768) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "b = torch.zeros((8,17,168))\n",
    "b + positional_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4069c3e6-b5f9-4323-b5c9-4b9f1cf64384",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
