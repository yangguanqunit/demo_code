{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6efa629c-aee2-4e3e-9ead-2fa12e1c0d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Union, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75880ea5-3d2d-47b4-a263-ca88d1269e19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "d_model = 768\n",
    "n_head = 12\n",
    "model = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762f9c05-304a-4ed0-b0b2-55090ed29efc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    \n",
    "    def __init__(self, inplanes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(inplanes, planes, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(planes, planes, 3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.avgpool = nn.AvgPool2d(stride) if stride > 1 else nn.Identity()\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(planes, planes * self.expansion, 1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(planes * self.expansion)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.downsample = None\n",
    "        self.stride = stride\n",
    "        \n",
    "        if stride > 1 or inplanes != planes * Bottleneck.expansion:\n",
    "            self.downsample = nn.Sequential(OrderedDict([\n",
    "                (\"-1\", nn.AvgPool2d(stride)),\n",
    "                (\"0\", nn.Conv2d(inplanes, planes * self.expansion, 1, stride=1, bias=False)),\n",
    "                (\"1\", nn.BatchNorm2d(planes * self.expansion))\n",
    "            ]))\n",
    "            \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        identity = x\n",
    "        out = self.relu1(self.bn1(self.conv1(x)))\n",
    "        out = self.relu2(self.bn2(self.conv2(x)))\n",
    "        out = self.avgpool(out)\n",
    "        out = self.bn3(self.conv3(x))\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            # 这里主要是为了原始的x与三层卷积后的x维度相匹配\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu3(out)\n",
    "        return out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fa1d0d28-4a1c-444c-92d3-b69b6400cfa2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class AttentionPool2d(nn.Module):\n",
    "    def __init__(self, spacial_dim: int, embed_dim: int, num_heads: int, output_dim: int=None):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(spacial_dim ** 2 + 1, embed_dim) / embed_dim ** 0.5)\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.c_proj = nn.Linear(embed_dim, output_dim or embed_dim)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = x.flatten(start_dim=2).permute(2, 0, 1) # 从第3维开始展平 NC(HW)->(HW)NC\n",
    "        x = torch.cat([x.mean(dim=0, keepdim=True), x], dim=0)# (HW+1)NC\n",
    "        x = x + self.positional_embedding[:, None, :].to(x.dtype) # positional这里是在中间增加了一维\n",
    "        x, _ = F.multi_head_attention_forward(\n",
    "            query=x[:1], key=x, value=x,\n",
    "            embed_dim_to_check=x.shape[-1], # channel的特征为embedding\n",
    "            num_heads=self.num_heads,\n",
    "            q_proj_weight=self.q_proj,\n",
    "            k_proj_weight=self.k_proj,\n",
    "            v_proj_weight=self.v_proj,\n",
    "            in_proj_weight=None,\n",
    "            in_proj_bias=torch.cat([self.q_proj.bias, self.k_proj.bias, self.v_proj.bias]), # 64 64 64 -> 192\n",
    "            bias_k=None,\n",
    "            bias_v=None,\n",
    "            add_zero_attn=False,\n",
    "            dropout_p=0,\n",
    "            out_proj_weight=self.c_proj.weight, # 不懂这是干嘛的\n",
    "            out_proj_bias=self.c_proj.bias, # 不懂这是干嘛的\n",
    "            use_separate_proj_weight=True, # 不懂这是干嘛的\n",
    "            training=self.training,\n",
    "            need_weights=False\n",
    "        )\n",
    "        \n",
    "        return x.squeeze(0)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "32c3db28-872b-4008-923c-67cb27604877",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = nn.Linear(64, 64)\n",
    "k = nn.Linear(64, 64)\n",
    "bias = torch.cat([q.bias, k.bias])\n",
    "bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278bf690-3009-423e-9c2a-aadd365596bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedResNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, layers, output_dim, heads, input_resolution=224, width=64):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.input_resolution = input_resolution\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, width // 2, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(width // 2)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(width // 2, width // 2, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(width // 2)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv3 = nn.Conv2d(width // 2, width, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(width)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        self.avgpool = nn.AvgPool2d(2)\n",
    "        \n",
    "        self._inplanes = width\n",
    "        self.layer1 = self._make_layer(width, layers[0])\n",
    "        self.layer2 = self._make_layer(width * 2, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(width * 4, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(width * 8, layers[3], stride=2)\n",
    "        \n",
    "        embed_dim = width * 32\n",
    "        self.attnpool = AttentionPool2d(input_resolution // 32, embed_dim, heads, output_dim)\n",
    "        \n",
    "    \n",
    "    def _make_layer(self, planes, blocks, stride=1):\n",
    "        layers = [Bottleneck(self._inplanes, planes, stride)]\n",
    "        \n",
    "        self._inplanes = planes * Bottleneck.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(Bottleneck(self._inplanes, planes))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        def stem(x):\n",
    "            x = self.relu1(self.bn1(self.conv1(x)))\n",
    "            x = self.relu2(self.bn2(self.conv2(x)))\n",
    "            x = self.relu3(self.bn3(self.conv3(x)))\n",
    "            x = self.avgpool(x)\n",
    "            return x\n",
    "        \n",
    "        x = x.type(self.conv1.weight.dtype)\n",
    "        x = stem(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.attnpool(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c38a714c-4110-480c-b090-af6802a656d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9aeba9-ba40-4e77-a248-98882a47cf6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 这里是利用了sigmoid对GELU的计算进行了简化？\n",
    "        return x * torch.sigmoid(1.702 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15578ed0-9e1d-401b-9557-6fd7de9d65e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, attn_mask=None):\n",
    "        super(ResidualAttentionBlock, self).__init__()\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "        \n",
    "    def attention(self, x: torch.Tensor):\n",
    "        self.attn_mask = self.attn_mask.to(dtype=x.dtype, device=x.device) if self.attn_mask is not None else None\n",
    "        return self.attn(x, x, x, need_weights=False, attn_mask=self.attn_mask)[0] # 三个x分别为q，k，v\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # openai的clip中使用的transformer模型是将正则化放在输入数据之前\n",
    "        # 与常规的atten->add->norm->feedforward->add->norm写法有所不同\n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "    \n",
    "class Transformer(nn.Module):\n",
    "    # 在NLP里叫d_model，在CV里叫width\n",
    "    def __init__(self, d_model: int, layers: int, heads: int, attn_mask: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.layers = layers\n",
    "        self.resblocks = nn.Sequential(*[ResidualAttentionBlock(d_model, heads, attn_mask) for _ in range(layers)])\n",
    "        \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return self.resblocks(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "033b8f7e-3792-4fd3-b031-b87e65e3b175",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, input_resolution: int, patch_size: int, width: int, layers: int,\n",
    "                 heads: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.input_resolution = input_resolution\n",
    "        self.output_dim = output_dim\n",
    "        # 卷积核大小为一个patch的大小，卷积核个数与输入attention层中的d_model(width)维度相同\n",
    "        # 例如512*512大小的图像，使用width个大小为128的卷积核，得到的数据就是b*width*4*4\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=width, kernel_size=patch_size, stride=patch_size, bias=False)\n",
    "        \n",
    "        scale = width ** -0.5  # 1/sqrt(width)\n",
    "        self.class_embedding = nn.Parameter(scale * torch.randn(width))\n",
    "        self.positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
    "        self.ln_pre = LayerNorm(width)\n",
    "        \n",
    "        self.transformer = Transformer(width, layers, heads)\n",
    "        \n",
    "        self.ln_post = LayerNorm(width)\n",
    "        self.proj = nn.Parameter(scale * torch.randn(width, output_dim))\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        x = self.conv1(x) # shape=[*,width,grid,grid]\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1) # shape=[*,width,grid**2]\n",
    "        x = x.permute(0, 2, 1) # shape=[*,grid**2,width] width表示这个patch的特征维度\n",
    "        x = torch.cat([self.class_embedding.to(x.dtype) + torch.zeros(x.shape[0], 1, x.shape[-1], dtype=x.dtype, device=x.device), x], dim=1)\n",
    "        # shape = [*, grid**2+1, width]\n",
    "        x = x + self.positional_embedding.to(x.dtype) #这里维度不一样相加可能有问题？\n",
    "        x = self.ln_pre(x)\n",
    "        \n",
    "        x = x.permute(1, 0, 2) #这里为啥要转换一下维度？ [grid**2+1, *, width]\n",
    "        x = self.transformer(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        \n",
    "        x = self.ln_post(x)\n",
    "        if self.proj is not None:\n",
    "            x = x @ self.proj\n",
    "\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8309e0-b227-48ac-ae8b-ddcee4b755f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self, \n",
    "                 embed_dim: int,\n",
    "                 # vision\n",
    "                 image_resolution: int,\n",
    "                 vision_layers: Union[Tuple[int, int, int, int], int],\n",
    "                 vision_width: int,\n",
    "                 vision_patch_size: int,\n",
    "                 # text\n",
    "                 context_length: int,\n",
    "                 vocab_size: int,\n",
    "                 transformer_width: int,\n",
    "                 transformer_heads: int,\n",
    "                 transformer_layers: int):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.context_length = context_length\n",
    "        \n",
    "        if isinstance(vision_layers, (tuple, list)):\n",
    "            vision_heads = vision_width * 32 // 64  # 这里的32和64都没看懂是干啥的\n",
    "            self.visual = ModifiedResNet(\n",
    "                layers=vision_layers,\n",
    "                output_dim=embed_dim,\n",
    "                heads=vision_heads,\n",
    "                input_resolution=image_resolution,\n",
    "                width=vision_width\n",
    "            )\n",
    "        else:\n",
    "            vision_heads = vision_width // 64\n",
    "            self.visual = VisionTransformer(\n",
    "                input_resolution=image_resolution,\n",
    "                patch_size=vision_patch_size,\n",
    "                width=vision_width,\n",
    "                layers=vision_layers,\n",
    "                heads=vision_heads,\n",
    "                output_dim=embed_dim\n",
    "            )\n",
    "            \n",
    "        self.transformer = Transformer(\n",
    "            width=transformer_width,\n",
    "            layers=transformer_layers,\n",
    "            heads=transformer_heads,\n",
    "            attn_mask==self.build_attention_mask()\n",
    "        )\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = nn.Embedding(vocab_size, transformer_width)\n",
    "        self.positional_embedding = nn.Parameter(torch.empty(self.context_length, transformer_width))\n",
    "        self.ln_final = LayerNorm(transformer_width)\n",
    "        \n",
    "        self.text_projection = nn.Parameter(torch.empty(transformer_width, embed_dim))\n",
    "        self.logit_scale = nn.Parameter(torch.ones([])) * np.log(1 / 0.07)\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        nn.init.normal_(self.token_embedding.weight, std=0.02)\n",
    "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
    "        \n",
    "        if isinstance(self.visual, ModifiedResNet):\n",
    "            if self.visual.attnpool is Not None:\n",
    "                std = self.visual.attnpool.c_proj.in_features ** -0.5\n",
    "                nn.init.normal_(self.visual.attnpool)\n",
    "        \n",
    "    def build_attention_mask(self):\n",
    "        mask = torch.empty(self.context_length, self.context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1) # 上三角矩阵，不包含对角线\n",
    "        return mask\n",
    "        \n",
    "    def encode_image(self, image):\n",
    "        return self.visual(image.type(self.dtype))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e10c5011-2b19-427a-8e37-ef13d7ad61c7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([17, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_resolution = 512\n",
    "patch_size = 128\n",
    "width = 768\n",
    "scale = 128 ** -0.5\n",
    "\n",
    "positional_embedding = nn.Parameter(scale * torch.randn((input_resolution // patch_size) ** 2 + 1, width))\n",
    "positional_embedding.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2787109-ebea-4cef-b230-366677f91353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (168) must match the size of tensor b (768) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m8\u001b[39m,\u001b[38;5;241m17\u001b[39m,\u001b[38;5;241m168\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpositional_embedding\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (168) must match the size of tensor b (768) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "b = torch.zeros((8,17,168))\n",
    "b + positional_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4069c3e6-b5f9-4323-b5c9-4b9f1cf64384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [0, 5, 6],\n",
       "        [0, 0, 9]], dtype=torch.int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.IntTensor([[1,2,3],[4,5,6],[7,8,9]])\n",
    "a.triu_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f202ee-99f3-46ff-a743-3dc801d5818d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
