{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6efa629c-aee2-4e3e-9ead-2fa12e1c0d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75880ea5-3d2d-47b4-a263-ca88d1269e19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadAttention(\n",
       "  (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 768\n",
    "n_head = 12\n",
    "model = nn.MultiheadAttention(embed_dim=d_model, num_heads=n_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38a714c-4110-480c-b090-af6802a656d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.LayerNorm):\n",
    "    \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        orig_type = x.dtype\n",
    "        ret = super().forward(x.type(torch.float32))\n",
    "        return ret.type(orig_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9aeba9-ba40-4e77-a248-98882a47cf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuickGELU(nn.Module):\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # 这里是利用了sigmoid对GELU的计算进行了简化？\n",
    "        return x * torch.sigmoid(1.702 * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15578ed0-9e1d-401b-9557-6fd7de9d65e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, n_head, attn_mask=None):\n",
    "        super(ResidualAttentionBlock, self).__init__()\n",
    "        \n",
    "        self.attn = nn.MultiheadAttention(d_model, n_head)\n",
    "        self.ln_1 = LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            (\"c_fc\", nn.Linear(d_model, d_model * 4)),\n",
    "            (\"gelu\", QuickGELU()),\n",
    "            (\"c_proj\", nn.Linear(d_model * 4, d_model))\n",
    "        ]))\n",
    "        self.ln_2 = LayerNorm(d_model)\n",
    "        self.attn_mask = attn_mask\n",
    "        \n",
    "    def attention(self, x: torch.Tensor):\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
