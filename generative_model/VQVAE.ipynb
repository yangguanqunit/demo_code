{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "027994e7-302d-4a12-ae87-e2fd36421ec1",
   "metadata": {},
   "source": [
    "# **一个VQVAE算法的简单实现**\n",
    "## **算法概述**\n",
    "- 传统的VAE算法将数据编码为高斯分布\n",
    "- 利用向量量化（vector quantisation）对数据进行编码\n",
    "- \n",
    "\n",
    "论文链接：*https://arxiv.org/abs/1711.00937*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1fcb12f4-eeb9-4cf7-a06b-2a2c7d4e618b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "device = 0 if torch.cuda.is_available else \"cpu\"\n",
    "dataset_name = \"mnist\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "045634e6-7176-443f-b183-cee6f279dab3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VectorQuantizer(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_e, e_dim, beta):\n",
    "        super(VectorQuantizer, self).__init__()\n",
    "        self.n_e = n_e\n",
    "        self.e_dim = e_dim\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.n_e, self.e_dim)\n",
    "        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)\n",
    "        \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "            1. get encoder input (B,C,H,W)\n",
    "            2. flatten input to (B*H*W, C)\n",
    "        \"\"\"\n",
    "        z = z.permute(0,2,3,1).contiguous()\n",
    "        z_flattened = z.view(-1, self.e_dim)  # C == self.e_dim ???\n",
    "        \n",
    "        # cal the dis between each emb in z_flattened and emb in embedding\n",
    "        d = torch.sum(z_flattened ** 2, dim=1, keepdim=True) + torch.sum(self.embedding.weight ** 2, dim=1) - 2 * torch.matmul(z_flattened, self.embedding.weight.t()) # norm l2 squared\n",
    "        \n",
    "        min_encoding_indices = torch.argmin(d, dim=1).unsqueeze(1) # B * H * W, 1\n",
    "        min_encodings = torch.zeros(\n",
    "            min_encoding_indices.shape[0], self.n_e).to(device) # B* H * W, n\n",
    "        min_encodings.scatter_(1, min_encoding_indices, 1)\n",
    "        \n",
    "        z_q = torch.matmul(min_encodings, self.embedding.weight).view(z.shape)\n",
    "        \n",
    "        # compute loss\n",
    "        loss = torch.mean((z_q.detach() - z)**2) + self.beta * torch.mean((z_q - z.detach())**2)\n",
    "        \n",
    "        # unknown\n",
    "        z_q = z + (z_q - z).detach()\n",
    "        \n",
    "        # perplexity\n",
    "        e_mean = torch.mean(min_encodings, dim=0) # the frequency of each n_emb\n",
    "        perplexity = torch.exp(-torch.sum(e_mean * torch.log(e_mean + 1e-10)))\n",
    "        \n",
    "        z_q = z_q.permute(0, 3, 1, 2).contiguous()\n",
    "        \n",
    "        return loss, z_q, perplexity, min_encodings, min_encoding_indices\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "837c7557-accb-4095-973a-aaf743d917af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ResidualLayer(nn.Module):\n",
    "    def __init__(self, in_dim, h_dim, res_h_dim):\n",
    "        super(ResidualLayer, self).__init__()\n",
    "        self.res_block = nn.Sequential(\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(in_dim, res_h_dim, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(res_h_dim, h_dim, kernel_size=1, stride=1, bias=False)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.res_block(x)\n",
    "        return x\n",
    "\n",
    "class ResidualStack(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, h_dim, res_h_dim, n_res_layers):\n",
    "        super(ResidualStack, self).__init__()\n",
    "        self.n_res_layers = n_res_layers\n",
    "        self.stack = nn.ModuleList(\n",
    "            [ResidualLayer(in_dim, h_dim, res_h_dim)] * n_res_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer in self.stack:\n",
    "            x = layer(x)\n",
    "        x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "60f52b9f-1141-4b53-aab4-1648bdb20e97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, h_dim, n_res_layers, res_h_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "        self.conv_stack = nn.Sequential(\n",
    "            nn.Conv2d(in_dim, h_dim // 2, kernel_size=kernel, stride=stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(h_dim // 2, h_dim, kernel_size=kernel, stride=stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(h_dim, h_dim, kernel_size=kernel-1, stride=stride-1, padding=1),\n",
    "            ResidualStack(h_dim, h_dim, res_h_dim, n_res_layers)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.conv_stack(x)\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim, h_dim, n_res_layers, res_h_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        kernel = 4\n",
    "        stride = 2\n",
    "        \n",
    "        self.inverse_conv_stack = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_dim, h_dim, kernel_size=kernel-1, stride=stride-1, padding=1),\n",
    "            ResidualStack(h_dim, h_dim, res_h_dim, n_res_layers),\n",
    "            \n",
    "            nn.ConvTranspose2d(h_dim, h_dim // 2, kernel_size=kernel, stride=stride, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(h_dim // 2, 1, kernel_size=kernel, stride=stride, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.inverse_conv_stack(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ffb4b25a-1200-472d-bd20-0d72cc855b5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VQVAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, h_dim, res_h_dim, n_res_layers,\n",
    "                 n_embeddings, embedding_dim, beta, save_img_embedding_map=False):\n",
    "        super(VQVAE, self).__init__()\n",
    "        \n",
    "        self.encoder = Encoder(1, h_dim,n_res_layers, res_h_dim)\n",
    "        self.pre_quantization_conv = nn.Conv2d(h_dim, embedding_dim, kernel_size=1, stride=1) # n_embeddings is a large num usually\n",
    "        self.vector_quantization = VectorQuantizer(n_embeddings, embedding_dim, beta)\n",
    "        self.decoder = Decoder(embedding_dim, h_dim, n_res_layers, res_h_dim)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        z_e = self.encoder(x)\n",
    "        \n",
    "        z_e = self.pre_quantization_conv(z_e)\n",
    "        embedding_loss, z_q, perplexity, _, _ = self.vector_quantization(z_e)\n",
    "        x_hat = self.decoder(z_q)\n",
    "        \n",
    "        return embedding_loss, x_hat, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "45fee4d4-d04c-4941-a197-1f4639625cdd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torchvision.transforms import Compose\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size_train = 32\n",
    "dataset = load_dataset(dataset_name)\n",
    "transform = Compose([\n",
    "    transforms.Resize((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t:(t*2) - 1)\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "def transforms_(examples):\n",
    "    examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "transformed_dataset = dataset.with_transform(transforms_).remove_columns(\"label\")\n",
    "\n",
    "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "930f49ae-1206-4dfc-a265-25706b7b96dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 batch count 100 , avg loss is 0.5926684428751469\n",
      "epoch 0 batch count 200 , avg loss is 0.8302716314047575\n",
      "epoch 0 batch count 300 , avg loss is 0.7681336539487044\n",
      "epoch 0 batch count 400 , avg loss is 0.6432002666592598\n",
      "epoch 0 batch count 500 , avg loss is 0.5553462177217007\n",
      "epoch 0 batch count 600 , avg loss is 0.49119510595997173\n",
      "epoch 0 batch count 700 , avg loss is 0.43868058823049066\n",
      "epoch 0 batch count 800 , avg loss is 0.39402047006413343\n",
      "epoch 0 batch count 900 , avg loss is 0.3575250707359778\n",
      "epoch 0 batch count 1000 , avg loss is 0.3273730631731451\n",
      "epoch 0 batch count 1100 , avg loss is 0.3019266311993653\n",
      "epoch 0 batch count 1200 , avg loss is 0.2804014690127224\n",
      "epoch 0 batch count 1300 , avg loss is 0.2619312561246065\n",
      "epoch 0 batch count 1400 , avg loss is 0.24590713979942458\n",
      "epoch 0 batch count 1500 , avg loss is 0.23185643459359806\n",
      "epoch 0 batch count 1600 , avg loss is 0.21950621614814736\n",
      "epoch 0 batch count 1700 , avg loss is 0.20847704347551746\n",
      "epoch 0 batch count 1800 , avg loss is 0.19857285557107793\n",
      "epoch 1 batch count 1900 , avg loss is 0.18963730475718255\n",
      "epoch 1 batch count 2000 , avg loss is 0.18153583331871778\n",
      "epoch 1 batch count 2100 , avg loss is 0.17414074679481842\n",
      "epoch 1 batch count 2200 , avg loss is 0.16737908651459624\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m batch_count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     33\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m batch count \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m , avg loss is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_sum_in_one_batch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mbatch_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[52], line 23\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m     21\u001b[0m     data \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 23\u001b[0m     embedding_loss, x_hat, perplexity \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m     recon_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(x_hat, data)\n\u001b[1;32m     25\u001b[0m     loss \u001b[38;5;241m=\u001b[39m embedding_loss \u001b[38;5;241m+\u001b[39m recon_loss\n",
      "File \u001b[0;32m~/miniconda3/envs/my/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[41], line 16\u001b[0m, in \u001b[0;36mVQVAE.forward\u001b[0;34m(self, x, verbose)\u001b[0m\n\u001b[1;32m     13\u001b[0m z_e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(x)\n\u001b[1;32m     15\u001b[0m z_e \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_quantization_conv(z_e)\n\u001b[0;32m---> 16\u001b[0m embedding_loss, z_q, perplexity, _, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvector_quantization\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_e\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m x_hat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z_q)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embedding_loss, x_hat, perplexity\n",
      "File \u001b[0;32m~/miniconda3/envs/my/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[49], line 24\u001b[0m, in \u001b[0;36mVectorQuantizer.forward\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     21\u001b[0m d \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(z_flattened \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mweight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(z_flattened, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mt()) \u001b[38;5;66;03m# norm l2 squared\u001b[39;00m\n\u001b[1;32m     23\u001b[0m min_encoding_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmin(d, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# B * H * W, 1\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m min_encodings \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_encoding_indices\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_e\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;66;03m# B* H * W, n\u001b[39;00m\n\u001b[1;32m     26\u001b[0m min_encodings\u001b[38;5;241m.\u001b[39mscatter_(\u001b[38;5;241m1\u001b[39m, min_encoding_indices, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     28\u001b[0m z_q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(min_encodings, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding\u001b[38;5;241m.\u001b[39mweight)\u001b[38;5;241m.\u001b[39mview(z\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    n_hiddens = 128\n",
    "    n_residual_hiddens = 32\n",
    "    n_residual_layers = 2\n",
    "    n_embeddings = 64\n",
    "    embedding_dim = 512\n",
    "    beta = 0.25\n",
    "    \n",
    "    learning_rate = 3e-4\n",
    "    epochs = 20\n",
    "    \n",
    "    model = VQVAE(n_hiddens, n_residual_hiddens, n_residual_layers, n_embeddings, embedding_dim, beta)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, amsgrad=True)\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    \n",
    "    batch_count = 0\n",
    "    loss_sum_in_one_batch = 0\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, batch in enumerate(dataloader):\n",
    "            data = batch[\"pixel_values\"].to(device)\n",
    "\n",
    "            embedding_loss, x_hat, perplexity = model(data)\n",
    "            recon_loss = F.mse_loss(x_hat, data)\n",
    "            loss = embedding_loss + recon_loss\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_sum_in_one_batch += loss.item()\n",
    "            batch_count += 1\n",
    "            if batch_count % 100 == 0:\n",
    "                print(f\"epoch {epoch} batch count {batch_count} , avg loss is {loss_sum_in_one_batch / batch_count}\")\n",
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8422e9d-9d90-4a7b-94de-9dbbdaa7ec9d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my",
   "language": "python",
   "name": "my"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
