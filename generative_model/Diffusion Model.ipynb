{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28dbc9e3-d94b-40c9-aaec-05322133b3b6",
   "metadata": {},
   "source": [
    "# **一个Diffusion Model算法的简单实现**\n",
    "## **算法概述**\n",
    "- AI绘画模型使用的基础算法\n",
    "- 分为前向过程和后向过程两个阶段\n",
    "    - 前向过程为向原图片中逐步添加高斯噪声，最终原图片成为完全高斯噪声\n",
    "    - 后向过程为从完全的高斯噪声恢复为原图片的过程（生成特指后向过程）\n",
    "- 模型的主体架构为一个U-net网络，在下采样和上采样的对应层之间加入了注意力模块\n",
    "- 模型的任务是预测在前向过程中添加的噪声\n",
    "\n",
    "论文链接：*https://docs.popo.netease.com/ofedit/ba6c1ddd73f242e89c704f37244cc083*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "455f2e60-f237-42a1-b302-bf8ee48c8ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from inspect import isfunction\n",
    "from functools import partial\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from einops import rearrange\n",
    "\n",
    "import torch\n",
    "from torch import nn, einsum\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 0 if torch.cuda.is_available else \"cpu\"\n",
    "device = \"cpu\"\n",
    "# dataset_name = \"mnist\"\n",
    "dataset_name = \"fashion_mnist\" # 一个时装数据集，以灰度图的形式保存了各种时装的照片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfcc0323-9fd1-4928-9848-8517a88b357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exists(x):\n",
    "    return x is not None\n",
    "\n",
    "def default(val, d):\n",
    "    if exists(val):\n",
    "        return val\n",
    "    return d() if isfunction(d) else d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7ca62660-0290-4385-9709-6c5803291a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "    \n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.fn(x, *args, **kwargs) + x\n",
    "    \n",
    "\n",
    "def Upsample(dim):\n",
    "    return nn.ConvTranspose2d(dim, dim, 4, 2, 1)\n",
    "\n",
    "def Downsample(dim):\n",
    "    return nn.Conv2d(dim, dim, 4, 2, 1)\n",
    "\n",
    "class SinusoidaPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, time:torch.Tensor):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embedding = math.log(10000) / (half_dim - 1)\n",
    "        embedding = torch.exp(torch.arange(half_dim, device=device) * - embedding)\n",
    "        embedding = rearrange(time, 'i -> i 1') * rearrange(embedding, 'j -> 1 j')\n",
    "        # embedding = time[:,None] * embedding[:,None]\n",
    "        return torch.cat((embedding.sin(), embedding.cos()), dim = -1)\n",
    "    \n",
    "class Block(nn.Module):\n",
    "    def __init__(self, dim, dim_out, groups=4):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(dim, dim_out, 3, padding=1)\n",
    "        self.norm = nn.GroupNorm(groups, dim_out)\n",
    "        self.act = nn.SiLU()\n",
    "        \n",
    "    def forward(self, x, scale_shift=None):\n",
    "        x = self.proj(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        if exists(scale_shift):\n",
    "            scale, shift = scale_shift\n",
    "            x = x * (scale + 1) + shift\n",
    "        \n",
    "        x = self.act(x)\n",
    "        return x\n",
    "    \n",
    "class ResnetBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim = None, groups=4):\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "        nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
    "            if exists(time_emb_dim) else None\n",
    "        )\n",
    "        \n",
    "        self.block1 = Block(dim, dim_out, groups=groups)\n",
    "        self.block2 = Block(dim_out, dim_out, groups=groups)\n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, time_emb=None):\n",
    "        h = self.block1(x)\n",
    "        \n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            time_emb = self.mlp(time_emb)\n",
    "            h = rearrange(time_emb, \"b c -> b c 1 1\") * h\n",
    "        \n",
    "        h = self.block2(h)\n",
    "        return h + self.res_conv(x)\n",
    "    \n",
    "class ConvNextBlock(nn.Module):\n",
    "    def __init__(self, dim, dim_out, *, time_emb_dim=None, mult=2, norm=True):\n",
    "        super().__init__()\n",
    "        self.mlp = (\n",
    "        nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out))\n",
    "            if exists(time_emb_dim) else None\n",
    "        )\n",
    "        \n",
    "        self.ds_conv = nn.Conv2d(dim, dim, 7, padding=3, groups=dim)\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.GroupNorm(1,dim) if norm else nn.Identity(),\n",
    "            nn.Conv2d(dim, dim_out * mult, 3, padding=1),\n",
    "            nn.GELU(),\n",
    "            nn.GroupNorm(1, dim_out * mult),\n",
    "            nn.Conv2d(dim_out * mult, dim_out, 3, padding=1),\n",
    "        )\n",
    "        \n",
    "        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()\n",
    "        \n",
    "    def forward(self, x, time_emb=None):\n",
    "        h = self.ds_conv(x)\n",
    "        \n",
    "        if exists(self.mlp) and exists(time_emb):\n",
    "            assert exists(time_emb), \"time_emb must be passed in\"\n",
    "            condition = self.mlp(time_emb)\n",
    "            h = h + rearrange(condition, \"b c -> b c 1 1\")\n",
    "        \n",
    "        h = self.net(h)\n",
    "        return h + self.res_conv(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8615c87-745b-422e-a2fb-3bc808da2465",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim *3 ,1, bias=False)\n",
    "        self.to_out = nn.Conv2d(hidden_dim, dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t:rearrange(t, \"b (h c) x y -> b h c (x y)\",h=self.heads), qkv\n",
    "        )\n",
    "        q = q * self.scale\n",
    "        \n",
    "        sim = einsum(\"b h d i, b h d j -> b h i j\", q, k)\n",
    "        attn = sim.softmax(dim=-1)\n",
    "        \n",
    "        out = einsum(\"b h i j, b h d j -> b h i d\", attn, v)\n",
    "        out = rearrange(out, \"b h (x y) d -> b (h d) x y\", x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class LinearAttention(nn.Module):\n",
    "    def __init__(self, dim, heads=4, dim_head=32):\n",
    "        super().__init__()\n",
    "        self.scale = dim_head**-0.5\n",
    "        self.heads = heads\n",
    "        hidden_dim = dim_head * heads\n",
    "        self.to_qkv = nn.Conv2d(dim, hidden_dim *3 ,1, bias=False)\n",
    "        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1),\n",
    "                                   nn.GroupNorm(1, dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=1)\n",
    "        q, k, v = map(\n",
    "            lambda t:rearrange(t, \"b (h c) x y -> b h c (x y)\", h=self.heads), qkv\n",
    "        )\n",
    "        \n",
    "        q = q.softmax(dim=-2)\n",
    "        k = k.softmax(dim=-1)\n",
    "        \n",
    "        q = q * self.scale\n",
    "        context = einsum(\"b h d n, b h e n -> b h d e\", k ,v)\n",
    "        \n",
    "        out = einsum(\"b h d e, b h d n -> b h e n\", context, q)\n",
    "        out = rearrange(out, \"b h c (x y) -> b (h c) x y\", h=self.heads, x=h, y=w)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.GroupNorm(1, dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c4c37726-d7e3-4a7f-8dea-c840531a4106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet(nn.Module):\n",
    "    def __init__(self, dim, init_dim=None, out_dim=None, dim_mults=(1, 2, 4, 8),\n",
    "                channels=3, with_time_emb=True, resnet_block_groups=4, use_convnext=False, convnext_mult=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.channels = channels\n",
    "        init_dim = default(init_dim, dim // 3 * 2)\n",
    "        self.init_conv = nn.Conv2d(channels, init_dim, 7, padding=3)\n",
    "        \n",
    "        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]\n",
    "        in_out = list(zip(dims[:-1], dims[1:]))\n",
    "        \n",
    "        if use_convnext:\n",
    "            block_klass = partial(ConvNextBlock, mult=convnext_mult)\n",
    "        else:\n",
    "            block_klass = partial(ResnetBlock, groups=resnet_block_groups)\n",
    "        \n",
    "        \n",
    "        if with_time_emb:\n",
    "            time_dim = dim * 4\n",
    "            self.time_mlp = nn.Sequential(\n",
    "                SinusoidaPositionEmbeddings(dim),\n",
    "                nn.Linear(dim, time_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(time_dim, time_dim),\n",
    "            )\n",
    "        else:\n",
    "            time_dim = None\n",
    "            self.time_mlp = None\n",
    "        \n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        num_resolution = len(in_out)\n",
    "        \n",
    "        for idx, (dim_in, dim_out) in enumerate(in_out):\n",
    "            is_last = idx >= (num_resolution - 1)\n",
    "            \n",
    "            self.downs.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_in, dim_out, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_out, dim_out, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),\n",
    "                        Downsample(dim_out) if not is_last else nn.Identity(),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        mid_dim = dims[-1]\n",
    "        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))\n",
    "        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)\n",
    "        \n",
    "        for idx, (dim_in, dim_out) in enumerate(reversed(in_out[1:])):\n",
    "            is_last = idx >= num_resolution - 1\n",
    "            \n",
    "            self.ups.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        block_klass(dim_out * 2, dim_in, time_emb_dim=time_dim),\n",
    "                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),\n",
    "                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),\n",
    "                        Upsample(dim_in) if not is_last else nn.Identity(),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "            \n",
    "        out_dim = default(out_dim, channels)\n",
    "        self.final_conv = nn.Sequential(\n",
    "            block_klass(dim, dim),\n",
    "            nn.Conv2d(dim, out_dim, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, time):\n",
    "        x = self.init_conv(x)\n",
    "        \n",
    "        t = self.time_mlp(time) if exists(self.time_mlp) else None\n",
    "        \n",
    "        h = []\n",
    "        \n",
    "        # downsample\n",
    "        for block1, block2, attn, downsample in self.downs:\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            h.append(x)\n",
    "            x = downsample(x)\n",
    "        \n",
    "        # bottleneck\n",
    "        x = self.mid_block1(x, t)\n",
    "        x = self.mid_attn(x)\n",
    "        x = self.mid_block2(x, t)\n",
    "        \n",
    "        # upsample\n",
    "        for block1, block2, attn, upsample in self.ups:\n",
    "            x = torch.cat((x, h.pop()), dim=1)\n",
    "            x = block1(x, t)\n",
    "            x = block2(x, t)\n",
    "            x = attn(x)\n",
    "            x = upsample(x)\n",
    "        res = self.final_conv(x)\n",
    "        return res          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6688ae3-cb69-4212-b371-ed7d7da431f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def consine_beta_schedule(timesteps, s=0.008):\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, timesteps, steps)\n",
    "    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]\n",
    "    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])\n",
    "    return torch.clip(betas, 0.0001, 0.9999)\n",
    "\n",
    "def linear_beta_schedule(timesteps):\n",
    "    beata_start = 0.0001\n",
    "    beta_end = 0.02\n",
    "    return torch.linspace(beata_start, beta_end, timesteps)\n",
    "\n",
    "# and more beta_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37cc87b4-51d4-4ee5-990d-e21971a6c85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 200\n",
    "\n",
    "betas =  linear_beta_schedule(timesteps)\n",
    "\n",
    "alphas = 1. - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, axis=0)\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "sqrt_recip_alphas = torch.sqrt(1.0 / alphas)\n",
    "\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1 - alphas_cumprod)\n",
    "\n",
    "sigma_t = torch.cat((torch.tensor([0]), alphas[1:] / betas[1:] + 1 / torch.pow(sqrt_one_minus_alphas_cumprod, 2)[:-1]))\n",
    "posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bb63032-8868-41b8-af07-f56a7b175a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Compose\n",
    "\n",
    "def q_sample(x_0, t, noise=None):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_0)\n",
    "    \n",
    "    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_0.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t, x_0.shape)\n",
    "    \n",
    "    return sqrt_one_minus_alphas_cumprod_t * x_0 + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "def get_noisy_image(x_0, t):\n",
    "    x_noisy = q_sample(x_0, t) \n",
    "    \n",
    "    noisy_image = reverse_transform(x_noisy.squeeze())\n",
    "    \n",
    "    return noisy_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51041471-cb44-4ee8-b357-8c47248c29fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_losses(denoise_model, x_0, t, noise=None, loss_type=\"l1\"):\n",
    "    if noise is None:\n",
    "        noise = torch.randn_like(x_0)\n",
    "    \n",
    "    x_noisy = q_sample(x_0, t, noise)\n",
    "    predicted_noise = denoise_model(x_noisy, t)\n",
    "    loss = F.l1_loss(noise, predicted_noise)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1c9f9b0-3226-4817-862d-64f4f863476a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset fashion_mnist (/home/ygq/.cache/huggingface/datasets/fashion_mnist/fashion_mnist/1.0.0/8d6c32399aa01613d96e2cbc9b13638f359ef62bb33612b077b4c247f6ef99c1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96c28312c1c64f47a845c59e898ee077",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(dataset_name)\n",
    "image_size = 28\n",
    "channels = 1\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee12a090-5681-4ec7-bad5-b1e28d0e662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda t:(t*2) - 1)\n",
    "])\n",
    "\n",
    "def transforms_(examples):\n",
    "    examples[\"pixel_values\"] = [transform(image.convert(\"L\")) for image in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples\n",
    "\n",
    "transformed_dataset = dataset.with_transform(transforms_).remove_columns(\"label\")\n",
    "\n",
    "dataloader = DataLoader(transformed_dataset[\"train\"], batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "171bd393-ebba-48cf-90a1-f54d41bb0055",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def generate_image(images):\n",
    "    image = images[0,0,:,:]\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4079e6a0-9ba5-42bb-9c9b-76da58f27496",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "\n",
    "model = Unet(\n",
    "    dim=image_size,\n",
    "    channels=channels,\n",
    "    dim_mults=(1, 2, 4)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22967638-773f-4cef-954e-bf19da14f6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def p_sample(model, x, t, t_index):\n",
    "    betas_t = extract(betas, t, x.shape)\n",
    "    sqrt_one_minus_alphas_cumprod_t = extract(\n",
    "        sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
    "    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)\n",
    "    \n",
    "    model_mean = sqrt_recip_alphas_t * (\n",
    "        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "    )\n",
    "    \n",
    "    if t_index == 0:\n",
    "        return model_mean\n",
    "    else:\n",
    "        posterior_variance_t = extract(posterior_variance, t, x.shape)\n",
    "        noise = torch.randn_like(x)\n",
    "        \n",
    "        return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "@torch.no_grad()\n",
    "def p_sample_loop(model, shape):\n",
    "    b = shape[0]\n",
    "    img = torch.randn(shape, device=device)\n",
    "    imgs = []\n",
    "    \n",
    "    for i in reversed(range(0, 200)):\n",
    "        img = p_sample(model, img, torch.full((b,), i, dtype=torch.long).to(device), i)\n",
    "        imgs.append(img.cpu().numpy())\n",
    "    return imgs\n",
    "\n",
    "def sample(model, image_size, batch_size=64, channels=3):\n",
    "    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9581b6f2-4108-4a4f-9e77-6dca10c4fb13",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 step 0, loss is 1.0175487995147705\n",
      "epoch 0 step 100, loss is 0.26050490140914917\n",
      "epoch 0 step 200, loss is 0.17977216839790344\n",
      "epoch 0 step 300, loss is 0.1644842028617859\n",
      "epoch 0 step 400, loss is 0.16392913460731506\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        batch_size = batch[\"pixel_values\"].shape[0]\n",
    "        batch = batch[\"pixel_values\"].to(device)\n",
    "        \n",
    "        t = torch.randint(0, timesteps, (batch_size,), device = device).long()\n",
    "        \n",
    "        loss = p_losses(model, batch, t)\n",
    "        if step % 100 == 0:\n",
    "            print(f\"epoch {epoch} step {step}, loss is {loss.item()}\")\n",
    "        \n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # generate_image(model)\n",
    "    sample_list = sample(model, image_size=image_size, batch_size=64,channels=channels)\n",
    "    generate_image(sample_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9016df3-325e-47c8-948f-2a71bc37e14d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a8d229-f66c-49d5-955a-7baf02c838d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264860b5-1f61-4492-8998-7f0cb9e96e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dalle2",
   "language": "python",
   "name": "dalle2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
